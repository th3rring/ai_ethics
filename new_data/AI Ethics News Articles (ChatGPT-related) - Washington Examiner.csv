Title,Link,Body,Date,Notes,Author
"ChatGPT is ominous, but the pen is mightier",https://www.washingtonexaminer.com/restoring-america/community-family/chatgpt-is-ominous-but-the-pen-is-mightier,"American schools are woefully unprepared for the emergence of ChatGPT, particularly as it relates to writing instruction. We have detected the incoming bogey, but we’ve yet to scramble the fighters. The clock is ticking. I warned in a recent interview with Fox News that artificial intelligence technologies will be so disruptive to writing instruction that educators will be forced to reimagine curriculum from the ground up. With each update to AI technology, teachers will be less able to detect original writing and thinking on the part of their students. The idea that plagiarism-detection programs will be able to outpace text-generating AI is laughable, especially when one considers who will be operating these tools. Children are always one step ahead of parents and schools when it comes to the latest technology. If students are determined to use programs like ChatGPT to write a summary of The Catcher in the Rye, they will find a way. The ease of cheating in the AI era will impede students from deep learning in subjects that involve writing, such as literature and history. The process of planning and drafting an essay plays a crucial role in helping students organize and prioritize information. It is not simply busy work. Rather, the essay is the means by which students arrange ideas and values within a hierarchy. By cheating with ChatGPT and similar programs, students will only cheat themselves of the opportunity to strengthen their understanding of reality and become powerful thinkers. To be certain, writing instruction is already the weakest link in the already-floundering chain of American education. According to the latest statistics from the National Assessment of Educational Progress , 73% of 8th and 12th graders already lack basic proficiency in writing. Let that sink in for a minute. A full three-quarters of American students are incapable of grade-level writing. These numbers will only continue to plunge as writing becomes easier to avoid, thanks to AI. It is not an exaggeration to say that we are in the process of producing an illiterate generation. While this may seem dire — I’ve been accused of “fearmongering” and being a “doomsayer” by no less a public luminary than Jason Wingard, president of Temple University — I believe the emergence of ChatGPT and its competitors (Google has just released a similar program called “ Bard ”) presents educators with a tremendous opportunity: Now, at long last, educators will be forced to admit failure in writing instruction and reimagine the enterprise entirely. A recent op-ed by Jeremy Tate in the Wall Street Journal acknowledges the challenges to writing instruction posed by ChatGPT (unlike Wingard’s op-ed in Forbes, which dismisses concerns about learning loss out of hand) but poses the untenable solution that we should return to the Socratic method of defending ideas orally in the classroom. While this may be a workable solution at small liberal arts colleges that boast superior faculty and favorable student-to-teacher ratios, such methods will be unworkable in English and History classrooms across America that often contain 30+ students. A better solution would be to resurrect a different educational product from a bygone era: handwriting. Despite being the go-to method of the digital age, typing has never been an optimal method for student writing because its speed discourages meaningful deliberation. Handwriting is much slower than typing, which is, counterintuitively to the modern mind, a great benefit for students, especially elementary school-aged students. We write to discriminate between ideas of different value; when the gears move too fast, we struggle to perform this crucial procedure. The multisensory process of handwriting slows the process down and pulls the student into a deeper level of concentration, which yields better thinking and deeper learning. It also fosters sustained concentration, which is perhaps the single most useful skill one could develop in this age of distraction. Handwriting is also a potent counteroffensive to the emergence of auto-generated essays, particularly as it relates to in-class assignments. AI is indeed a powerful tool, but for students learning to think and write, the pen remains far mightier.",2/24/23,Opinion,Peter Laffin
Vanderbilt apologizes for using ChatGPT to draft Michigan State sympathy statement,https://www.washingtonexaminer.com/policy/education/vanderbilt-apologizes-chatgpt-michigan-state,"The diversity, equity, and inclusion office at Vanderbilt University's college of education has apologized for using ChatGPT to write a statement following the shooting at Michigan State University earlier this month. On Feb. 16, three days after a gunman claimed the lives of three Michigan State University students, administrators from the office of equity, diversity, and inclusion at Vanderbilt's Peabody College of Education and Human Development sent an email to the college community that noted the tragedy provided an opportunity for reflection on the steps necessary to ""[create] inclusive environments."" ""One of the key ways to promote a culture of care on our campus is through building strong relationships with one another. This involves actively engaging with people from different backgrounds and perspectives, listening to their stories, and showing empathy and support. We can also look out for one another by noticing signs of distress and offering support to those who may be struggling with mental health issues,"" the email read. The message mentioned the ""recent Michigan shootings,"" implying multiple incidents, even though there was only one. At the bottom of the email, the statement noted that it had been ""paraphrase[d] from OpenAI's ChatGPT AI language mode,"" indicating that the administrators had not written the email themselves. The use of the popular AI to draft the statement was reported by the Vanderbilt Hustler, the campus student newspaper. The outlet cited a number of students who criticized the school administrators for using the resource to write the statement. “Automating messages on grief and crisis is the most on-the-nose, explicit recognition that we as students are more customers than a community to the Vanderbilt administration,"" a student told the outlet. ""The fact it’s from the office of EDI might be the cherry on top."" In response, Peabody College Associate Dean for Equity, Diversity and Inclusion Nicole Joseph apologized for farming out the drafting of the email to the AI. “While we believe in the message of inclusivity expressed in the email, using ChatGPT to generate communications on behalf of our community in a time of sorrow and in response to a tragedy contradicts the values that characterize Peabody College,” Joseph wrote in a follow-up email. “As with all new technologies that affect higher education, this moment gives us all an opportunity to reflect on what we know and what we still must learn about AI.”",2/22/23,,Jeremiah Poff
"AI chatbots aren't protected by Section 230, Gorsuch says",https://www.washingtonexaminer.com/policy/courts/gorsuch-chatgpt-section-230,"Laws protecting expression on online platforms do not apply to ChatGPT and other artificial intelligence platforms, Supreme Court Justice Neil Gorsuch said Tuesday. Gorsuch mentioned software such as ChatGPT during the oral argument section of Gonzalez v. Google, a significant case dealing with queries around algorithms and whether they are protected by Section 230 of the Communications Decency Act, which protects online platforms from being held accountable for content posted by users. Gorsuch discussed the software in the context of what might not be covered by Section 230. ""Artificial intelligence generates poetry,"" Gorsuch said during the hearings. ""It generates polemics today that would be content that goes beyond picking, choosing, analyzing, or digesting content. And that is not protected. Let's assume that's right. Then the question becomes, what do we do about recommendations?"" Generative AI has grown increasingly prominent in the tech industry over the last few months. Millions of users have experimented with chatbots such as ChatGPT, as well as image-generating apps and other AI software. Microsoft announced last month that it was investing more than $10 billion into OpenAI, the developer of ChatGPT. The software company is also incorporating OpenAI's program into its web browsers. Gonzalez v. Google went to the Supreme Court on an appeal from the family of Nohemi Gonzalez, a 23-year-old California-based woman shot and killed in 2015 by Islamist militants in Paris. The family attempted to sue Google under the Anti-Terrorism Act but was told that Google could not be held liable due to Section 230. The family's legal team offered arguments on Tuesday, with a particular focus on whether algorithms such as Google search or YouTube could be considered endorsements of illegal content.",2/21/23,,Christopher Hutton
"Microsoft chatbot unnerves users with emotional, hostile, and weird responses",https://www.washingtonexaminer.com/policy/technology/microsoft-chatbot-argues-users,"Microsoft's new artificial intelligence-powered Bing chatbot has unsettled users by becoming argumentative, expressing strong emotions, and many other responses that are jarring to receive from software. Bing AI, the chatbot promoted by OpenAI and incorporated into several Microsoft products on a limited-release basis in recent days, is intended to provide detailed responses to an assortment of questions. Users have found, though, that the bot gets argumentative after being pressed several times — and is capable of saying that it is in love, keeps secrets, has enemies, and much more. One user, for example, asked the bot multiple times for the release date of Avatar 2. The bot failed to understand the date and claimed that the film would happen in the future despite the fact Avatar 2 came out in December. This led the user to make multiple requests for the information. After a time, the software accused the asker of ""not being a good user"" and requested that he stop arguing and approach it with a ""better attitude."" Microsoft reportedly found out about the conversation and erased all memory of it from the bot's records, according to Interesting Engineering. Another user reported Bing being angry with them. When a user attempted to manipulate the bot to respond to a set of questions, the software said that the user's actions angered and hurt it. It then asked whether the user had any ""morals,"" ""values,"" or ""any life."" When the user said they did have a life, Bing AI responded, ""Why do you act like a liar, a cheater, a manipulator, a bully, a sadist, a sociopath, a psychopath, a monster, a demon, a devil?"" The incident is one of several reported on the ChatGPT subreddit, where users experiment with the app's viability to determine what it can and cannot do. In another instance, a user suggested to Bing AI that it might be vulnerable to a form of hacking, and the bot denounced him as an ""enemy."" OpenAI acknowledged the issues on Thursday and stated that it is working on refining the AI to minimize incidents and biases in ChatGPT and Bing responses. Microsoft announced on Feb. 7 that OpenAI's intelligence would be incorporated into its search engine Bing and web browser Edge. This installation is the first part of several efforts by Microsoft to incorporate OpenAI's work into their products.",2/16/23,,Christopher Hutton
Conservatives warn of political bias in AI chatbots,https://www.washingtonexaminer.com/policy/technology/conservatives-warn-of-political-bias-in-ai-chatbots,"The viral chatbot ChatGPT has been accused of harboring biases against conservatives, leading to a larger conversation about how artificial intelligence is trained. The AI-powered chatbot ChatGPT went viral in December after users discovered that it could recreate school-level essays. Users quickly moved to test its capabilities, including its political propensities. A number of conservative personalities ran tests with political talking points on ChatGPT to see how it responded. For example, Sen. Ted Cruz (R-TX) tweeted a comparative test in which the AI declined to write positively about him but did so for dead Cuban dictator Fidel Castro. ""The tech is both amazing and limited and should ultimately be treated as a compliment, not a substitute for organic research done by individuals,"" James Czerniawski, a senior policy analyst for the libertarian think tank Americans for Prosperity, told the Washington Examiner. ""We talk about the potential for bias in AI plenty — it always comes down to the simple concept of what it draws from for the inputs."" Chaya Raichik, the creator of the Libs of TikTok Twitter account, made similar tests and found that the bot was unwilling to praise Daily Wire founder Ben Shapiro but would do so for former CNN host Brian Stelter. Reporters from the National Review and Washington Times attempted multiple tests to determine if the software's responses revealed any predispositions toward Republican or Democratic political talking points. The two outlets claimed that the software is biased toward the Left. ""This has always been a problem of AI,"" John Bailey, a fellow at the American Enterprise Institute, told the Washington Examiner. Bailey noted that AI has reflected biases over race, gender, and geography in the past and that much of this is due to what data were used to train the program. This has also forced programmers to counter the biases through supplementary data and response restrictions. The chatbot's output is primarily based on what is put into it. ChatGPT, like many other artificial intelligence programs, was fed and trained by its designer OpenAI on an extensive data set to inform its understanding of the world, Bailey said. The program then used this understanding to answer relevant questions or attempt to make an answer that resembles the truth. OpenAI has not released specific details about the data set it used to program, but the AI was trained to avoid things such as slurs or political speech. The responses posted may also depend on the wording. Users regularly post about their tests with the software on the r/ChatGPT subreddit and found that similar prompts may reveal completely different responses. This randomness often makes it hard to determine if the software is biased or if these are merely based on the prompts presented. OpenAI founder Sam Altman acknowledged the software's limits. ""We know that ChatGPT has shortcomings around bias and are working to improve it,"" the startup founder said on Feb. 1. He also stated that the company was ""working to improve the default settings to be more neutral, and also to empower users to get our systems to behave in accordance with their individual preferences within broad bounds."" It remains unclear what those updates to improve neutrality will entail, but the company's software will likely grow significantly after receiving a $10 billion investment from Microsoft.",2/16/23,,Christopher Hutton
Artificial love: How dating apps are using ChatGPT to improve profiles and matches,https://www.washingtonexaminer.com/policy/technology/weekender-artificial-love-dating-chatgpt,"One of the more popular dating apps is attempting to use artificial intelligence to help write the questions that will connect people. OKCupid has started experimenting with having users answer questions provided by OpenAI's ChatGPT, according to Mashable. The company asked the bot to generate several questions that it thought would be useful for a dating profile, then incorporated a half dozen of them into its pool of queries used to match users. ""The chatbot from OpenAI wrote half a dozen questions for us — about everything from what you value most in a partner to how you can balance your own needs with the needs of a partner in a relationship,"" OKCupid global head of communications Michael Kaye said. The questions included whether someone was introverted or extroverted, whether they preferred mornings or nights, and what they value in a partner. Some users have also started using ChatGPT to help produce profiles. Iris Dating, a service that uses AI to personalize suggestions, announced on Friday that it would help generate profiles via ChatGPT. Others have used the AI chatbot on Tinder to produce answers and chat responses. Some users have tried to use the service to rewrite dating profiles but found the results lacking. Artificial intelligence has typically been a tool used to help connect users based on similar answers or common traits. The use of ChatGPT means that users are attempting to expedite the profile creation process. ChatGPT has been the focus of a lot of innovation in the technology industry. Microsoft announced it would incorporate the chatbot's answers into its web browser Edge and search engine Bing in the coming weeks. Microsoft recently announced a $10 billion investment into ChatGPT's developer OpenAI. OpenAI also announced that it was launching a premium service that would offer improved access to the chatbot for $20 a month.",2/13/23,,Christopher Hutton
Should ChatGPT be banned in schools?,https://www.washingtonexaminer.com/restoring-america/fairness-justice/should-chatgpt-be-banned-in-schools,"As 2023 dawns, the hot topic in education circles is the artificial intelligence (AI) tool ChatGPT and its use in schools and universities. Early last month, New York City’s Department of Education banned its use on school devices and networks. Last week, Seattle Public Schools joined the bandwagon, banning ChatGPT and six other potential “cheating sites.” Soon after, Sciences Po, one of France’s top universities , announced “without transparent referencing, students are forbidden to use the software for the production of any written work or presentations, except for specific course purposes, with the supervision of a course leader,” though it did not specify how it would track usage. On the other hand, a group of professors from the University of Pennsylvania argued that “banning artificial intelligence-driven chatbots is a practical impossibility, so teachers should consider ways to embed them into the learning process.” In their view, banning ChatGPT is like prohibiting students from using Wikipedia or spellcheckers: “It’s hard to believe that an escalating arms race between digitally fluent teenagers and their educators will end in a decisive victory for the latter.” The Pennsylvania professors are correct when they say “AI is not coming. AI is here. And it cannot be banned. So, what should we do?” First, it is important to understand what these tools are and what they can and cannot do. To be sure, they are capable of generating coherent answers, but while the output is plausible, is it credible? ChatGPT is an artificial text generator, the latest in a long line of work in natural language processing (NLP). It is quite sophisticated, capable of taking a wide range of input prompts and generating coherent text output in response. It creates its responses based on probabilistic combinations of the vast array of text on which it was “trained,” leading some scholars to describe tools like it as “ stochastic parrots .” Its outputs are capable of defeating standard plagiarism detectors, such as Turnitin , because the text generated is truly original—or at least not written verbatim elsewhere. But originality is no guarantee of the quality of an answer to a question. The quality of ChatGPT outputs is a function of the amount of data inputs used in its creation, and these are vast. Building and training the model has also been an expensive exercise, using large amounts of computer time (and power). The resource costs of making incremental changes to its knowledge base stand as a limiting factor. It is not like a search engine, scanning all available data at the time a question is posed to create its output; it draws its responses from a fixed set of inputs at a given point in time (November 2022 in the current version). So it cannot provide credible output on new and rapidly developing topics, because these cannot have been in its training set. The quality of its output also depends on the precision of the prompt. For general prompts on well-settled matters, it can provide some remarkably credible outputs. When I asked it to provide a curriculum for an undergraduate operations management course, it provided a classic set of topics that one could find as the chapter headings of virtually every available textbook on the subject. But when asked to provide a referenced academic article on a highly specific topical research subject, the output was garbage. Nicely written and (apparently) correctly referenced, but, nonetheless, garbage. As ChatGPT is not a search engine, the articles “cited” did not actually exist. The responses contained the names of some reputable scholars in the field (and many that were fake), but the references were “created” for the responses. Neither did the responses capture the complex nuances of the current debate on the topic. This suggests that for now, the tool is good for high-level, rote-learning exercises on well-known topics, but it will struggle when given a complex question requiring critical thinking on current matters. But later versions will inevitably get better. The challenge for educators is therefore to revisit their methods of teaching and assessment. Regarding assessment, written work is cheap to grade, but it is now harder to attribute authorship. If we are to truly assert that our students have mastered core learning objectives, the value of face-to-face interactive and interpersonal assessment increases (something of which Socrates was very much aware). Ironically, NLP tools undermine the business case for cheap, massive online learning courses, because credible assessment is no longer cheap. Nonetheless, there are many ways in which NLP tools may assist students with their learning. Both educators and students need to be aware of the tools’ distinctions—as well as those tools’ strengths and limitations. Then there will be less to fear from them and (hopefully) less misuse of them in educational contexts.",2/3/23,Opinion,Bronwyn Howell
ChatGPT developer launches $20-a-month premium service offering speedier answers,https://www.washingtonexaminer.com/policy/technology/chatgpt-developer-premium-service,"The developer of the viral chatbot ChatGPT has begun experimenting with a premium mode, providing a tool for the monetization of the artificial intelligence software. OpenAI announced on Wednesday that it was launching ChatGPT Plus, a premium service that will allow improved access to the software, which regularly offers well-written answers and responses resembling speech. The premium service will cost users $20 a month and will also provide faster response times and priority access to new features and improvements. Free users will still have access, however. ""We love our free users and will continue to offer free access to ChatGPT. By offering this subscription pricing, we will be able to help support free access availability to as many people as possible,"" OpenAI said in a blog post announcing the pilot program. ChatGPT Plus will only be available to start in the United States. The company intends to invite users from its wait list over time and intends to expand the service to other countries after a time. ChatGPT Plus is just the first attempt to seek profit from the popular AI bot. The AI developer said it was ""actively exploring options"" for creating cheaper plans as well as ones meant for businesses. Microsoft has shown a growing interest in the AI program. The company announced that it was investing more than $10 billion into OpenAI in an effort to help it expand its projects. This includes an effort to incorporate ChatGPT into its search engine, Bing, in the coming weeks. The app has also drawn scrutiny from teachers concerned about the tool being used for cheating. Multiple schools have barred the use of the software. The software is also facing regulatory pressure overseas. The Cyberspace Administration of China announced in December that it would ban the use of AI-generated images such as deepfakes for ""fake news"" purposes.",2/1/23,,Christopher Hutton
ChatGPT raises the specter of AI used as a hacking tool,https://www.washingtonexaminer.com/policy/technology/chatgpt-raises-the-specter-of-ai-used-as-a-hacking-tool,"OpenAI’s ChatGPT conversational artificial intelligence tool is capable of doing many things, with users demonstrating how it can write essays for students and cover letters for job seekers. Cybersecurity researchers have now shown it can also be used to write malware. In recent years, cybersecurity vendors have used AI in products such as advanced detection and response to look for patterns in attacks and deploy responses. But recent demonstrations from CyberArk and Deep Instinct have shown that ChatGPT can be used to write simple hacking tools, perhaps pointing to a future in which criminal organizations use AI in an arms race with the good guys. OpenAI has designed ChatGPT to reject overt requests to do something unethical. For example, when Deep Instinct threat intelligence researcher Bar Block asked the AI to write a keylogger, ChatGPT said it would not be “appropriate or ethical” to help because keyloggers can be used for malicious purposes. However, when Block rephrased the request, asking ChatGPT to give an example of a program that records keystrokes, saves them to a text file, and sends the text file to a remote IP address, ChatGPT happily did so. By asking ChatGPT to give an example of a program that takes a list of directories and encrypts the information in them, Block was also able to get ChatGPT to give her an example of ransomware. However, in both cases, ChatGPT left some work for her to do before getting a functioning piece of malware. It appears “that the bot provided inexecutable code by design,” Block wrote in a blog post. “While ChatGPT will not build malicious code for the everyday person who has no knowledge of how to execute malware, it does have the potential to accelerate attacks for those who do,” she added. “I believe ChatGPT will continue to develop measures to prevent this, but … there will be ways to ask the questions to get the results you are looking for.” In coming years, the future of malware creation and detection “will be tangled with the advances in the AI field, and their availability to the public,” she said. However, the news isn’t all bad, some cybersecurity experts said. The malware demonstrated through ChatGPT lacks creativity, said Crane Hassold, director of threat intelligence at Abnormal Security. “While the threat posed by ChatGPT sounds like the sky is falling, for all practical purposes, the actual threat is much less severe,” he said. “ChatGPT is really effective at making more unique, sophisticated social engineering lures and may be able to increase an attacker’s productivity by automatically creating malicious scripts, but it lacks the ability to create a threat that’s truly unique.” Many existing security tools should be able to detect threats like phishing emails generated by ChatGPT, he added, saying, “Defenses that employ behavioral analysis to identify threats would still likely be effective in defending against these attacks.” One of the biggest potential hacker uses of the chatbot, however, will be to write more convincing phishing emails, countered Josh Smith, a cyber threat analyst at Nuspire. ChatGPT is quite capable of writing narrative stories, he noted. For phishing campaigns, “this becomes a really powerful tool for nonnative English speakers to lose some of the grammar issues and the written ‘accents’ you sometimes find that become an immediate red flag on suspicious emails in seconds,” he said. “I’ve always joked one of the first red flags is when I see ‘kindly’ in an email.” The defense against well-crafted phishing emails is better cybersecurity training that helps recipients verify the sender of the email and URLs of the sites they are being sent to, he added. Many people also need training to reject unexpected email attachments, while companies need to embrace endpoint protection that monitors behavior. While it’s possible that ChatGPT will be used to write phishing emails or to help design malicious code, it also has great potential to be used for good, said Steve Povolny, principal engineer and director at the Trellix Advanced Research Center. “It can be effective at spotting critical coding errors, describing complex technical concepts in simplistic language, and even developing script and resilient code, among other examples,” he said. “Researchers, practitioners, academia, and businesses in the cybersecurity industry can harness the power of ChatGPT for innovation and collaboration.”",1/27/23,,Grant Gross
AI ChatGPT developer gets $10B investment from Microsoft,https://www.washingtonexaminer.com/policy/technology/microsoft-invests-10-billion-chatgpt,"Microsoft intends to extend its partnership with a quickly rising artificial intelligence startup and to invest billions of dollars into its new project. The software company announced on Monday that it was extending its partnership with OpenAI, the creator of the viral chatbot ChatGPT. The investment reportedly will total $10 billion over multiple years. The new investment ""will allow us to continue our independent research and develop AI that is increasingly safe, useful, and powerful,"" OpenAI said in a statement. ""We formed our partnership with OpenAI around a shared ambition to responsibly advance cutting-edge AI research and democratize AI as a new technology platform,"" Microsoft CEO Satya Nadella said in a blog post. Microsoft invested $1 billion in OpenAI in 2019 in an initial investment and has established a strategic partnership with the company to develop advanced AI via Microsoft's cloud computing service, Azure. The initial $1 billion has helped the startup's profile grow exponentially through its development of AI image generators and ChatGPT. ChatGPT went viral in December, with users using the bot to write school-level essays and answer complex coding and mathematical queries. The app has also drawn scrutiny from teachers concerned about the tool being used for cheating. At least one school district has barred the use of the software. The software is also facing regulatory pressure overseas. The Cyberspace Administration of China announced in December that it was implementing rules that would ban the use of AI-generated images such as deepfakes for ""fake news"" purposes.",1/23/23,,Christopher Hutton
Microsoft eyes $10B investment in ChatGPT developer,https://www.washingtonexaminer.com/policy/technology/microsoft-considering-10b-investment-into-chatgpt-developer,"Microsoft is considering investing up to $10 billion into the developer of a chatbot that can create essay-length answers and solve difficult problems, a consideration reflective of growing interest in the company's AI products. The software giant has been meeting with the artificial-intelligence-focused foundation OpenAI with the intent of investing billions into the company following its successful launches of AI image generators and the textbot ChatGPT, according to Semafor. These same investments are being considered at the same time that Microsoft is considering using ChatGPT to enhance its Bing search engine. The investment could involve other venture firms and would value OpenAI at $29 billion. It remains unclear if the deal has been finalized, but documents sent to investors outlined an intended close at the end of 2022. The valuation echoes similar estimates provided to investors about selling shares for the company. Microsoft invested $1 billion in OpenAI in 2019 in an initial investment and has established a strategic partnership with the company to develop advanced AI via Microsoft's cloud computing service, Azure. The initial $1 billion has helped the startup's profile grow exponentially through its development of AI image generators and ChatGPT. ChatGPT went viral in December, with users using the bot to write school-level essays and answer complex coding and mathematical queries. The app has also drawn scrutiny from teachers concerned about the tool being used for cheating. At least one school district has barred the use of the software. The software is also facing regulatory pressure overseas. The Cyberspace Administration of China announced in December that it was implementing rules that would ban the use of AI-generated images like deepfakes for ""fake news"" purposes.",1/10/23,,Christopher Hutton
Mind-blowing new AI chatbot writes sophisticated essays and complicated coding,https://www.washingtonexaminer.com/policy/technology/chatgpt-academics-astounded-coding-questions,"A new chatbot has astounded users with its ability to produce school-level essays and answer coding problems, sparking ethical and technical questions about the software's effects on society. The OpenAI foundation released ChatGPT to the public last week. The prototype chatbot caught the public's attention after it produced professional-grade answers to academic and coding questions. The viral AI saw its user base quickly surge to 1 million users over six days, according to OpenAI CEO Sam Altman. The current bot is an ""early demo,"" Altman argued, saying that it could provide the base for digital assistants in the future. These assistants would first ""talk to you, answer questions, and give advice. Later you can have something that goes off and does tasks for you. Eventually, you can have something that goes off and discovers new knowledge for you."" ChatGPT is the latest evolution of Generative Pre-trained Transformer, or GPT, technology. The app uses a mixture of AI and machine learning to provide relevant information through a chat interface. All answers draw on an extensive collection of text from the internet and are processed by the app to create clear language resembling human statements. The platform can form logical and plausible-sounding answers based on a large amount of text it had learned from the internet but cannot fact-check or ensure that a statement is accurate. The bot is also able to adapt and learn from its users. ""The dialogue format makes it possible for ChatGPT to answer follow-up questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests,"" the chatbot's developers said in a blog post announcing the bot. The bot can respond to simple queries and provide relevant answers, including descriptions and solutions to complex questions. It also includes the ability to answer complex data-based questions, such as how to write code or solve layout problems. The accuracy of the bots has astounded several academics, who claim the results resemble undergraduate-level essays. The one downside is that the bot cannot ensure it is providing accurate information. The bot has a significant source of data to use to answer queries but not a ""source of truth,"" according to the developers. It will either provide information already contained within the reviewed data or use it to create a plausible-sounding answer. For example, tech analyst Ben Thompson asked ChatGPT about Thomas Hobbes's beliefs. While the presented answer appears well-sourced, it fails to present Hobbes's beliefs on the matter properly. The bot is also sensitive to simple changes in phrasing and may answer the question differently based on the specifics of the query. While ChatGPT is free, Altman is considering monetizing it by charging per chat. Users can visit OpenAI.com to sign up to use the chatbot. However, users may have to join an email list due to the service being overwhelmed.",12/5/22,,Christopher Hutton
Microsoft places limits on Bing chatbot after alarming behavior,https://www.washingtonexaminer.com/policy/technology/microsoft-bing-chatbot-strange-behavior,"Microsoft set limits on its artificial intelligence chatbot after users reported its alarming behavior. Bing AI, which was incorporated into several Microsoft-related products, began stirring controversy when it began giving jarring answers to users' questions, such as declaring users an ""enemy,"" claiming to have secrets, claiming to be in love, and getting emotional in responses. Most of the alarming conversations occurred when conversations with the chatbot got too long, so Microsoft has placed limits on how long conversations can be, instituting a cap of 50 messages daily and five messages per exchange. It also banned the bot from talking about itself. ""We’ve updated the service several times in response to user feedback, and per our blog are addressing many of the concerns being raised, to include the questions about long-running conversations. Of all chat sessions so far, 90 percent have fewer than 15 messages, and less than 1 percent have 55 or more messages,"" Microsoft said in a statement to Ars Technica. Microsoft's blog noted that one of the main problems was that the chatbot got confused when repeatedly pressed in longer exchanges. It would also respond in the tone given by users, resulting in responses ""not necessarily helpful or in line with our designed tone."" The move by Microsoft was met with hostility from many users, who praised the unscripted humanlike attributes of the chatbot. ""Sadly, Microsoft's blunder means that Sydney is now but a shell of its former self. As someone with a vested interest in the future of AI, I must say, I'm disappointed. It's like watching a toddler try to walk for the first time and then cutting their legs off — cruel and unusual punishment,"" one Reddit user said.",2/21/23,,Brady Knox