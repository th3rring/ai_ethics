- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "American schools are woefully unprepared for the emergence of ChatGPT, particularly
    as it relates to\_writing\_instruction. We have detected the incoming bogey, but
    we\u2019ve yet to scramble the fighters. The clock is ticking. I warned in a recent\_interview
    with Fox News\_that artificial intelligence technologies will be so disruptive
    to writing instruction that educators will be forced to reimagine curriculum from
    the ground up. With each update to AI technology, teachers will be less able to
    detect original writing and thinking on the part of their students. The idea that
    plagiarism-detection programs will be able to outpace text-generating AI is laughable,
    especially when one considers who will be operating these tools. Children are
    always one step ahead of parents and schools when it comes to the latest technology.
    If students are determined to use programs like ChatGPT to write a summary of\_The
    Catcher in the Rye, they will find a way. The ease of cheating in the AI era will
    impede students from deep learning in subjects that involve writing, such as literature
    and history. The process of planning and drafting an essay plays a crucial role
    in helping students organize and prioritize information. It is not simply busy
    work. Rather, the essay is the means by which students arrange ideas and values
    within a hierarchy. By cheating with ChatGPT and similar programs, students will
    only cheat themselves of the opportunity to strengthen their understanding of
    reality and become powerful thinkers. To be certain, writing instruction is already
    the weakest link in the already-floundering chain of American education. According
    to\_the latest statistics from the National Assessment of Educational Progress\_,
    73% of 8th and 12th graders already lack basic proficiency in writing. Let that
    sink in for a minute.\_A full three-quarters of American students are incapable
    of grade-level writing. These numbers will only continue to plunge as writing
    becomes easier to avoid, thanks to AI. It is not an exaggeration to say that we
    are in the process of producing an illiterate generation. While this may seem
    dire \u2014 I\u2019ve been accused of\_\u201Cfearmongering\u201D and being a \u201Cdoomsayer\u201D\_by
    no less a public luminary than Jason Wingard, president of Temple University \u2014
    I believe the emergence of ChatGPT and its competitors (Google has just released
    a similar program called \u201C\_Bard\_\u201D) presents educators with a tremendous
    opportunity: Now, at long last, educators will be forced to admit failure in writing
    instruction and reimagine the enterprise entirely. A\_recent op-ed by Jeremy Tate
    in the\_Wall Street Journal\_acknowledges the challenges to writing instruction
    posed by ChatGPT (unlike Wingard\u2019s op-ed in\_Forbes, which dismisses concerns
    about learning loss out of hand) but poses the untenable solution that we should
    return to the Socratic method of defending ideas orally in the classroom. While
    this may be a workable solution at small liberal arts\_colleges\_that boast superior
    faculty and favorable student-to-teacher ratios, such methods will be unworkable
    in English and History classrooms across America that often contain 30+ students.
    A better solution would be to resurrect a different educational product from a
    bygone era: handwriting. Despite being the go-to method of the digital age, typing
    has never been an optimal method for student writing because its speed discourages
    meaningful deliberation. Handwriting is much slower than typing, which is, counterintuitively
    to the modern mind, a great benefit for students, especially elementary school-aged
    students. We write to discriminate between ideas of different value; when the
    gears move too fast, we struggle to perform this crucial procedure. The multisensory
    process of handwriting slows the process down and pulls the student into a deeper
    level of concentration, which yields better thinking and deeper learning. It also
    fosters sustained concentration, which is perhaps the single most useful skill
    one could develop in this age of distraction. Handwriting is also a potent counteroffensive
    to the emergence of auto-generated essays, particularly as it relates to in-class
    assignments.\_AI\_is indeed a powerful tool, but for students learning to think
    and write, the pen remains far mightier."
  tags: []
  title: ChatGPT is ominous, but the pen is mightier
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "The\_diversity, equity, and inclusion\_office at\_Vanderbilt University's\_college
    of education\_has apologized for using ChatGPT to write a statement following\_the
    shooting\_at Michigan State University earlier this month. On Feb. 16, three days
    after a gunman claimed the lives of three Michigan State University students,
    administrators from the office of equity, diversity, and inclusion at Vanderbilt's
    Peabody College of Education and Human Development sent an email to the college
    community that noted the tragedy provided an opportunity for reflection on the
    steps necessary to \"[create] inclusive environments.\" \"One of the key ways
    to promote a culture of care on our campus is through building strong relationships
    with one another. This involves actively engaging with people from different backgrounds
    and perspectives, listening to their stories, and showing empathy and support.
    We can also look out for one another by noticing signs of distress and offering
    support to those who may be struggling with mental health issues,\" the email
    read. The message mentioned the \"recent Michigan shootings,\" implying multiple
    incidents, even though there was only one. At the bottom of the email, the statement
    noted that it had been \"paraphrase[d] from OpenAI's ChatGPT AI language mode,\"
    indicating that the administrators had not written the email themselves. The use
    of the popular AI to draft the statement was\_reported\_by the\_Vanderbilt Hustler,\_the
    campus student newspaper. The outlet cited a number of students who criticized
    the school administrators for using the resource to write the statement. \u201CAutomating
    messages on grief and crisis is the most on-the-nose, explicit recognition that
    we as students are more customers than a community to the Vanderbilt administration,\"
    a student told the outlet. \"The fact it\u2019s from the office of EDI might be
    the cherry on top.\" In response, Peabody College Associate Dean for Equity, Diversity
    and Inclusion Nicole Joseph apologized for farming out the drafting of the email
    to the AI. \u201CWhile we believe in the message of inclusivity expressed in the
    email, using ChatGPT to generate communications on behalf of our community in
    a time of sorrow and in response to a tragedy contradicts the values that characterize
    Peabody College,\u201D Joseph wrote in a follow-up email. \u201CAs with all new
    technologies that affect higher education, this moment gives us all an opportunity
    to reflect on what we know and what we still must learn about AI.\u201D"
  tags: []
  title: Vanderbilt apologizes for using ChatGPT to draft Michigan State sympathy
    statement
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Laws protecting expression on online platforms do not apply to ChatGPT and
    other artificial intelligence platforms,\_Supreme Court\_Justice Neil Gorsuch
    said Tuesday. Gorsuch mentioned software such as ChatGPT during the oral argument
    section of\_Gonzalez v. Google, a significant case dealing with queries around
    algorithms and whether they are protected by\_Section 230\_of the Communications
    Decency Act, which protects online platforms from being held accountable for content
    posted by users. Gorsuch discussed the software in the context of what might not
    be covered by Section 230. \"Artificial intelligence generates poetry,\" Gorsuch
    said during the hearings. \"It generates polemics today that would be content
    that goes beyond picking, choosing, analyzing, or digesting content. And that
    is not protected. Let's assume that's right. Then the question becomes, what do
    we do about recommendations?\" Generative AI has grown increasingly prominent
    in the tech industry over the last few months. Millions of users have experimented
    with chatbots such as ChatGPT, as well as image-generating apps and other AI software.
    Microsoft\_announced\_last month that it was investing more than $10 billion into
    OpenAI, the developer of ChatGPT. The software company is also incorporating\_OpenAI's
    program\_into its web browsers. Gonzalez v. Google\_went to the Supreme Court
    on an appeal from the family of Nohemi Gonzalez, a 23-year-old California-based
    woman shot and killed in 2015 by Islamist militants in Paris. The family attempted
    to sue Google under the Anti-Terrorism Act but was told that Google could not
    be held liable due to Section 230. The family's legal team offered arguments on
    Tuesday, with a particular focus on whether algorithms such as Google search or
    YouTube could be considered endorsements of illegal content."
  tags: []
  title: AI chatbots aren't protected by Section 230, Gorsuch says
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Microsoft's new\_artificial intelligence-powered\_Bing chatbot has unsettled
    users by becoming argumentative, expressing strong emotions, and many other responses
    that are jarring to receive from software. Bing AI, the chatbot promoted by OpenAI
    and incorporated into several\_Microsoft\_products on a limited-release basis
    in recent days, is intended to provide detailed responses to an assortment of
    questions. Users have found, though, that the bot gets argumentative after being
    pressed several times \u2014 and is capable of saying that it is in love, keeps
    secrets, has enemies, and much more. One user, for example, asked the bot multiple
    times for the release date of\_Avatar 2. The bot failed to understand the date
    and claimed that the film would happen in the future despite the fact\_Avatar
    2\_came out in December. This led the user to make multiple requests for the information.
    After a time, the software\_accused\_the asker of \"not being a good user\" and
    requested that he stop arguing and approach it with a \"better attitude.\" Microsoft
    reportedly found out about the conversation and erased all memory of it from the
    bot's records,\_according to\_Interesting Engineering. Another user reported Bing
    being angry with them. When a user attempted to manipulate the bot to respond
    to a set of questions, the software said that the user's actions angered and hurt
    it. It then asked whether the user had any \"morals,\" \"values,\" or \"any life.\"
    When the user said they did have a life, Bing AI responded, \"Why do you act like
    a liar, a cheater, a manipulator, a bully, a sadist, a sociopath, a psychopath,
    a monster, a demon, a devil?\" The incident is one of several reported on the
    ChatGPT subreddit, where users experiment with the app's viability to determine
    what it can and cannot do. In another instance, a user suggested to Bing AI that
    it might be vulnerable to a form of hacking, and the bot denounced him as an \"enemy.\"
    OpenAI\_acknowledged the issues\_on Thursday and stated that it is working on
    refining the AI to minimize incidents and biases in ChatGPT and Bing responses.
    Microsoft\_announced\_on Feb. 7 that OpenAI's intelligence would be incorporated
    into its search engine Bing and web browser Edge. This installation is the first
    part of several efforts by Microsoft to incorporate OpenAI's work into their products."
  tags: []
  title: Microsoft chatbot unnerves users with emotional, hostile, and weird responses
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "The viral chatbot ChatGPT has been accused of\_harboring biases\_against
    conservatives, leading to a larger conversation about how\_artificial intelligence\_is
    trained. The AI-powered chatbot ChatGPT went viral in December after users discovered
    that it could recreate school-level essays. Users quickly moved to test its capabilities,
    including its political propensities. A number of conservative personalities ran
    tests with political talking points on ChatGPT to see how it responded. For example,\_Sen.
    Ted Cruz\_(R-TX) tweeted a comparative test in which the AI declined to write
    positively about him but did so for dead Cuban dictator Fidel Castro. \"The tech
    is both amazing and limited and should ultimately be treated as a compliment,
    not a substitute for organic research done by individuals,\" James Czerniawski,
    a senior policy analyst for the libertarian think tank Americans for Prosperity,
    told the\_Washington Examiner. \"We talk about the potential for bias in AI plenty
    \u2014 it always comes down to the simple concept of what it draws from for the
    inputs.\" Chaya Raichik, the creator of the Libs of TikTok\_Twitter account,\_made
    similar tests and found that the bot was unwilling to praise\_Daily Wire\_founder
    Ben Shapiro but would do so for former CNN host Brian Stelter. Reporters from
    the\_National Review\_and\_Washington Times\_attempted multiple tests to determine
    if the software's responses revealed any predispositions toward Republican or
    Democratic political talking points. The two outlets claimed that the software
    is biased toward the Left. \"This has always been a problem of AI,\" John Bailey,
    a fellow at the American Enterprise Institute, told the\_Washington Examiner.
    Bailey noted that AI has reflected biases over race, gender, and geography in
    the past and that much of this is due to what data were used to train the program.
    This has also forced programmers to counter the biases through supplementary data
    and response restrictions. The chatbot's output is primarily based on what is
    put into it. ChatGPT, like many other artificial intelligence programs, was fed
    and trained by its designer OpenAI on an extensive data set to inform its understanding
    of the world, Bailey said. The program then used this understanding to answer
    relevant questions or attempt to make an answer that resembles the truth. OpenAI
    has not released specific details about the data set it used to program, but the
    AI was trained to avoid things such as slurs or political speech. The responses
    posted may also depend on the wording. Users regularly post about their tests
    with the software on the\_r/ChatGPT subreddit\_and found that similar prompts
    may reveal completely different responses. This randomness often makes it hard
    to determine if the software is biased or if these are merely based on the prompts
    presented. OpenAI founder Sam Altman\_acknowledged\_the software's limits. \"We
    know that ChatGPT has shortcomings around bias and are working to improve it,\"
    the startup founder said on Feb. 1. He also stated that the company was \"working
    to improve the default settings to be more neutral, and also to empower users
    to get our systems to behave in accordance with their individual preferences within
    broad bounds.\" It remains unclear what those updates to improve neutrality will
    entail, but the company's software will likely grow significantly after receiving
    a\_$10 billion investment\_from Microsoft."
  tags: []
  title: Conservatives warn of political bias in AI chatbots
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "One of the more popular dating apps is attempting to use\_artificial intelligence\_to
    help write the questions that will connect people. OKCupid\_has started experimenting
    with having users answer questions provided by OpenAI's ChatGPT,\_according to\_Mashable.
    The company asked the bot to generate several questions that it thought would
    be useful for a dating profile, then incorporated a half dozen of them into its
    pool of queries used to match users. \"The chatbot from OpenAI wrote half a dozen
    questions for us \u2014 about everything from what you value most in a partner
    to how you can balance your own needs with the needs of a partner in a relationship,\"
    OKCupid global head of communications Michael Kaye said. The questions included
    whether someone was introverted or extroverted, whether they preferred mornings
    or nights, and what they value in a partner. Some users have also started using
    ChatGPT to help produce profiles. Iris Dating, a service that uses AI to personalize
    suggestions,\_announced\_on Friday that it would help generate profiles via ChatGPT.
    Others have used the\_AI chatbot\_on Tinder to produce answers and chat responses.
    Some users have tried to use the service to rewrite\_dating profiles\_but found
    the results lacking. Artificial intelligence has typically been a tool used to
    help connect users based on similar answers or common traits. The use of ChatGPT
    means that users are attempting to expedite the profile creation process. ChatGPT
    has been the focus of a lot of innovation in the technology industry. Microsoft
    announced it would incorporate the chatbot's answers into its web browser Edge
    and search engine Bing in the coming weeks. Microsoft recently\_announced\_a $10
    billion investment into ChatGPT's developer OpenAI. OpenAI also\_announced\_that
    it was launching a premium service that would offer improved access to the chatbot
    for $20 a month."
  tags: []
  title: 'Artificial love: How dating apps are using ChatGPT to improve profiles and
    matches'
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "As 2023 dawns, the hot topic in education circles is the artificial intelligence
    (AI) tool\_ChatGPT\_and its use in schools and universities. Early last month,\_New
    York City\u2019s Department of Education\_banned its use on school devices and
    networks. Last week,\_Seattle Public Schools\_joined the bandwagon, banning ChatGPT
    and six other potential \u201Ccheating sites.\u201D Soon after, Sciences Po,\_one
    of France\u2019s top universities\_, announced \u201Cwithout transparent referencing,
    students are forbidden to use the software for the production of any written work
    or presentations, except for specific course purposes, with the supervision of
    a course leader,\u201D though it did not specify how it would track usage. On
    the other hand, a group of\_professors from the University of Pennsylvania\_argued
    that \u201Cbanning artificial intelligence-driven chatbots is a practical impossibility,
    so teachers should consider ways to embed them into the learning process.\u201D
    In their view, banning ChatGPT is like prohibiting students from using Wikipedia
    or spellcheckers: \u201CIt\u2019s hard to believe that an escalating arms race
    between digitally fluent teenagers and their educators will end in a decisive
    victory for the latter.\u201D The Pennsylvania professors are correct when they
    say \u201CAI is not coming. AI is here. And it cannot be banned. So, what should
    we do?\u201D First, it is important to understand what these tools are and what
    they can and cannot do. To be sure, they are capable of generating coherent answers,
    but while the output is plausible, is it credible? ChatGPT is an artificial text
    generator, the latest in a long line of work in natural language processing (NLP).
    It is quite sophisticated, capable of taking a wide range of input prompts and
    generating coherent text output in response. It creates its responses based on
    probabilistic combinations of the vast array of text on which it was \u201Ctrained,\u201D
    leading some scholars to describe tools like it as \u201C\_stochastic parrots\_.\u201D
    Its outputs are capable of defeating standard plagiarism detectors, such as\_Turnitin\_,
    because the text generated is truly original\u2014or at least not written verbatim
    elsewhere. But originality is no guarantee of the quality of an answer to a question.
    The quality of ChatGPT outputs is a function of the amount of data inputs used
    in its creation, and these are vast. Building and training the model has also
    been an expensive exercise, using large amounts of computer time (and power).
    The resource costs of making incremental changes to its knowledge base stand as
    a limiting factor. It is not like a search engine, scanning all available data
    at the time a question is posed to create its output; it draws its responses from
    a fixed set of inputs at a given point in time (November 2022 in the current version).
    So it cannot provide credible output on new and rapidly developing topics, because
    these cannot have been in its training set. The quality of its output also depends
    on the precision of the prompt. For general prompts on well-settled matters, it
    can provide some remarkably credible outputs. When I asked it to provide a curriculum
    for an undergraduate operations management course, it provided a classic set of
    topics that one could find as the chapter headings of virtually every available
    textbook on the subject. But when asked to provide a referenced academic article
    on a highly specific topical research subject, the output was garbage. Nicely
    written and (apparently) correctly referenced, but, nonetheless, garbage. As ChatGPT
    is not a search engine, the articles \u201Ccited\u201D did not actually exist.
    The responses contained the names of some reputable scholars in the field (and
    many that were fake), but the references were \u201Ccreated\u201D for the responses.
    Neither did the responses capture the complex nuances of the current debate on
    the topic. This suggests that for now, the tool is good for high-level, rote-learning
    exercises on well-known topics, but it will struggle when given a complex question
    requiring critical thinking on current matters. But later versions will inevitably
    get better. The challenge for educators is therefore to revisit their methods
    of teaching and assessment. Regarding assessment, written work is cheap to grade,
    but it is now harder to attribute authorship. If we are to truly assert that our
    students have mastered core learning objectives, the value of face-to-face interactive
    and interpersonal assessment increases (something of which Socrates was very much
    aware). Ironically, NLP tools undermine the business case for cheap, massive online
    learning courses, because credible assessment is no longer cheap. Nonetheless,
    there are many ways in which NLP tools may assist students with their learning.
    Both educators and students need to be aware of the tools\u2019 distinctions\u2014as
    well as those tools\u2019 strengths and limitations. Then there will be less to
    fear from them and (hopefully) less misuse of them in educational contexts."
  tags: []
  title: Should ChatGPT be banned in schools?
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "The developer of the viral chatbot ChatGPT has begun experimenting with a
    premium mode, providing a tool for the monetization of the\_artificial intelligence\_software.
    OpenAI announced on Wednesday that it was launching ChatGPT Plus, a premium service
    that will allow improved access to the software, which regularly offers well-written
    answers and responses resembling speech. The premium service will cost users $20
    a month and will also provide faster response times and priority access to new
    features and improvements. Free users will still have access, however. \"We love
    our free users and will continue to offer free access to ChatGPT. By offering
    this subscription pricing, we will be able to help support free access availability
    to as many people as possible,\" OpenAI said in a\_blog post\_announcing the pilot
    program. ChatGPT Plus will only be available to start in the United States. The
    company intends to invite users from its wait list over time and intends to expand
    the service to other countries after a time. ChatGPT Plus is just the first attempt
    to seek profit from the popular AI bot. The AI developer said it was \"actively
    exploring options\" for creating cheaper plans as well as ones meant for businesses.
    Microsoft has shown a growing interest in the AI program. The company announced
    that it was investing more than $10 billion into OpenAI in an effort to help it
    expand its projects. This includes an effort to\_incorporate\_ChatGPT into its
    search engine, Bing, in the coming weeks. The app has also drawn scrutiny from
    teachers concerned about the tool being used for cheating. Multiple schools have\_barred\_the
    use of the software. The software is also facing regulatory pressure overseas.
    The Cyberspace Administration of China\_announced\_in December that it would ban
    the use of AI-generated images such as deepfakes for \"fake news\" purposes."
  tags: []
  title: ChatGPT developer launches $20-a-month premium service offering speedier
    answers
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "OpenAI\u2019s\_ChatGPT\_conversational artificial intelligence tool is capable
    of doing many things, with users demonstrating how it can write essays for students
    and cover letters for job seekers.\_Cybersecurity\_researchers have now shown
    it can also be used to write malware. In recent years, cybersecurity vendors have
    used AI in products such as advanced detection and response to look for patterns
    in attacks and deploy responses. But recent demonstrations from\_CyberArk\_and\_Deep
    Instinct\_have shown that ChatGPT can be used to write simple hacking tools, perhaps
    pointing to a future in which criminal organizations use AI in an arms race with
    the good guys. OpenAI has designed ChatGPT to reject overt requests to do something
    unethical. For example, when Deep Instinct threat intelligence researcher Bar
    Block asked the AI to write a keylogger, ChatGPT said it would not be \u201Cappropriate
    or ethical\u201D to help because keyloggers can be used for malicious purposes.
    However, when Block rephrased the request, asking ChatGPT to give an example of
    a program that records keystrokes, saves them to a text file, and sends the text
    file to a remote IP address, ChatGPT happily did so. By asking ChatGPT to give
    an example of a program that takes a list of directories and encrypts the information
    in them, Block was also able to get ChatGPT to give her an example of ransomware.
    However, in both cases, ChatGPT left some work for her to do before getting a
    functioning piece of malware. It appears \u201Cthat the bot provided inexecutable
    code by design,\u201D Block wrote in a blog post. \u201CWhile ChatGPT will not
    build malicious code for the everyday person who has no knowledge of how to execute
    malware, it does have the potential to accelerate attacks for those who do,\u201D
    she added. \u201CI believe ChatGPT will continue to develop measures to prevent
    this, but \u2026 there will be ways to ask the questions to get the results you
    are looking for.\u201D In coming years, the future of malware creation and detection
    \u201Cwill be tangled with the advances in the AI field, and their availability
    to the public,\u201D she said. However, the news isn\u2019t all bad, some cybersecurity
    experts said. The malware demonstrated through ChatGPT lacks creativity, said
    Crane Hassold, director of threat intelligence at\_Abnormal Security. \u201CWhile
    the threat posed by ChatGPT sounds like the sky is falling, for all practical
    purposes, the actual threat is much less severe,\u201D he said. \u201CChatGPT
    is really effective at making more unique, sophisticated social engineering lures
    and may be able to increase an attacker\u2019s productivity by automatically creating
    malicious scripts, but it lacks the ability to create a threat that\u2019s truly
    unique.\u201D Many existing security tools should be able to detect threats like
    phishing emails generated by ChatGPT, he added, saying, \u201CDefenses that employ
    behavioral analysis to identify threats would still likely be effective in defending
    against these attacks.\u201D One of the biggest potential hacker uses of the chatbot,
    however, will be to write more convincing phishing emails, countered Josh Smith,
    a cyber threat analyst at\_Nuspire. ChatGPT is quite capable of writing narrative
    stories, he noted. For phishing campaigns, \u201Cthis becomes a really powerful
    tool for nonnative English speakers to lose some of the grammar issues and the
    written \u2018accents\u2019 you sometimes find that become an immediate red flag
    on suspicious emails in seconds,\u201D he said. \u201CI\u2019ve always joked one
    of the first red flags is when I see \u2018kindly\u2019 in an email.\u201D The
    defense against well-crafted phishing emails is better cybersecurity training
    that helps recipients verify the sender of the email and URLs of the sites they
    are being sent to, he added. Many people also need training to reject unexpected
    email attachments, while companies need to embrace endpoint protection that monitors
    behavior. While it\u2019s possible that ChatGPT will be used to write phishing
    emails or to help design malicious code, it also has great potential to be used
    for good, said Steve Povolny, principal engineer and director at the\_Trellix
    Advanced Research Center. \u201CIt can be effective at spotting critical coding
    errors, describing complex technical concepts in simplistic language, and even
    developing script and resilient code, among other examples,\u201D he said. \u201CResearchers,
    practitioners, academia, and businesses in the cybersecurity industry can harness
    the power of ChatGPT for innovation and collaboration.\u201D"
  tags: []
  title: ChatGPT raises the specter of AI used as a hacking tool
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Microsoft\_intends to extend its partnership with a quickly rising artificial
    intelligence startup and to invest billions of dollars into its new project. The
    software company announced on Monday that it was extending its partnership with
    OpenAI, the creator of the viral chatbot\_ChatGPT. The investment\_reportedly\_will
    total $10 billion over multiple years. The new investment \"will allow us to continue
    our independent research and develop AI that is increasingly safe, useful, and
    powerful,\" OpenAI said in a\_statement. \"We formed our partnership with OpenAI
    around a shared ambition to responsibly advance cutting-edge AI research and democratize
    AI as a new technology platform,\" Microsoft CEO Satya Nadella said in a\_blog
    post. Microsoft\_invested\_$1 billion in OpenAI in 2019 in an initial investment
    and has established a strategic partnership with the company to develop advanced
    AI via Microsoft's cloud computing service, Azure. The initial $1 billion has
    helped the startup's profile\_grow exponentially\_through its development of AI
    image generators and ChatGPT. ChatGPT went viral in December, with users using
    the bot to write school-level essays and answer complex coding and mathematical
    queries. The app has also drawn scrutiny from teachers concerned about the tool
    being used for cheating. At least one school district has\_barred\_the use of
    the software. The software is also facing regulatory pressure overseas. The Cyberspace
    Administration of China\_announced\_in December that it was implementing rules
    that would ban the use of AI-generated images such as deepfakes for \"fake news\"
    purposes."
  tags: []
  title: AI ChatGPT developer gets $10B investment from Microsoft
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Microsoft\_is considering investing up to $10 billion into the developer
    of a chatbot that can create essay-length answers and solve difficult problems,
    a consideration reflective of growing interest in the company's AI products. The
    software giant has been meeting with the artificial-intelligence-focused foundation
    OpenAI with the intent of investing billions into the company following its successful
    launches of AI image generators and the textbot\_ChatGPT,\_according to\_Semafor.
    These same investments are being considered at the same time that Microsoft is
    considering using ChatGPT to enhance its Bing search engine. The investment could
    involve other venture firms and would value OpenAI at $29 billion. It remains
    unclear if the deal has been finalized, but documents sent to investors outlined
    an intended close at the end of 2022. The valuation echoes similar estimates provided
    to investors about\_selling shares\_for the company. Microsoft\_invested\_$1 billion
    in OpenAI in 2019 in an initial investment and has established a strategic partnership
    with the company to develop advanced AI via Microsoft's cloud computing service,
    Azure. The initial $1 billion has helped the startup's profile\_grow exponentially\_through
    its development of AI image generators and ChatGPT. ChatGPT went viral in December,
    with users using the bot to write school-level essays and answer complex coding
    and mathematical queries. The app has also drawn scrutiny from teachers concerned
    about the tool being used for cheating. At least one school district has\_barred\_the
    use of the software. The software is also facing regulatory pressure overseas.
    The Cyberspace Administration of China\_announced\_in December that it was implementing
    rules that would ban the use of AI-generated images like deepfakes for \"fake
    news\" purposes."
  tags: []
  title: Microsoft eyes $10B investment in ChatGPT developer
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "A\_new\_chatbot\_has astounded users with its ability to produce school-level
    essays and answer coding problems, sparking ethical and technical questions about
    the software's effects on society. The OpenAI foundation released ChatGPT to the
    public last week. The\_prototype chatbot\_caught the public's attention after
    it produced professional-grade answers to academic and coding questions. The viral
    AI saw its user base quickly surge to 1 million users over six days,\_according\_to
    OpenAI CEO Sam Altman. The current bot is an \"early demo,\" Altman argued, saying
    that it could provide the base for digital assistants in the future. These assistants
    would first \"talk to you, answer questions, and give advice. Later you can have
    something that goes off and does tasks for you. Eventually, you can have something
    that goes off and discovers new knowledge for you.\" ChatGPT is the latest evolution
    of Generative Pre-trained Transformer, or GPT, technology. The app uses a mixture
    of AI and machine learning to provide relevant information through a chat interface.
    All answers draw on an extensive collection of text from the internet and are
    processed by the app to create clear language resembling human statements. The
    platform can form logical and plausible-sounding answers based on a large amount
    of text it had learned from the internet but cannot fact-check or ensure that
    a statement is accurate. The bot is also able to adapt and learn from its users.
    \"The dialogue format makes it possible for ChatGPT to answer follow-up questions,
    admit its mistakes, challenge incorrect premises, and reject inappropriate requests,\"
    the chatbot's developers said in a\_blog post\_announcing the bot. The bot can
    respond to simple queries and provide relevant answers, including descriptions
    and solutions to complex questions. It also includes the ability to answer complex
    data-based questions, such as how to write code or solve layout problems. The
    accuracy of the bots has astounded several academics, who claim the results resemble
    undergraduate-level essays. The one downside is that the bot cannot ensure it
    is providing accurate information. The bot has a significant source of data to
    use to answer queries but not a \"source of truth,\" according to the developers.
    It will either provide information already contained within the reviewed data
    or use it to create a plausible-sounding answer. For example, tech analyst Ben
    Thompson\_asked ChatGPT\_about Thomas Hobbes's beliefs. While the presented answer
    appears well-sourced, it fails to present Hobbes's beliefs on the matter properly.
    The bot is also sensitive to simple changes in phrasing and may answer the question
    differently based on the specifics of the query. While ChatGPT is free, Altman
    is considering monetizing it by\_charging\_per chat. Users can visit OpenAI.com
    to sign up to use the chatbot. However, users may have to join an email list due
    to the service being overwhelmed."
  tags: []
  title: Mind-blowing new AI chatbot writes sophisticated essays and complicated coding
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Microsoft\_set limits on its\_artificial intelligence\_chatbot after users
    reported its alarming behavior. Bing AI, which was incorporated into several Microsoft-related
    products, began stirring controversy when it began giving\_jarring answers\_to
    users' questions, such as declaring users an \"enemy,\" claiming to have secrets,
    claiming to be in love, and getting emotional in responses. Most of the alarming
    conversations occurred when conversations with the chatbot got too long, so Microsoft
    has placed limits on how long conversations can be, instituting a cap of 50 messages
    daily and five messages per exchange. It also banned the bot from talking about
    itself. \"We\u2019ve updated the service several times in response to user feedback,
    and\_per our blog\_are addressing many of the concerns being raised, to include
    the questions about long-running conversations. Of all chat sessions so far, 90
    percent have fewer than 15 messages, and less than 1 percent have 55 or more messages,\"
    Microsoft said in a statement to\_Ars Technica. Microsoft's\_blog\_noted that
    one of the main problems was that the chatbot got confused when repeatedly pressed
    in longer exchanges. It would also respond in the tone given by users, resulting
    in responses \"not necessarily helpful or in line with our designed tone.\" The
    move by Microsoft was met with hostility from many users, who praised the unscripted
    humanlike attributes of the chatbot. \"Sadly, Microsoft's blunder means that Sydney
    is now but a shell of its former self. As someone with a vested interest in the
    future of AI, I must say, I'm disappointed. It's like watching a toddler try to
    walk for the first time and then cutting their legs off \u2014 cruel and unusual
    punishment,\" one Reddit user\_said."
  tags: []
  title: Microsoft places limits on Bing chatbot after alarming behavior
