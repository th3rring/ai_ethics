Title,Link,Body,Date,Notes,Author
Opinion | Why I’m not worried about my students using ChatGPT,https://www.washingtonpost.com/opinions/2023/02/06/college-students-professor-concerns-chatgpt/,"Lawrence Shapiro is a professor of philosophy at the University of Wisconsin-Madison.  ChatGPT has many of my university colleagues shaking in their Birkenstocks. This artificial-intelligence tool excels at producing grammatical and even insightful essays — just what we’re hoping to see from our undergraduates. How good is it, really? A friend asked ChatGPT to write an essay about “multiple realization.” This is an important topic in the course I teach on the philosophy of mind, having to do with the possibility that minds might be constructed in ways other than our own brains. The essay ran shorter than the assigned word count, but I would have given it an A grade. Apparently ChatGPT is good enough to create an A-level paper on a topic that’s hardly mainstream.  Universities are treating the threat as more dire than an epidemic or even a budget reduction. The most obvious response, and one that I suspect many professors will pursue, involves replacing the standard five-page paper assignment with an in-class exam. Others expect to continue with the papers but have suggested that the assigned topics should be revised to focus on lesser-known works or ideas about which a chatbot might not “know” too much.  Good luck with that. If ChatGPT can pen a solid essay on multiple realization, an issue on which I happen to be a world authority in good part thanks to lack of company, I doubt it would have difficulty constructing essays about lesser-known Shakespearean sonnets or unremarkable soldiers who fought for the Union Army. Besides, if we’re going to demand deep thought from our students, shouldn’t it be about the more important stuff?  Here’s what I plan to do about chatbots in my classes: pretty much nothing. Let me say first that as much as I value the substance of what I teach, realistically my students will not spend more than a semester thinking about it. It’s unlikely that Goldman Sachs or Leakey’s Plumbing or wherever my students end up will expect their employees to have a solid background in philosophy of mind. Far more likely is that the employees will be required to write a letter or an analysis or a white paper, and to do this they will need to know how to write effectively in the first place. This is the skill that I most hope to cultivate in my students, and I spend a lot of time reading their essays and providing them with comments that really do lead to improvements on subsequent assignments. In-class exams — the ChatGPT-induced alternative to writing assignments — are worthless when it comes to learning how to write, because no professor expects to see polished prose in such time-limited contexts.  I should emphasize just how desperately my students need formal instruction in writing. My wife confirms that I’m noticeably crankier than when I first started teaching 30 years ago. Everything today seems worse than it was back then: traffic, TV news, macaroni and cheese. But I don’t believe that the deterioration in writing quality that I see is a consequence of age-tinted glasses. I read too many papers from upperclassmen, from students who have taken other writing-intensive courses, in which only one sentence out of five is not grammatically or stylistically defective. I would be failing these students if I let ChatGPT discourage me from teaching them what might be the most essential competence they can gain from me.  But what about the cheaters, the students who let a chatbot do their writing for them? I say, who cares? In my normal class of about 28 students, I encounter one every few semesters whom I suspect of plagiarism. Let’s now say that the temptation to use chatbots for nefarious ends increases the number of cheaters to an (unrealistic) 20 percent. It makes no sense to me that I should deprive 22 students who can richly benefit from having to write papers only to prevent the other six from cheating (some of whom might have cheated even without the help of a chatbot).  Here’s an idea for extracting something positive from the inevitable prominence that chatbots will achieve in coming years. My students and I can spend some class time critically appraising a chatbot-generated essay, revealing its shortcomings and deconstructing its strengths. This exercise would bring a couple of rewards. First, analytical writing, like any skill, benefits from seeing examples of what works and what does not. While students might reasonably object to having their own essays made a target of public inspection, chatbots couldn’t possibly care. Second, given that chatbots are not going to fade away, my students might as well learn how to refine their products for whatever uses the future holds.  I urge my colleagues not to abandon writing assignments for fear that some students will let artificial intelligence do their work for them. Instead, let’s devise ways to make chatbots work for all of us. Truly, the cheaters are only hurting themselves — unless we respond to them by removing writing assignments from the syllabus. ",,,MISSING
Opinion | Here’s how teachers can foil ChatGPT: Handwritten essays,https://www.washingtonpost.com/opinions/2022/12/29/handwritten-essays-defeat-chatgpt/,"The era of deepfake authorship has arrived. Since the release in November of ChatGPT, the artificial-intelligence program has impressed, entertained and caused more than a little hand-wringing about its ability to produce coherent and credible pieces of writing. Much of the worry has focused on ChatGPT’s potential for powering fake news. But commentators have also worried about the toll AI-aided plagiarism could take on education. Teachers might soon find it impossible to detect AI-generated text. “The College Essay Is Dead,” the Atlantic declared. That’s unlikely. There are some obvious workarounds. For example, even laptop-equipped students wouldn’t benefit from ChatGPT if they were required to write essays in class without the aid of their phone or an internet connection. But there’s another fix — one that might have been worth implementing even before the arrival of ChatGPT: Make students write out essays by hand. Apart from outflanking the latest AI, a return to handwritten essays could benefit students in meaningful ways. For one thing, neuroscience research has revealed that, to the human brain, the act of handwriting is very different from punching letters on a keyboard. Handwriting requires precise motor skills — controlling the individual strokes and the pressure of the pen — that vary for each letter, and these stimulate greater activity in a broader group of brain regions when compared with typing. (Anyone who has ever helped a child learn to write will recognize how much concentration and practice it requires.) These letter-specific motor skills, coupled with subtle differences in other sensory input, engage the brain in ways that researchers have linked to learning and memory improvements. And those added layers of stimulation might be beneficial even when a student is merely copying an AI-written essay by hand. “Handwriting forces those areas responsible for memory and learning to communicate with each other, which helps form networks that can make it easier to recall or learn new information,” Audrey van der Meer, professor of neuropsychology at the Norwegian University of Science and Technology, told me. Much of the research comparing the differing neurological effects of handwriting and typing has focused on children or younger students. But there’s evidence that, even for older students and adults, writing by hand is a more cognitively involved process. For example, some work has found that writing by hand leads to better processing of ideas, and that students produce more original work when they complete assignments in longhand. Meanwhile, research on foreign-language learners has found that handwriting is associated with improvements in some measures of accuracy and comprehension. Especially when it comes to essay writing, producing something by hand is a fundamentally different task that writing it on a computer. When you’re writing by hand, you need to know where you’re going with a sentence — what you want it to say, and the structure it will take — before you begin. If you don’t, you’ll have to cross things out or start over. Typing on a computer requires far less forethought; you can dump out the contents of your brain and then hammer it into shape. The dump-and-edit method isn’t necessarily an inferior way to produce quality writing. But in many ways, it is less challenging for the brain — and challenging the brain is central to education itself. “Handwriting requires you to put a filter on what you’re producing in a way that typing doesn’t,” according to Karin H. James, a professor of psychological and brain sciences at Indiana University. A return to handwritten essays wouldn’t be easy for students. Schools have largely surrendered to a screen-dominated world, and the Common Core curriculum standards don’t mandate cursive training for grades K-12. Most secondary school students, never mind college kids, aren’t accustomed to writing longhand. It wouldn’t be easy on teachers either, who might have to reduce the length of assignments or allocate extra class time for completion. They’d also have the chore of reading sloppy text that wasn’t neatly turned out by a word processor. But some might find all that preferable to harboring the constant suspicion that they’re being outwitted by a bot. Toward the end of the 19th century, health issues forced the German philosopher Friedrich Nietzsche to abandon his pen in favor of a typewriter, a new invention at the time. Some of his friends noticed a change in his writing style — a change that one scholar later described as a departure from “sustained argument and prolonged reflection” to a terser “telegram style.” Nietzsche himself felt the change. “Our writing tools work on our thoughts,” he observed. Ensuring that today’s students have more than one writing tool at their disposal might pay off in ways experts are only beginning to grasp. ChatGPT and other AI-powered technologies will win only if we agree to play on their home turf.",,,MISSING
"So far, AI chatbots’ great talent is flooding inboxes",https://www.washingtonpost.com/opinions/2023/02/27/ai-chatbots-fiction-nonfiction-writing/,"Was it really only December when I first heard, at a conference, buzz about the new AI chatbot that was going to change the world? Usually, that sort of talk means there’s a good chance that, in a couple of years, I might discover some mildly useful new service. But in less than three months, ChatGPT and its near relations really have changed my world. Bing, Microsoft’s search engine, is adding chat features, and I’m using a different engine to do literature reviews. Professor friends are being flooded with machine answers on assignments and thinking about how to redesign coursework to make it unhackable. And the machines are already nibbling around the edges of my profession: Reuters reports that AI-generated books are popping up on Amazon, while the science-fiction magazine Clarkesworld just announced that it would temporarily close submissions because the slush pile was overwhelmed with machine-manufactured dreck. This is a major problem, though not exactly the one you might think I’d be complaining about: I’m not worried that artificial intelligence is coming for my job. Indeed, as I wrote a few months back, in the short term, I expect that AI will actually be good for established writers and outlets, precisely because it generates so much bad writing. The productivity of these AIs is astounding; in a few minutes they can pound out a thousand words that would have taken a human hours to write. But luckily, for those of us who already have jobs, AI quality is astoundingly bad. CNET and Men’s Journal experimented with AI-generated articles, only to find that they were riddled with errors, because AI doesn’t know or care what is true; it knows only what sort of thing its prediction engine tells it ought to come next in a sentence or paragraph. (The site Futurism helped identify the errors.) Unscrupulous people will nonetheless be happy to swamp the internet with this garbage, in hopes of attracting reader eyeballs long enough to sell ads. Readers drowning in unreliable ersatz content will probably learn to place more value on journalistic brand names with reputations for accuracy to defend. Our biggest problem, in the short term, is likely to be akin to what Clarkesworld is facing: Publicity agents armed with AIs and mailing lists will stuff our inboxes with even more inappropriate pitches. Yet if AI isn’t truthful enough to do good journalism, neither is it a good enough liar to write good fiction, as best-selling science fiction author John Scalzi pointed out on his blog. Current versions have no creative spark or deep understanding of human motivations; they serve up warmed-over pastiches of better authors, rendered in a prose style that seems to have been picked up from databases of regulatory filings. What, then, is the problem? Well, for one thing, this will make it harder for fiction and nonfiction outlets to find new talent. The internet created a lot of new pathways to success for nontraditional writers — 20 years ago, for instance, blogs helped me break into journalism, and Scalzi to break into fiction writing. Other writers have found success self-publishing on Amazon. But none of us had to swim through a boundless sea of AI-generated nonsense to reach editors or readers. In the longer term, I confess, I am less optimistic than Scalzi, who believes that “they just don’t have what it takes” to do his job, “and short of actual consciousness in the AI, may not ever.” AIs aren’t human (notwithstanding the lovelorn AI who begged a New York Times reporter to ditch his wife and run away with her). But I’m not sure they won’t quickly become very good at emulating humans in all the ways that readers care about. After all, it takes quite a while for us to learn how to emulate humans. Many of the funny errors made by AI strike me as similar to the funny things my parent friends report their kids saying — like AI, kids know a lot of facts and rules, but don’t necessarily have a good mental model for how everything should hang together. As for its larger flaws, even good young writers need time to develop their prose style, or master journalistic ethics. And unlike a young writer, AI can brute-force its way to reader-pleasing output. It can become human — or close enough — in roughly the same way humanity did, through endless evolution, except over the course of hours and days rather than millennia. The machines can test small changes over and over, and over and over and over, keeping what people like, jettisoning what we don’t. It may take them a lot of effort to attract sufficient human attention to make a good test. But of course, they’ll never get tired or bored, or decide to give up and go to law school. I expect this will take some time and, as I say, in the meantime, an established reputation will only become more valuable. Still, I wonder … how much, time, exactly?",,,MISSING
