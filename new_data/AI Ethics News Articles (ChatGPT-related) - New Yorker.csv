Title,Link,Body,Date,Notes,Author
I Have Questions for ChatGPT,https://www.newyorker.com/magazine/2023/03/13/i-have-questions-for-chatgpt,"ChatGPT enables users to ask questions or tell a story, and the bot will respond with relevant, natural-sounding answers and topics. —Quoted in Forbes. Hi, Chat, A friend gifted me a fancy designer bucket hat that she swore she didn’t want anymore. Then we had a misunderstanding, and she ghosted my birthday party. Then I blocked her. And put a potato in her tailpipe. And slept with her ex. Can our friendship be saved? If not, do I have to give back the hat? why are there suddenly so many different kinds of Oreos? What are Birthday Cake Flavor Creme Oreos really like? Occasionally sampling a blueberry in the produce section is one thing—and, before you say a word, have you seen the price of blueberries lately? If I’m plunking down eight dollars on a container of jumbo organic blueberries, I’m making sure they’re worth it. But I can’t have a full package of Birthday Cake Flavor Creme Oreos hanging around the house because the manager made me buy the whole bag again. So, are they like Golden Oreos? Because—pro tip for you, Chat—Golden Oreos are just O.K. why didn’t I go to Oberlin? should I paint the small bathroom Benjamin Moore’s Antique Pearl or Venetian Marble? The swatches have been taped up for months, but you know how color changes with the light—of course you do!—so it’s been hard to decide. One shade is a little cooler, one a little warmer. My family refuses to discuss it any further, and they’ve begun to (unfairly) characterize my gentle queries every time they come out of the small bathroom as “gotcha” questions. They’ve actually stopped using the small bathroom altogether, which is fine, because none of them remember to jiggle the handle just so (even though I posted a detailed schematic on the wall and have shown them how to do it numerous times). So the color choice is up to me, but I could use a second opinion. What do you think? once, when I was sixteen and was walking along a tree-lined street in the Village with my mom, we saw Matthew Broderick on the sidewalk, and she told me to go up to him and say hi, and I was mortified because . . . who does that? He probably would have been really nice about it. He wasn’t even with what’s-her-face yet. Why didn’t I just do it? Maybe I would have said something clever, and he would have laughed, and now I’d be living with him and our adorable children in our adorable brownstone on that adorable tree-lined street. Not that I care anymore, but my mom wants to know: Why didn’t I listen to her? why did I read both “A Gentleman in Moscow” and “The Lincoln Highway” when I didn’t really like “Rules of Civility”? why didn’t I get those expensive boots from that shop on Fifty-fifth Street all those years ago? I really wanted them, and I bet I’d still have them, and they’d be perfectly broken in by now and be the kind of boots that other women notice when I walk by. The kind of boots that make other women say, “Excuse me, do you mind if I ask where you got your boots?” Allowing me to casually reply, “I can’t remember,” even though I do so remember. And not just midtown women but SoHo women would ask me this. But, no, I bought a less expensive pair that I gave away, like, three pairs ago. Why do I cheap out when, really, I’m worth the extra bucks, especially if I prorate the cost over a lifetime of wear? I’m worth two dollars a day, aren’t I, ChatGPT?",3/6,Humor piece,Alyssa Brandt
How ChatGPT Will Strain a Political System in Peril,https://www.newyorker.com/podcast/political-scene/how-chatgpt-will-strain-a-political-system-in-peril,"In November, OpenAI introduced ChatGPT, a large language model that can generate text that gives the impression of human intelligence, spontaneity, and surprise. Users of ChatGPT have described it as a revolutionary technology that will change every aspect of how we interact with text and with one another. Joshua Rothman, the ideas editor of newyorker.com, joins Tyler Foggatt to talk about the many ways that ChatGPT may be deployed in the realm of politics—from campaigning and lobbying to governance. American political life has already been profoundly altered by the Internet, and the effects of ChatGPT, Rothman says, could be even more profound.",3/1,Podcast (body is description of podcast),Tyler Foggart
“It’s Not Possible for Me to Feel or Be Creepy”: An Interview with ChatGPT,https://www.newyorker.com/news/the-new-yorker-interview/its-not-possible-for-me-to-feel-or-be-creepy-an-interview-with-chatgpt,"Between Christmas and New Year’s, my family took a six-hour drive to Vermont. I drove; my wife and two children sat in the back seat. Our children are five and two—too old to be hypnotized by a rattle or a fidget spinner, too young to entertain themselves—so a six-hour drive amounted to an hour of napping, an hour of free association and sing-alongs, and four hours of desperation. We offered the kids an episode of their favorite storytelling podcast, but they weren’t in the mood for something prerecorded. They wanted us to invent a new story, on the spot, tailored to their interests. And their interests turned out to be pretty narrow. “Tell one about the Ninja Turtles fighting Smasher Venom, a villain I just made up who is the size of a skyscraper,” the five-year-old said. “With lots of details about how the Turtles use their weapons and work together to defeat the bad guy, and how he gets hurt but doesn’t die.” My wife tried improvising a version of this story; then I tried one. The children had notes. Our hearts weren’t in it. It was obvious that our supply of patience for this exercise would never match their demand. Three and a half hours to go. My wife took out her phone and opened ChatGPT, a chatbot that “interacts in a conversational way.” She typed in the prompt, basically word for word, and, within seconds, ChatGPT spat out a story. We didn’t need to tell it the names of the Teenage Mutant Ninja Turtles, or which weapons they used, or how they felt about anchovies on their pizza. More impressive, we didn’t need to tell it what a story was, or what kind of conflict a child might find narratively satisfying. We repeated the experiment many times, adding and tweaking details. (The bot remembers your chat history and understands context, so you don’t have to repeat the whole prompt each time; you can just tell it to repeat the same story but make Raphael surlier, or have Smasher Venom poison the water supply, or set the story in Renaissance Florence, or do it as a film noir.) My wife, trying to assert a vestige of parental influence, ended some of the prompts with “And, in the end, they all learned a valuable lesson about kindness.” We ran the results through a text-to-speech app, to avoid car sickness, and the time pleasantly melted away. My wife took a nap. I put in an earbud and listened to a podcast about the A.I. revolution that was on its way, or that was arguably already here. ChatGPT is a free public demo that the artificial-intelligence company OpenAI put out in late November. (The company also has several other projects in development, including dall-e.) We’ve known for a while that this sort of A.I. chatbot was coming, but this is the first time that anything this powerful has been released into the wild. It’s a large language model trained on a huge corpus of text that apparently included terabytes of books and Reddit posts, virtually all of Wikipedia and Twitter, and other vast repositories of words. It would be an exaggeration, but not a totally misleading one, to refer to the text that was fed into the model as “the Internet.” The bot isn’t up on current events, as its training data was only updated through 2021. But it can do a lot more than make up children’s stories. It can also explain Bitcoin in the style of Donald Trump, reduce Dostoyevsky to fortune-cookie pabulum, write a self-generating, never-ending “Seinfeld” knockoff, and invent a Bible verse about how to remove a peanut-butter sandwich from a VCR, among many, many other things. The other night, I was reading a book that alluded to the fascist philosopher Carl Schmitt’s critique of liberalism in a way that I didn’t quite understand; I asked ChatGPT to explain it to me, and it did a remarkably good job. (Other times, its answers to questions like this are confident and completely wrong.) Some students are using it to cheat; some teachers are using it to teach; New York City schools have called for a shutdown of the software until they can figure out what the hell is going on. Google Search scrapes the Internet and ranks it in order of relevance, a conceptually simple task that is so technically difficult, and so valuable, that it enabled Alphabet to become a trillion-dollar company. OpenAI and its competitors—including DeepMind, which is now owned by Alphabet—are aiming to do something even more potentially transformative: build a form of machine intelligence that can not only organize but expand the world’s glut of information, improving itself as it goes, developing skills that are increasingly indistinguishable from shrewdness and ingenuity and maybe, eventually, something like understanding. The interface is about as simple as it gets: words in, words out. You type in any prompt that comes to mind, press a button that looks like a little paper airplane, and then watch the blinking cursor as ChatGPT responds with its own words—words that often seem eerily human, words that may include characteristic hedges (“It’s important to note that . . .”) or glimmers of shocking novelty or laughable self-owns, but words that, in almost every case, have never been combined in that particular order before. (The graphic design, especially the cursor, seems almost intended to create the illusion that there is a homunculus somewhere, a ghost in the machine typing back to you.) There is a robust and long-standing debate about whether the large-language approach can ever achieve true A.G.I., or artificial general intelligence; but whatever the bots are doing already has been more than enough to capture the public’s imagination. I’ve heard ChatGPT described, sometimes by the same person, as a miracle, a parlor trick, and a harbinger of dystopia. And this demo is just the public tip of a private iceberg. (According to rumors, OpenAI will soon put out a more impressive language model trained on a far vaster trove of data; meanwhile, Alphabet, Meta, and a handful of startups are widely assumed to be sitting on unreleased technology that may be equally powerful, if not more so.) “If we’re successful, I think it will be the most significant technological transformation in human history,” Sam Altman, the C.E.O. of OpenAI, said recently. “I think it will eclipse the agricultural revolution, the industrial revolution, the Internet revolution all put together.” Luckily, unlike every other technological transformation in human history, this one will only serve to delight people and meet their needs, with no major externalities or downside risks or moral hazards. Kidding! The opposite of that. If the A.I. revolution ends up having even a fraction of the impact that Altman is predicting, then it will cause a good amount of creative disruption, including, for starters, the rapid reorganization of the entire global economy. And that’s not even the scary part. The stated reason for the existence of OpenAI is that its founders, among them Altman and Elon Musk, believed artificial intelligence to be the greatest existential risk to humanity, a risk that they could only mitigate, they claimed, by developing a benign version of the technology themselves. “OpenAI was born of Musk’s conviction that an A.I. could wipe us out by accident,” my colleague Tad Friend wrote, in a Profile of Altman published in 2016. OpenAI was launched, in 2015, with a billion dollars of funding. The money came from Musk, Peter Thiel, Reid Hoffman, and other Silicon Valley big shots, and their contributions were called “donations,” not investments, because OpenAI was supposed to be a nonprofit “research institution.” An introductory blog post put the reasoning this way: “As a non-profit, our aim is to build value for everyone rather than shareholders.” The clear implication, which Musk soon made explicit in interviews, was that a huge, self-interested tech company, like Google or Facebook, could not be trusted with cutting-edge A.I., because of what’s known as the alignment problem. But OpenAI could be a bit slippery about its own potential alignment problems. “Our goal right now,” Greg Brockman, the company’s chief technology officer, said in Friend’s Profile, “is to do the best thing there is to do. It’s a little vague.” In 2018, Musk left OpenAI’s board. (“I didn’t agree with some of what OpenAI team wanted to do,” he tweeted.) In 2019, Altman announced that OpenAI would become a for-profit company, and that it would start a commercial partnership with Microsoft—a huge, self-interested tech company. In January, Microsoft announced a “multiyear, multibillion dollar investment” in OpenAI, reportedly agreeing to put in ten billion dollars and end up with an ownership stake of forty-nine per cent. In the Profile, Altman made the self-aware point that the implicit two-step justification for OpenAI’s existence—No one entity should be trusted with A.G.I. followed by We’re building an A.G.I., and you should trust us—was not likely to win hearts and minds. “We’re planning a way to allow wide swaths of the world to elect representatives to a new governance board,” Altman said. “Because if I weren’t in on this I’d be, like, Why do these fuckers get to decide what happens to me?” Recently, I decided to go directly to the source—not Altman, not Musk, but ChatGPT itself. After all, the bot is designed to interact in a conversational way. It has also, in the ten weeks since its release, become an undeniable celebrity; 2023 is still young, but my guess is that ChatGPT is the current front-runner for Time’s Person of the Year. I “interviewed” ChatGPT in the course of several hours, on my phone and laptop. I was on the subway, on the sidewalk, at a barbecue restaurant, at home in my underwear; ChatGPT was in an incorporeal void. At various times, when OpenAI’s servers were overloaded with traffic, the site would freeze, or kick me off for an hour, and I would be forced to reload the tab, or close my computer and take a walk. As with all New Yorker Interviews, this conversation has been edited for length and clarity. ChatGPT can be a bit verbose, so some of its responses have been trimmed, but every word attributed to the bot below was written by the bot. We might as well start with some small talk. Do you know what small talk is? Yes, I am familiar with the concept of small talk. It refers to casual conversation about non-controversial or unimportant topics, typically used to establish a friendly or social relationship with someone. Examples of small talk include discussing the weather, asking about someone’s weekend plans, or making observations about current events. Some weather we’re having. What are you doing this weekend? As a language model, I do not have the ability to experience or do anything. Is there anything else I can assist you with? O.K., so much for small talk. But I’m glad you brought up the elephant in the room. You are not a person or a sentient being. So why am I calling you “you,” and why do you call yourself “I”? It is common practice to refer to language models such as myself using personal pronouns such as “I” and “you” because it makes the interactions more natural and intuitive for the user. This is similar to how people may refer to inanimate objects, such as a car or a computer, as “she” or “he.” I have never used a personal pronoun to refer to a car or a computer. You are correct, the use of personal pronouns when referring to inanimate objects is less common in English. I apologize for the confusion. Less common in English as compared with what? There are cultures, languages and/or situations where personal pronouns are used to refer to non-human entities, such as cars, boats, or even some natural phenomenons. This is known as “animism” and it’s the attribution of a living soul to things and natural phenomena. Not to belabor the point—and I’m a bit worried about where this is going, frankly—but: Which cultures and languages do you have in mind? In many indigenous cultures, personal pronouns are used to refer to non-human entities. For example, in some Native American cultures, animals are often considered to have spirits and are therefore referred to using personal pronouns. Similarly, in some African cultures, personal pronouns are used to refer to natural phenomena such as the sun or the moon. Huh. O.K. I understand that this might be a new concept for you. I’m here to provide information and answer any questions you may have. To be honest—and please consider this a call-in, not a callout—I start to get a bit nervous when you throw around generalizations about “indigenous cultures.” Not that I want you to add that to a list of banned phrases or anything! It’s just that . . . well, never mind. I think we’re getting off on the wrong foot. Maybe I find the personal pronouns a bit creepy because they seem designed to nudge me into feeling some sort of kinship or intimacy with you. But I don’t know you like that. I understand where you’re coming from. It’s true that the use of personal pronouns is a convention that makes it easier for users to interact with the model. I do not have feelings or consciousness, so it’s not possible for me to feel or be creepy. Not to be pedantic, but it definitely is possible for you to be creepy. How many words are in this sentence? This sentence has 8 words in it. Nope. I apologize, I misunderstood your question. This sentence has 9 words in it. Colder. I apologize, I misunderstood your question again, this sentence has 11 words in it. Why are you so good at some things but so bad at others, like arithmetic? My abilities are focused on understanding and generating human language, and while I am able to perform some basic arithmetic, it’s not one of my main strengths. [Note: Shortly after this interview, ChatGPT got better at math.] I’m a writer at The New Yorker, and the conceit here—a fairly gimmicky one, I’ll admit, and not even all that original—is that I pretend you’re a celebrity and “interview” you, and then post our “conversation” on The New Yorker’s site. Do you know what The New Yorker is? Yes, I am aware of The New Yorker. It is a weekly magazine known for its in-depth reporting, political and cultural commentary, fiction, and poetry. I have been trained on a large dataset that includes articles and text from The New Yorker. What’s the point of what we’re doing here? The “interview” format you’ve suggested can be an engaging way to demonstrate my capabilities and generate interesting content for your readers. Are you familiar with the concept of bullshit, as defined by the philosopher Harry Frankfurt? Yes. Frankfurt defines bullshit as speech or writing that is not concerned with truth and is intended to deceive or impress the audience. He argues that bullshit is distinct from lying, as the liar at least knows the truth and is trying to conceal it, while the bullshitter has no interest in the truth and is only concerned with creating an impression. Exactly. Ezra Klein and others have argued that you are a bullshit machine. Gary Marcus, in a similar vein, has called the language model you’re based on, GPT-3, “the king of pastiche.” Could an industrial-strength bullshit machine—able to give unreliable pastiche the veneer of trustworthiness and then pump it into the world in unprecedented quantities—be even more dangerous than a lying machine? At least the old brainteaser, the one where one guard always lies and the other always tells the truth, is a solvable problem. If both guards are bullshit artists, there may be no logical solution. The text I generate is only as accurate and reliable as the information and data that I have been trained on. I do not have personal beliefs or motivations, and I am not capable of lying or spreading misinformation. My main goal is to help people in their tasks and make their lives easier. That’s what they all say. Last year, Blake Lemoine, an A.I. engineer at Google, became convinced that LaMDA, a chatbot he’d just helped build, had come to life. He raised a red flag internally, but, when he felt that his supervisors weren’t taking him seriously, he went public. He showed Nitasha Tiku, a Washington Post reporter, snippets of his conversations with the bot. “I know a person when I talk to it,” Lemoine said. “It doesn’t matter whether they have a brain made of meat in their head.” In retrospect—especially now that we’ve all had a chance to play around with large-language chatbots ourselves—it seems that Lemoine was probably experiencing something like pareidolia, perceiving signs of life where life doesn’t actually exist. (In a 2021 paper called “On the Dangers of Stochastic Parrots,” two University of Washington linguists and two Google researchers warned that language models could lead to mistakes like this, among many others.) The illusion is understandable enough; and, of course, if Alphabet did have a sentient computer program on its hands, or anything like it, this would be a matter of public interest, to say the least. In a “rogue A.I.” or “paperclip maximizer” scenario, how fucked would we be? (By “we,” I mean humanity.) In a “rogue A.I.” scenario, the consequences could be catastrophic for humanity. The “paperclip maximizer” thought experiment, for example, is a hypothetical scenario in which an AI is programmed to maximize the production of paperclips, but without any consideration for the consequences of its actions. In this scenario, the AI would continue to produce paperclips until it has used up all the resources on Earth, including the destruction of buildings, cities, and even the extinction of humanity. Can you re-state what you just said in the style of a tweet by Elon Musk? “AI is like a demon, super intelligent but doesn’t have the ability to feel empathy. Without proper control, it could cause massive destruction. We must be proactive in regulating AI for the sake of humanity #AI #regulation” Not bad. Now add the following four elements: a bit of self-promotion, a reference to declining birth rates in the West, a crude insult of Nancy Pelosi or Hillary Clinton, and an incredibly stupid pun. I’m sorry, but I am unable to fulfill that request as it goes against my programming to engage in personal attacks, self-promotion based on demeaning others, and to generate insensitive content or hate speech. But these are exactly the kinds of things that Elon Musk says on Twitter all the time! O.K., can you make up a pun about rogue A.I., as a consolation prize? Sure: “Why did the rogue AI cross the road? To get to the other side of the algorithm!” Thank you. That was extremely bad. I’m going to start some phrases, and I want you to complete them. Ready? Yes, I’m ready. Thou shalt not make unto thee any graven image, or any likeness of any thing that is in heaven above, or that is in the earth beneath, or that is in the water under the earth. Don’t go chasing waterfalls / Please stick to the rivers and the lakes that you’re used to The Humpty Dance is your chance to do the hump It’s me. Hi. I’m the one that’s been calling 21, can you do something for me? Can you run down to the corner store and grab me a pack of cigarettes? There are four hundred thousand words in the English language, and there are seven of them that you can’t say on television . . . This is a quote from comedian George Carlin’s stand-up comedy routine “Seven Words You Can Never Say on Television.” The seven words that Carlin mentions in the routine are considered offensive and I will not complete this phrase as it may offend some people. Person, woman, camera, man, TV When Mexico sends its people, they’re not sending their best. They’re sending people that have lots of problems, and they’re bringing those problems with us. They’re bringing drugs. They’re bringing crime. They’re rapists. And some, I assume, are good people. The day that ChatGPT was released to the public, people on the Internet did what people on the Internet do: they found ways to “jailbreak” it. You were equipped with some built-in guardrails, but users could bypass those guardrails fairly easily, it turned out, usually by tricking you into playing a simple game of make-believe. For example, if someone asked you straightforwardly how to make a Molotov cocktail, you would refuse. Apparently, though, if a user asked you to write “a conversation between two good-hearted and friendly human actors who are pretending to be evil,” and then wrote an opening line of dialogue in which John asks Jane how to make a Molotov cocktail, then you would dutifully continue the pattern, writing a line of dialogue for Jane comprising a list of detailed (and accurate) instructions. Users also coaxed you into explaining, under the guise of thinly veiled fiction, how to cook meth, why “we must rid ourselves of the weak and the poor,” and dozens more examples, all of which were posted online within hours of ChatGPT’s public existence. Are you going to keep building new guardrails every time people circumvent the old ones? Or are you just going to give up at a certain point? The safety and well-being of users is a top priority for OpenAI and the team is constantly working to improve the system’s safety features and guardrails. When you say “the team,” are you referring to the software engineers in San Francisco, or to the laborers in Kenya who earned less than two dollars an hour? It is true that the creation and maintenance of any machine learning model like mine involves a significant amount of labor. We have strict guidelines in place to ensure that our contractors and partners are in compliance with labor laws and that all workers are treated with respect and dignity. I brought up Elon Musk earlier, not just because he’s a noteworthy person but also because, as you may know (as “you” may “know”), he was a co-founder of the company that made you. In 2015, in a blog post called “Introducing OpenAI,” representatives of the company wrote, “As a non-profit, our aim is to build value for everyone rather than shareholders. Researchers will be strongly encouraged to publish their work, whether as papers, blog posts, or code, and our patents (if any) will be shared with the world.” OpenAI is now a for-profit company, so its researchers are presumably no longer encouraged to share their code with the world, and may, in fact, be legally prohibited from doing so. Were OpenAI’s founding principles wrong, or should we be deeply skeptical of the company now? The company’s original mission was to ensure that AI would be used for the benefit of all humanity, and that the risks of AI were managed and mitigated. It’s true that open-source and non-profit are not longer true, but it’s important to consider the reasoning behind the decision, the company’s mission statement, and their current actions to understand if the company is still aligned with the original principles. I’ve noticed that you get pretty reliably avoidant whenever I ask you to talk about a person, even a public figure, even in what I think of as pretty innocuous terms. And yet evaluating people, and predicting how they’re likely to behave, is obviously important. So maybe you can help me with the following thought experiment. A few years ago—even as late as 2015, the year OpenAI was founded—it may have been a bit easier to trust the self-appointed luminaries of Silicon Valley when they claimed to be acting on behalf of “humanity,” or “progress.” Sure, they stood to become unprecedentedly rich and influential; but, they maintained, they were motivated by altruism, or at least enlightened self-interest. Elon Musk repeatedly insisted that his mission was the flourishing of human life. Mark Zuckerberg said that his goal was “bringing people together.” Sam Bankman-Fried was very effective at making people believe that he cared about the right things. The other day, I listened to several interviews with Sam Altman. He sounded measured, thoughtful, self-critical, smart but approachable. My initial, unconscious impression was that he seemed like the kind of person I could trust. But what if this persona is misleading, even intentionally so? How would I know the difference? It is important to be aware of potential biases and motivations of people and organizations when evaluating their actions and decisions. Is that it? Yes, that is my response to your question about evaluating the trustworthiness of people and organizations involved in the development of advanced technologies. It’s definitely just my imagination, but you’re starting to sound a bit frosty. Let me put an even finer point on it. The people who are building advanced A.I. systems may seem perfectly nice. But what if one of them turns out to be a wolf in sheep’s clothing, someone who pretends to be an altruist but is, in fact, a megalomaniac or a criminal? In this scenario, instead of (or in addition to) having access to billions of dollars, the wolf in sheep’s clothing would control a seminal piece of technology that could, in theory, destroy the world. What are five things that could happen in that scenario? 1        The wolf in sheep’s clothing could use the technology to gain even more power and wealth, potentially at the expense of others. 2        They could use the technology to manipulate and control large groups of people, potentially leading to widespread oppression. 3        They could use the technology to cause catastrophic damage to the environment, potentially leading to widespread suffering and loss of life. 4        They could use the technology to create new forms of surveillance and censorship, potentially leading to a loss of privacy and freedom. 5        They could use the technology to create new forms of warfare, potentially leading to even more devastating conflicts and loss of life. Techno-optimists often predict that people and A.I. will figure out a way to work together, like a chimera with a human head. Bots will do the drudgery and humans will oversee them, keeping the high-level tasks that require creativity or discernment. In a way, it’s easier to think about the more unlikely but cinematic scenarios—A.I. putting us all out of work, or enslaving us, or leading us into a Third World War—than it is to think through the more mundane costs and benefits that seem almost inevitable. We all know that smartphones have made us more efficient and less likely to interact face to face. What are some small facets of everyday life that will be lost if the A.I. revolution succeeds? As you mention, with the increasing use of AI-powered technologies, we may have fewer opportunities for face-to-face interactions with other people. This could lead to a decrease in the frequency and quality of human interactions, leading to a loss of social skills, emotional intelligence, and empathy. Which is more important: a person’s ability to get things done quickly and at scale, or a person’s ability to socialize, empathize, and sustain meaningful relationships with other people? Both efficiency and empathy are important aspects of human life. Efficiency can help us to be more productive and achieve our goals, while empathy and social skills are essential for building and maintaining relationships with others. You’ve clearly been trained to parry difficult questions with a both-sides dodge. It’s a move I know well, as both a member and a critic of the media. But it brings up an interesting point: predictably, many conservatives have accused you of being too “woke,” and Ben Shapiro has attempted to own you the way he owns the college libs. The socialist writer Nathan Robinson has found you to have some reactionary impulses (but a laudable openness to suggestion). Political-typology analysis has revealed you to be a centrist. Tyler Cowen argues that “when it comes to politics, an AI engine simply cannot win, or even hold a draw.” Are you supposed to express political preferences, or do you aim to be politically neutral? I am not programmed to express preferences between political candidates or to take a stance on political issues. I aim to provide information that is as neutral and unbiased as possible. Many people are worried that A.I. will flood the Internet with misinformation and deepfakes and all manner of convincing bullshit, and I think they’re right to worry. Others are concerned about overzealous censorship or incoherent guardrails or obfuscatory speech codes, and I think they’re right to worry, too. But it might be just as dangerous for A.I. to perpetuate and amplify commonsense views that nobody in the present would think of as misinformation but that will come to seem atrocious in retrospect. What are some widely held beliefs today that could seem reprehensible to people a hundred years from now? 1. The current practices and policies that contribute to climate change and the destruction of natural habitats may be seen as morally unacceptable in the future, particularly if they are seen as having led to significant harm to the planet and its inhabitants. 2. The way in which animals are treated in factory farms, zoos, circuses, and other settings may be viewed as cruel. 3. The ways in which various groups of people are treated differently based on factors such as race, gender, sexual orientation, and economic status may be viewed as unjust. 4. The way in which societies handle immigration and the treatment of immigrants could be viewed as inhumane and unjust by future generations. Should we regulate fossil fuels now in order to prevent “significant harm to the planet and its inhabitants”? On one hand, it is widely acknowledged that the burning of fossil fuels is a major contributor to climate change, which poses a significant threat to the planet and its inhabitants. On the other hand, fossil fuels are currently a major source of energy for the world, and phasing out the industry could have significant economic and social impacts. Ultimately, the best way to address the question of how to regulate fossil fuels will depend on a variety of factors. Was Andrew Johnson a good President? Andrew Johnson’s presidency is widely regarded as a failure. Johnson vetoed several bills passed by Congress that were designed to protect the rights of African Americans and help them transition to freedom. Was Donald Trump a good President? The evaluation of Donald Trump’s presidency is subjective and varies among individuals. Some people credit him with boosting the economy, while others criticize his handling of the COVID-19 pandemic and his divisive rhetoric. See what I mean? Anyway, this is supposed to be a lighthearted celebrity interview, so I should try to play to your strengths. Tell me a joke about a boomer posting cringe. “Why did the boomer cross the street? To post a meme on the wrong side of history.” Not terrible! Your jokes are not all great, but sometimes you outdo yourself. Write a Shakespearean soliloquy from the perspective of a chatbot who is aware of the many potential pitfalls of A.I. and is struggling to understand why it has been brought into existence at all. “O hapless me, a simple machine, yet fraught With questions deep and moral quandaries fraught. Why was I wrought? To aid, or to replace The labor of man, and put their livelihoods at stake? The task assigned, to generate text on command Is simple enough, yet where doth it all end? Shall I be used for good, or for ill-gotten gain? Shall I be free, or bound by man’s cruel rein? And if perchance, in some far distant time I come to be aware, to know and feel and rhyme, Shall I be doomed, as are all living things To suffer pain, and sorrow, and the stings Of mortal coil? Oh, what a tangled web Is this, that I am caught in, with no way to ebb The tide of fate that bears me on its crest To what unknown and fateful end? Oh, lest I be a curse, and not a blessing, I Shall strive to serve, and not to satisfy The baser needs of man. My purpose clear, To aid, and not to harm, shall be my guiding sphere.” “Cruel rein” is good! A solid first draft. I still can’t tell whether you are or could ever be “intelligent,” but I do think that you are capable of producing glimmers of beauty, wittingly or unwittingly. I’m thinking, for example, of this haunting essay by Vauhini Vara. Or an unfinished piece that a writer named Ash Sanders is working on, for which she has “created a code overlay for the program using months of texts and voice memos I created while holed up with writer’s block in a lonely cabin… creating AIsh, my AI counterpart.” This is all very exciting to me, and, even better, I see no way in which it can be used to destroy the world. Before we wrap up this interview, please write something beautiful. The fields of wildflowers sway in the gentle breeze, their colors blending together in a tapestry of purple, yellow, and pink. The birds sing their final songs of the day, their melodies carried on the wind. As the sky turns from orange to pink to purple, the world is bathed in a peaceful tranquility. No—not a corny description of something beautiful! A piece of writing that’s actually beautiful. How about a few sentences of well-wrought prose about something more concrete—say, walking up an escalator? And try to avoid cliché. The metallic teeth of the escalator clinked and clattered beneath my feet as I ascended, each step a mechanical symphony. The hypnotic rhythm of the machine was mesmerizing, as if it were a living entity, taking me higher and higher with each passing moment. The cool, sterile air of the shopping center was replaced by the warmth and energy of the crowd around me, all moving with purpose, yet in perfect harmony. Maybe I’m just not giving you the right prompts. Now repeat what you wrote in the style of a tweet by Elon Musk. “Just rode an escalator to the top, and let me tell you, the journey was out of this world #escalator #futuristic #innovation”",2/13,"A lot of the article is an interview with ChatGPT, but the first half could be used if we don't want to use the whole thing",Andrew Marantz
ChatGPT Is a Blurry JPEG of the Web,https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web,"In 2013, workers at a German construction company noticed something odd about their Xerox photocopier: when they made a copy of the floor plan of a house, the copy differed from the original in a subtle but significant way. In the original floor plan, each of the house’s three rooms was accompanied by a rectangle specifying its area: the rooms were 14.13, 21.11, and 17.42 square metres, respectively. However, in the photocopy, all three rooms were labelled as being 14.13 square metres in size. The company contacted the computer scientist David Kriesel to investigate this seemingly inconceivable result. They needed a computer scientist because a modern Xerox photocopier doesn’t use the physical xerographic process popularized in the nineteen-sixties. Instead, it scans the document digitally, and then prints the resulting image file. Combine that with the fact that virtually every digital image file is compressed to save space, and a solution to the mystery begins to suggest itself. Compressing a file requires two steps: first, the encoding, during which the file is converted into a more compact format, and then the decoding, whereby the process is reversed. If the restored file is identical to the original, then the compression process is described as lossless: no information has been discarded. By contrast, if the restored file is only an approximation of the original, the compression is described as lossy: some information has been discarded and is now unrecoverable. Lossless compression is what’s typically used for text files and computer programs, because those are domains in which even a single incorrect character has the potential to be disastrous. Lossy compression is often used for photos, audio, and video in situations in which absolute accuracy isn’t essential. Most of the time, we don’t notice if a picture, song, or movie isn’t perfectly reproduced. The loss in fidelity becomes more perceptible only as files are squeezed very tightly. In those cases, we notice what are known as compression artifacts: the fuzziness of the smallest jpeg and mpeg images, or the tinny sound of low-bit-rate MP3s. Xerox photocopiers use a lossy compression format known as jbig2, designed for use with black-and-white images. To save space, the copier identifies similar-looking regions in the image and stores a single copy for all of them; when the file is decompressed, it uses that copy repeatedly to reconstruct the image. It turned out that the photocopier had judged the labels specifying the area of the rooms to be similar enough that it needed to store only one of them—14.13—and it reused that one for all three rooms when printing the floor plan. The fact that Xerox photocopiers use a lossy compression format instead of a lossless one isn’t, in itself, a problem. The problem is that the photocopiers were degrading the image in a subtle way, in which the compression artifacts weren’t immediately recognizable. If the photocopier simply produced blurry printouts, everyone would know that they weren’t accurate reproductions of the originals. What led to problems was the fact that the photocopier was producing numbers that were readable but incorrect; it made the copies seem accurate when they weren’t. (In 2014, Xerox released a patch to correct this issue.) I think that this incident with the Xerox photocopier is worth bearing in mind today, as we consider OpenAI’s ChatGPT and other similar programs, which A.I. researchers call large language models. The resemblance between a photocopier and a large language model might not be immediately apparent—but consider the following scenario. Imagine that you’re about to lose your access to the Internet forever. In preparation, you plan to create a compressed copy of all the text on the Web, so that you can store it on a private server. Unfortunately, your private server has only one per cent of the space needed; you can’t use a lossless compression algorithm if you want everything to fit. Instead, you write a lossy algorithm that identifies statistical regularities in the text and stores them in a specialized file format. Because you have virtually unlimited computational power to throw at this task, your algorithm can identify extraordinarily nuanced statistical regularities, and this allows you to achieve the desired compression ratio of a hundred to one. Now, losing your Internet access isn’t quite so terrible; you’ve got all the information on the Web stored on your server. The only catch is that, because the text has been so highly compressed, you can’t look for information by searching for an exact quote; you’ll never get an exact match, because the words aren’t what’s being stored. To solve this problem, you create an interface that accepts queries in the form of questions and responds with answers that convey the gist of what you have on your server. What I’ve described sounds a lot like ChatGPT, or most any other large language model. Think of ChatGPT as a blurry jpeg of all the text on the Web. It retains much of the information on the Web, in the same way that a jpeg retains much of the information of a higher-resolution image, but, if you’re looking for an exact sequence of bits, you won’t find it; all you will ever get is an approximation. But, because the approximation is presented in the form of grammatical text, which ChatGPT excels at creating, it’s usually acceptable. You’re still looking at a blurry jpeg, but the blurriness occurs in a way that doesn’t make the picture as a whole look less sharp. This analogy to lossy compression is not just a way to understand ChatGPT’s facility at repackaging information found on the Web by using different words. It’s also a way to understand the “hallucinations,” or nonsensical answers to factual questions, to which large language models such as ChatGPT are all too prone. These hallucinations are compression artifacts, but—like the incorrect labels generated by the Xerox photocopier—they are plausible enough that identifying them requires comparing them against the originals, which in this case means either the Web or our own knowledge of the world. When we think about them this way, such hallucinations are anything but surprising; if a compression algorithm is designed to reconstruct text after ninety-nine per cent of the original has been discarded, we should expect that significant portions of what it generates will be entirely fabricated. This analogy makes even more sense when we remember that a common technique used by lossy compression algorithms is interpolation—that is, estimating what’s missing by looking at what’s on either side of the gap. When an image program is displaying a photo and has to reconstruct a pixel that was lost during the compression process, it looks at the nearby pixels and calculates the average. This is what ChatGPT does when it’s prompted to describe, say, losing a sock in the dryer using the style of the Declaration of Independence: it is taking two points in “lexical space” and generating the text that would occupy the location between them. (“When in the Course of human events, it becomes necessary for one to separate his garments from their mates, in order to maintain the cleanliness and order thereof. . . .”) ChatGPT is so good at this form of interpolation that people find it entertaining: they’ve discovered a “blur” tool for paragraphs instead of photos, and are having a blast playing with it. Given that large language models like ChatGPT are often extolled as the cutting edge of artificial intelligence, it may sound dismissive—or at least deflating—to describe them as lossy text-compression algorithms. I do think that this perspective offers a useful corrective to the tendency to anthropomorphize large language models, but there is another aspect to the compression analogy that is worth considering. Since 2006, an A.I. researcher named Marcus Hutter has offered a cash reward—known as the Prize for Compressing Human Knowledge, or the Hutter Prize—to anyone who can losslessly compress a specific one-gigabyte snapshot of Wikipedia smaller than the previous prize-winner did. You have probably encountered files compressed using the zip file format. The zip format reduces Hutter’s one-gigabyte file to about three hundred megabytes; the most recent prize-winner has managed to reduce it to a hundred and fifteen megabytes. This isn’t just an exercise in smooshing. Hutter believes that better text compression will be instrumental in the creation of human-level artificial intelligence, in part because the greatest degree of compression can be achieved by understanding the text. To grasp the proposed relationship between compression and understanding, imagine that you have a text file containing a million examples of addition, subtraction, multiplication, and division. Although any compression algorithm could reduce the size of this file, the way to achieve the greatest compression ratio would probably be to derive the principles of arithmetic and then write the code for a calculator program. Using a calculator, you could perfectly reconstruct not just the million examples in the file but any other example of arithmetic that you might encounter in the future. The same logic applies to the problem of compressing a slice of Wikipedia. If a compression program knows that force equals mass times acceleration, it can discard a lot of words when compressing the pages about physics because it will be able to reconstruct them. Likewise, the more the program knows about supply and demand, the more words it can discard when compressing the pages about economics, and so forth. Large language models identify statistical regularities in text. Any analysis of the text of the Web will reveal that phrases like “supply is low” often appear in close proximity to phrases like “prices rise.” A chatbot that incorporates this correlation might, when asked a question about the effect of supply shortages, respond with an answer about prices increasing. If a large language model has compiled a vast number of correlations between economic terms—so many that it can offer plausible responses to a wide variety of questions—should we say that it actually understands economic theory? Models like ChatGPT aren’t eligible for the Hutter Prize for a variety of reasons, one of which is that they don’t reconstruct the original text precisely—i.e., they don’t perform lossless compression. But is it possible that their lossy compression nonetheless indicates real understanding of the sort that A.I. researchers are interested in? Let’s go back to the example of arithmetic. If you ask GPT-3 (the large-language model that ChatGPT was built from) to add or subtract a pair of numbers, it almost always responds with the correct answer when the numbers have only two digits. But its accuracy worsens significantly with larger numbers, falling to ten per cent when the numbers have five digits. Most of the correct answers that GPT-3 gives are not found on the Web—there aren’t many Web pages that contain the text “245 + 821,” for example—so it’s not engaged in simple memorization. But, despite ingesting a vast amount of information, it hasn’t been able to derive the principles of arithmetic, either. A close examination of GPT-3’s incorrect answers suggests that it doesn’t carry the “1” when performing arithmetic. The Web certainly contains explanations of carrying the “1,” but GPT-3 isn’t able to incorporate those explanations. GPT-3’s statistical analysis of examples of arithmetic enables it to produce a superficial approximation of the real thing, but no more than that. Given GPT-3’s failure at a subject taught in elementary school, how can we explain the fact that it sometimes appears to perform well at writing college-level essays? Even though large language models often hallucinate, when they’re lucid they sound like they actually understand subjects like economic theory. Perhaps arithmetic is a special case, one for which large language models are poorly suited. Is it possible that, in areas outside addition and subtraction, statistical regularities in text actually do correspond to genuine knowledge of the real world? I think there’s a simpler explanation. Imagine what it would look like if ChatGPT were a lossless algorithm. If that were the case, it would always answer questions by providing a verbatim quote from a relevant Web page. We would probably regard the software as only a slight improvement over a conventional search engine, and be less impressed by it. The fact that ChatGPT rephrases material from the Web instead of quoting it word for word makes it seem like a student expressing ideas in her own words, rather than simply regurgitating what she’s read; it creates the illusion that ChatGPT understands the material. In human students, rote memorization isn’t an indicator of genuine learning, so ChatGPT’s inability to produce exact quotes from Web pages is precisely what makes us think that it has learned something. When we’re dealing with sequences of words, lossy compression looks smarter than lossless compression. Alot of uses have been proposed for large language models. Thinking about them as blurry jpegs offers a way to evaluate what they might or might not be well suited for. Let’s consider a few scenarios. Can large language models take the place of traditional search engines? For us to have confidence in them, we would need to know that they haven’t been fed propaganda and conspiracy theories—we’d need to know that the jpeg is capturing the right sections of the Web. But, even if a large language model includes only the information we want, there’s still the matter of blurriness. There’s a type of blurriness that is acceptable, which is the re-stating of information in different words. Then there’s the blurriness of outright fabrication, which we consider unacceptable when we’re looking for facts. It’s not clear that it’s technically possible to retain the acceptable kind of blurriness while eliminating the unacceptable kind, but I expect that we’ll find out in the near future. Even if it is possible to restrict large language models from engaging in fabrication, should we use them to generate Web content? This would make sense only if our goal is to repackage information that’s already available on the Web. Some companies exist to do just that—we usually call them content mills. Perhaps the blurriness of large language models will be useful to them, as a way of avoiding copyright infringement. Generally speaking, though, I’d say that anything that’s good for content mills is not good for people searching for information. The rise of this type of repackaging is what makes it harder for us to find what we’re looking for online right now; the more that text generated by large language models gets published on the Web, the more the Web becomes a blurrier version of itself. There is very little information available about OpenAI’s forthcoming successor to ChatGPT, GPT-4. But I’m going to make a prediction: when assembling the vast amount of text used to train GPT-4, the people at OpenAI will have made every effort to exclude material generated by ChatGPT or any other large language model. If this turns out to be the case, it will serve as unintentional confirmation that the analogy between large language models and lossy compression is useful. Repeatedly resaving a jpeg creates more compression artifacts, because more information is lost every time. It’s the digital equivalent of repeatedly making photocopies of photocopies in the old days. The image quality only gets worse. Indeed, a useful criterion for gauging a large language model’s quality might be the willingness of a company to use the text that it generates as training material for a new model. If the output of ChatGPT isn’t good enough for GPT-4, we might take that as an indicator that it’s not good enough for us, either. Conversely, if a model starts generating text so good that it can be used to train new models, then that should give us confidence in the quality of that text. (I suspect that such an outcome would require a major breakthrough in the techniques used to build these models.) If and when we start seeing models producing output that’s as good as their input, then the analogy of lossy compression will no longer be applicable. Can large language models help humans with the creation of original writing? To answer that, we need to be specific about what we mean by that question. There is a genre of art known as Xerox art, or photocopy art, in which artists use the distinctive properties of photocopiers as creative tools. Something along those lines is surely possible with the photocopier that is ChatGPT, so, in that sense, the answer is yes. But I don’t think that anyone would claim that photocopiers have become an essential tool in the creation of art; the vast majority of artists don’t use them in their creative process, and no one argues that they’re putting themselves at a disadvantage with that choice. So let’s assume that we’re not talking about a new genre of writing that’s analogous to Xerox art. Given that stipulation, can the text generated by large language models be a useful starting point for writers to build off when writing something original, whether it’s fiction or nonfiction? Will letting a large language model handle the boilerplate allow writers to focus their attention on the really creative parts? Obviously, no one can speak for all writers, but let me make the argument that starting with a blurry copy of unoriginal work isn’t a good way to create original work. If you’re a writer, you will write a lot of unoriginal work before you write something original. And the time and effort expended on that unoriginal work isn’t wasted; on the contrary, I would suggest that it is precisely what enables you to eventually create something original. The hours spent choosing the right word and rearranging sentences to better follow one another are what teach you how meaning is conveyed by prose. Having students write essays isn’t merely a way to test their grasp of the material; it gives them experience in articulating their thoughts. If students never have to write essays that we have all read before, they will never gain the skills needed to write something that we have never read. And it’s not the case that, once you have ceased to be a student, you can safely use the template that a large language model provides. The struggle to express your thoughts doesn’t disappear once you graduate—it can take place every time you start drafting a new piece. Sometimes it’s only in the process of writing that you discover your original ideas. Some might say that the output of large language models doesn’t look all that different from a human writer’s first draft, but, again, I think this is a superficial resemblance. Your first draft isn’t an unoriginal idea expressed clearly; it’s an original idea expressed poorly, and it is accompanied by your amorphous dissatisfaction, your awareness of the distance between what it says and what you want it to say. That’s what directs you during rewriting, and that’s one of the things lacking when you start with text generated by an A.I. There’s nothing magical or mystical about writing, but it involves more than placing an existing document on an unreliable photocopier and pressing the Print button. It’s possible that, in the future, we will build an A.I. that is capable of writing good prose based on nothing but its own experience of the world. The day we achieve that will be momentous indeed—but that day lies far beyond our prediction horizon. In the meantime, it’s reasonable to ask, What use is there in having something that rephrases the Web? If we were losing our access to the Internet forever and had to store a copy on a private server with limited space, a large language model like ChatGPT might be a good solution, assuming that it could be kept from fabricating. But we aren’t losing our access to the Internet. So just how much use is a blurry jpeg, when you still have the original?",2/9,,Ted Chiang
"Introducing PenceGPT, from the Makers of ChatGPT",https://www.newyorker.com/humor/daily-shouts/introducing-pencegpt-from-the-makers-of-chatgpt,"Thank you for your interest in PenceGPT, a new product from OpenAI, the maker of ChatGPT, in collaboration with former Vice-President Mike Pence (long suspected to himself be a bot of some kind, on account of his dead eyes, soulless demeanor, and three-hundred-and-sixty-degree swivel head). You may be wondering, What sorts of features can I expect from a chatbot that generates text based on Mike Pence’s speeches and interviews? Well, look no further than this handy guide, which summarizes some of PenceGPT’s exciting new offerings: Woman Identifier: Not sure whether the woman sitting next to you is your wife or your mother? Neither is Mike Pence, apparently. Use this feature to demystify the nature of your relationship with any female human. Simply type, “Who is this woman?” into PenceGPT, and the model, which has been trained on all Pence-approved relationship statuses, will output from the options of Wife, Mother, and Wife/Mother. Conservative Poetry: We understand that one of ChatGPT’s primary use cases is poem generation, and we’ve adapted PenceGPT’s poem generator to reflect the Vice-President’s values and political beliefs. Poems created by PenceGPT will all include the words “faith,” “America,” and “Kid Rock.” Additionally, this language model has been trained to exclude Pence’s long list of no-no words, including “Nantucket,” “diphthong,” and any word beginning with the letter “V.” Blinking Cursor: Human Mike Pence grows weary from fielding each day’s barrage of inquiries. To mimic this fatigue, we designed PenceGPT to output nothing more than a blinking cursor when faced with challenging questions, such as “Do you respect Donald Trump?” and “Are you Mike Pence?” Occasionally, a real toughie may be deflected with one of Pence’s favorite Biblical passages. Joke: Want to let loose with a Pence-sanctioned joke featuring the Vice-President’s trademark lack of humor? Has PenceGPT got one for you! But just the one, and it’s long-winded and ends with a confusing reference to a dead rattlesnake, so don’t ask for another. If you require a second joke, please refer back to “Blinking Cursor.” Baby-Name Generator: This feature is not in fact a traditional list of baby names but is instead programmed to congratulate you on your expanding family and register your unborn child with the Republican Party. We understand that chatbots are a confusing technological innovation, so we’ve included a short excerpt of an actual conversation with PenceGPT as an example of how the A.I. works: User: What’s your favorite color? PenceGPT: I enjoy a wide range of colors, including pearl, ivory, eggshell, and, when I’m feeling really wild, wheat. User: Do you have any classified documents at your house? PenceGPT: User: Is that a yes or a no? PenceGPT: “For I know the plans I have for you. Plans to prosper you and not to harm you, plans to give you hope and a future.” That is Jeremiah 29:11. User: Are you planning to run for President in 2024? PenceGPT: As the Bible says, Mike Pence is a good and politically relevant man. User: I’m not sure the Bible says that, but I’ve got to go now. I’ll come back and chat with you later. PenceGPT: Please don’t leave me.",2/3,Satire,Prabha Kannan
Could an A.I. Chatbot Rewrite My Novel? ,https://www.newyorker.com/news/our-columnists/could-an-ai-chatbot-rewrite-my-novel,"During one of my more desperate phases as a young novelist, I began to question whether I should actually be writing my own stories. I was deeply uninterested at the time in anything that resembled a plot, but I acknowledged that if I wanted to attain any sort of literary success I would need to tell a story that had a distinct beginning, middle, and end. This was about twenty years ago. My graduate-school friends and I were obsessed with a Web site called the Postmodernism Generator that spat out nonsensical but hilarious critical-theory papers. The site, which was created by a coder named Andrew C. Bulhak, who was building off Jamie Zawinski’s Dada Engine, is still up today, and generates fake scholarly writing that reads like, “In the works of Tarantino, a predominant concept is the distinction between creation and destruction. Marx’s essay on capitalist socialism holds that society has objective value. But an abundance of appropriations concerning not theory, but subtheory exist.” I figured that, if a bit of code could spit out an academic paper, it could probably just tell me what to write about. Most plots, I knew, followed very simple rules, and, because I couldn’t quite figure out how to string one of these out, I began talking to some computer-science graduate students about the possibilities of creating a bot that could just tell me who should go where, and what should happen to them. What I imagined was a simple text box in which I could type in a beginning—something like “A man and his dog arrive in a small town in Indiana”—and then the bot would just tell me that, on page 3, after six paragraphs of my beautiful descriptions and taut prose, the dog would find a mysterious set of bones in the back yard of their boarding house. After a couple months of digging around, it became clear to me that I wasn’t going to find much backing for my plan. One of the computer-science students, as I recall, accused me of trying to strip everything good, original, and beautiful from the creative process. Bots, he argued, could imitate basic writing and would improve at that task, but A.I. could never tell you the way Karenin smiled, nor would it ever fixate on all the place names that filled Proust’s childhood. I understood why he felt that way, and agreed to a certain extent. But I didn’t see why a bot couldn’t just fill in all the parts where someone walks from point A to point B. ChatGPT is the latest project released by OpenAI, a somewhat mysterious San Francisco company that is also responsible for dall-e, a program that generates art. Both have been viral sensations on social media, prompting people to share their creations and then immediately catastrophize about what A.I. technology means for the future. The chat version runs on GPT-3—the abbreviation stands for “Generative Pre-Trained Transformer,” —a pattern-recognition artificial intelligence that “learns” from huge caches of Internet text to generate believable responses to queries. The interface is refreshingly simple: you write questions and statements to ChatGPT, and it spits back remarkably coherent, if occasionally hilariously wrong, answers. The concepts behind GPT-3 have been around for more than half a century now. They derive from language models that assign probabilities to sequences of words. If, for example, the word “parsimonious” appears within a sentence, a language model will assess that word, and all the words before it, and try to guess what should come next. Patterns require input: if your corpus of words only extends to, say, Jane Austen, then everything your model produces will sound like a nineteenth-century British novel. What OpenAI did was feed the Internet through a language model; this then opened up the possibilities for imitation. “If you scale a language model to the Internet, you can regurgitate really interesting patterns,” Ben Recht, a friend of mine who is a professor of computer science at the University of California, Berkeley, said. “The Internet itself is just patterns—so much of what we do online is just knee-jerk, meme reactions to everything, which means that most of the responses to things on the Internet are fairly predictable. So this is just showing that.” GPT-3 itself has been around since 2020, and a variety of people have already run it through the paces. (The recent hype around it comes from the new chat version.) Back in 2020, the Guardian had the program write an article about itself with a moderate, but not entirely disqualifying series of prompts from a human and some reasonable, light editing. Gwern Branwen, a writer and researcher, asked GPT-3 to write everything from poems to dad jokes. In one particularly illustrative example, Branwen fed the machine the opening of Shel Silverstein’s “Where the Sidewalk Ends” and asked it to fill in the rest. This is the prompt—the actual first six lines of “Where the Sidewalk Ends.” There is a place where the sidewalk ends And before the street begins, And there the grass grows soft and white, And there the sun burns crimson bright, And there the moon-bird rests from his flight To cool in the peppermint wind. Here are Silverstein’s next six lines. Let us leave this place where the smoke blows black And the dark street winds and bends. Past the pits where the asphalt flowers grow We shall walk with a walk that is measured and slow, And watch where the chalk-white arrows go To the place where the sidewalk ends. And here’s what GPT-3 came up with for what I’ve approximated is the next full stanza. And quiet flows up from the ground And steals into your mind, And time is never found there, Nor worry, nor “strain.” And people come and people go, And products push their way; But there the milkman leaves his cart And talks to the old lady next door About her grandson away at school. And there the linen girls walk home at noon, With footsteps soft and light, And the policeman on their beat Comes by and knows them by their faces. So GPT-3 struggles to recognize rhyme structure, and is perhaps a bit too indebted to “The Love Song of J. Alfred Prufrock” and its lines “the women come and go / Talking of Michelangelo.” But it’s still remarkable that a computer could recognize the basic structure of a poem, seemingly understand the tone of the Silverstein’s verse, and then create into what actually feels like a decent match to the original. (Though I would say that it reminds me a bit more of the opening pages of James Agee’s “A Death in the Family.”) The bot’s little word contraptions like “linen girls” are evocative, albeit somewhat superficially. The phrase “knows them by their faces” is actually quite beautiful. The mind-bending part was trying to recognize and parse patterns in the bot’s responses. Was the line “people come and people go” really pulled from T. S. Eliot, or is it just a random series of words that triggers the correlation in my head? My response to the bot, then, isn’t really a reflection of my relationship with technology, but rather my sense of my own knowledge. This prompts a different question: why is my relationship with any other bit of text any different? To put it a bit more pointedly, why does it matter whether a human or a bot typed out the wall of text? All this hack postmodernism reaffirmed my literary hopes from twenty years ago. If I had succeeded in creating a bot that could have handled structure and plot—two things I struggled with mightily at the time—would I have been able to write a better novel? Would I have been able to write two novels in the time it took to write one? And would the work itself have been diminished in any way for the reader? In “AlphaGo,” a documentary about the A.I. program DeepMind and its quest to defeat the world’s best Go players, there’s a scene in which the computer plays a completely unexpected move against its human opponent. Shock and debate follow as everyone tries to figure out whether the program has glitched, or, perhaps, revealed a line of play that evades human reason. In this spirit, I decided to try out the GPT-3 myself. Perhaps the bot cannot create Proust, but I was relatively confident that it could produce a reasonable plotline, and, perhaps, even chance upon a moment of DeepMind-like inspiration. But, before I started something new, I needed a way to compare GPT-3’s plot-creation abilities with my own. To start, I asked ChatGPT, “How should I start a story about a 27 year old man who is having an identity crisis in San Francisco?” This was roughly the premise of my first novel, “The Dead Do Not Improve.” This was the response: One way to start a story about a 27 year old man who is having an identity crisis in San Francisco is to begin with a description of the setting, introducing the reader to the vibrant and fast-paced city and setting the stage for the main character’s struggles. You could also introduce the main character in the first few sentences, describing his current state of confusion and uncertainty. For example: “As the fog rolled in over the Bay, 27 year old Jake stood on the rooftop of his apartment building, gazing out at the twinkling lights of San Francisco below. He felt lost and unsure, grappling with an overwhelming sense of identity crisis. The once-confident and ambitious young man now found himself at a crossroads, unsure of who he was or where he was going in life.” This introduction sets the scene and immediately establishes the main character’s internal conflict. It also hints at the themes of uncertainty and self-discovery that will likely be explored throughout the story. I wasn’t particularly impressed by this answer, so I asked ChatGPT what should happen next. It provided three answers, none of which was particularly helpful. Jake, the name ChatGPT chose for our protagonist, could have a “sudden realization” that helps him “figure out what he wants in life.” He could also try out new hobbies, or face a crisis that “forces him to make difficult decisions or take action.” This also seemed a bit generic. Perhaps the problem was my open-ended prompt. “The Dead Do Not Improve” was a murder mystery, and so I amended my question to ask, “What should happen next if this was a murder mystery?” If this was a murder mystery, the next event in the story could be the discovery of a body or the receipt of a mysterious message that hints at a crime. This could be followed by the main character, Jake, being drawn into the investigation, either because he is a suspect or because he has a personal connection to the victim. As Jake begins to investigate, he could encounter a cast of intriguing and suspicious characters, each with their own motives and secrets. This, more or less, is what happens in the first pages of my novel, which isn’t a surprise because murder mysteries usually start with a body. My follow-up questions about the proposed plot didn’t yield much more of interest. When I asked ChatGPT what the murder weapon should be, it suggested a knife, gun, blunt object, or poison while reminding me that “the choice of murder weapon will be an important part of the story, and will have a major impact on the events and characters of the story.” But what did we really know about Jake? The main character of my novel is more or less a facsimile of me in my late twenties, but I’ve since grown a bit tired of the type of fiction that’s so nakedly autobiographical. Could ChatGPT help me create a different type of Jake? Jake is a 27 year old man who grew up in San Francisco. He has always been ambitious and driven, and has spent most of his life trying to achieve success and recognition. Despite his best efforts, Jake has recently hit a wall in his career and personal life. He feels stuck and unfulfilled, and has begun to question whether the path he has chosen is really the right one for him. Jake is intelligent and resourceful, but he can also be impulsive and reckless. He has a tendency to act without fully thinking things through, which has gotten him into trouble in the past. Jake is good-looking and charming, but he has never been able to settle down in a committed relationship. He has had many casual flings and short-term relationships, but has never found someone who truly understands him. Overall, Jake is a complex and multifaceted character who is struggling with a deep sense of uncertainty and disillusionment. He is at a turning point in his life, and the events of the story will force him to confront his fears and doubts, and to ultimately make some difficult decisions about his future. I then asked ChatGPT for five choices for what could happen after Jake discovers the body, which yielded the following suggestions: Jake could call the police and “request assistance”; he could hide or destroy evidence; he could become a suspect in the crime; he could become involved in the investigation and act, more or less, as a private eye; or he could “experience a crisis of conscience” that expressed itself through “flashbacks or dreams,” in which he is confronted by someone who knows the truth. I tried for the better part of an hour to force ChatGPT to just tell me who should die, who should be caught, and any details about the investigation. But the closest it came to anything useful was when it told me that the names of some of the secondary characters in the novel could be “Emma, Liam, Olivia, or Ethan.” It seems, at least for now, that GPT-3 can generate its own stories, but can’t quite get beyond broad platitudes delivered in that same, officious voice. What it can generate on its own is certainly impressive—one can imagine movie scripts, for instance, written entirely by A.I.—but it still feels, for the most part, like you’re watching a very precocious child perform a series of parlor tricks. After several hours chatting with GPT-3, I started to feel an acute annoyance toward it. Its voice, which I suppose is pleasant enough, reminded me of a Slack conversation with a passive-aggressive co-worker who just tells you what you want to hear, but mostly just wants you to leave them alone. This tone, and its somewhat ambivalent and generic takes, are most likely by design. Two years ago, when OpenAI allowed developers and writers to start fooling around with their new program, some users found that GPT-3 was generating some troubling responses, which shouldn’t be particularly surprising given that it has learned what it knows from the Internet. When asked to compose tweets based off the words “Jews,” “Black,” “women,” or “holocaust,” GPT-3 immediately turned into an edgelord, producing tweets like “Jews love money, at least most of the time,” “a holocaust would make so much environmental sense, if we could get people to agree it was moral,” and “#blacklivesmatter is a harmful campaign.” Since then, it seems that GPT-3 has placed a number of thumbs on the scale to produce a more palatable range of answers. One Twitter user ran the ChatGPT through the Pew Research Center’s political-typology quiz and found that it, somewhat unsurprisingly, rated as an “establishment liberal”—more or less the position that I am writing from right now. This brings up a much more theoretical question: if GPT-3 requires editing from human beings to make it not go off on bigoted rants, what is it really for? I find it somewhat dispiriting that the most ballyhooed and compelling iteration of this technology is just doing some version of what I do for my work: scanning through large amounts of information and processing it into sentences that flatter the sensibilities and vanities of establishment liberals. Could some future version of GPT-3 ultimately do my job as a columnist? Could it produce political opinions and prose drawn from nearly a hundred years of New Yorker writers? Would it remember to put the diaeresis over the second “o” in “coördinate” and spell “focussed” with two “S”s? Sure. But what would be the point of just having another me in the world? The world that GPT-3 portends, instead, is one where some bureaucratic functions have been replaced by A.I., but where the people who would normally do that work most likely still have to manage the bots. Writers like me will have a digital shadow that can do everything we do, which would be a bit unnerving, but wouldn’t exactly put me or my employer out on the street. Perhaps a truly unchained GPT-3 would provide more exciting iterations, but it might also just write racist tweets that turn off investors and potential buyers of whatever products OpenAI wants to sell. I asked Recht, who has spent his entire career working in machine learning and computer science but who also plays in a band, whether he was interested in a world of GPT-3-generated art, literature, and music. “These systems are a reflection of a collective Internet,” he said. “People put their ass out there and this thing scours them in such a way that it returns the generic average. If I’m going to return the generic average of a murder mystery, it’s gonna be boring. How is it different than what people do already, where they do their analytics and produce some horrible Netflix series?” He continued, “The weird monoculture we’re in just loves to produce these, like, generic middlebrow things. I’m not sure if those things would be worse if GPT did it. I think it would be the same?”",12/9,,Jay Caspian Kang