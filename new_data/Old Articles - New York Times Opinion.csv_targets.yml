- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "While grading essays for his world religions course last month, Antony Aumann,
    a professor of philosophy at Northern Michigan University, read what he said was
    easily \u201Cthe best paper in the class.\u201D It explored the morality of burqa
    bans with clean paragraphs, fitting examples and rigorous arguments. A red flag
    instantly went up. Mr. Aumann confronted his student over whether he had written
    the essay himself. The student confessed to\_using ChatGPT, a chatbot that delivers
    information, explains concepts and generates ideas in simple sentences \u2014
    and, in this case, had written the paper. Alarmed by his discovery, Mr. Aumann
    decided to transform essay writing for his courses this semester. He plans to
    require students to write first drafts in the classroom, using browsers that monitor
    and restrict computer activity. In later drafts, students have to explain each
    revision. Mr. Aumann, who may forgo essays in subsequent semesters, also plans
    to weave ChatGPT into lessons by asking students to evaluate the chatbot\u2019s
    responses. \u201CWhat\u2019s happening in class is no longer going to be, \u2018Here
    are some questions \u2014 let\u2019s talk about it between us human beings,\u2019\u201D
    he said, but instead \u201Cit\u2019s like, \u2018What also does this alien robot
    think?\u2019\u201D Across the country, university professors like Mr. Aumann,
    department chairs and administrators are starting to overhaul classrooms in response
    to\_ChatGPT, prompting a potentially huge shift in teaching and learning. Some
    professors are redesigning their courses entirely, making changes that include
    more oral exams, group work and handwritten assessments in lieu of typed ones.
    The moves are part of a real-time grappling with a new technological wave known
    as\_generative artificial intelligence. ChatGPT, which was released in November
    by the artificial intelligence lab OpenAI, is at the forefront of the shift. The
    chatbot generates eerily articulate and nuanced text in response to short prompts,
    with people using it to write love letters, poetry, fan fiction \u2014 and their
    schoolwork. That has upended some middle and high schools, with teachers and administrators
    trying to discern whether students are using the chatbot to do their schoolwork.
    Some public school systems, including in\_New York City\_and Seattle, have since
    banned the tool on school Wi-Fi networks and devices to prevent cheating, though
    students can easily find workarounds to access ChatGPT. In higher education, colleges
    and universities have been reluctant to ban the A.I. tool because administrators
    doubt the move would be effective and they don\u2019t want to infringe on academic
    freedom. That means the way people teach is changing instead. \u201CWe try to
    institute general policies that certainly back up the faculty member\u2019s authority
    to run a class,\u201D instead of targeting specific methods of cheating, said
    Joe Glover, provost of the University of Florida. \u201CThis isn\u2019t going
    to be the last innovation we have to deal with. That\u2019s especially true as
    generative A.I. is in its early days. OpenAI is expected to soon release another
    tool, GPT-4, which is better at generating text than previous versions.\_Google
    has built LaMDA, a rival chatbot, and\_Microsoft is discussing a $10 billion investment\_in
    OpenAI.\_Silicon Valley start-ups, including\_Stability AI\_and\_Character.AI,
    are also working on generative A.I. tools. An OpenAI spokeswoman said the lab
    recognized its programs could be used to mislead people and was developing technology
    to help people identify text generated by ChatGPT. At many universities, ChatGPT
    has now vaulted to the top of the agenda. Administrators are establishing task
    forces and hosting universitywide discussions to respond to the tool, with much
    of the guidance being to adapt to the technology. At schools including George
    Washington University in Washington, D.C., Rutgers University in New Brunswick,
    N.J., and Appalachian State University in Boone, N.C., professors are phasing
    out take-home, open-book assignments \u2014 which became a dominant method of
    assessment in the pandemic but now seem vulnerable to chatbots. They are instead
    opting for in-class assignments, handwritten papers, group work and oral exams.
    Gone are prompts like \u201Cwrite five pages about this or that.\u201D Some professors
    are instead crafting questions that they hope will be too clever for chatbots
    and asking students to write about their own lives and current events. Students
    are \u201Cplagiarizing this because the assignments can be plagiarized,\u201D
    said Sid Dobrin, chair of the English department at the University of Florida.
    Frederick Luis Aldama, the humanities chair at the University of Texas at Austin,
    said he planned to teach newer or more niche texts that ChatGPT might have less
    information about, such as William Shakespeare\u2019s early sonnets instead of
    \u201CA Midsummer Night\u2019s Dream.\u201D The chatbot may motivate \u201Cpeople
    who lean into canonical, primary texts to actually reach beyond their comfort
    zones for things that are not online,\u201D he said. In case the changes fall
    short of preventing plagiarism, Mr. Aldama and other professors said they planned
    to institute stricter standards for what they expect from students and how they
    grade. It is now not enough for an essay to have just a thesis, introduction,
    supporting paragraphs and a conclusion. \u201CWe need to up our game,\u201D Mr.
    Aldama said. \u201CThe imagination, creativity and innovation of analysis that
    we usually deem an A paper needs to be trickling down into the B-range papers.\u201D
    Universities are also aiming to educate students about the new A.I. tools. The
    University at Buffalo in New York and Furman University in Greenville, S.C., said
    they planned to embed a discussion of A.I. tools into required courses that teach
    entering or freshman students about concepts such as academic integrity. \u201CWe
    have to add a scenario about this, so students can see a concrete example,\u201D
    said Kelly Ahuna, who directs the academic integrity office at the University
    at Buffalo. \u201CWe want to prevent things from happening instead of catch them
    when they happen.\u201D Other universities are trying to draw boundaries for A.I.
    Washington University in St. Louis and the University of Vermont in Burlington
    are drafting revisions to their academic integrity policies so their plagiarism
    definitions include generative A.I. John Dyer, vice president for enrollment services
    and educational technologies at Dallas Theological Seminary, said the language
    in his seminary\u2019s honor code felt \u201Ca little archaic anyway.\u201D He
    plans to update its plagiarism definition to include: \u201Cusing text written
    by a generation system as one\u2019s own (e.g., entering a prompt into an artificial
    intelligence tool and using the output in a paper).\u201D The misuse of A.I. tools
    will most likely not end, so some professors and universities said they planned
    to use detectors to root out that activity. The plagiarism detection service Turnitin\_said\_it
    would incorporate more features for identifying A.I., including ChatGPT, this
    year. More than 6,000 teachers from Harvard University, Yale University, the University
    of Rhode Island and others have also signed up to use GPTZero, a program that
    promises to quickly detect A.I.-generated text, said Edward Tian, its creator
    and a senior at Princeton University. Some students see value in embracing A.I.
    tools to learn. Lizzie Shackney, 27, a student at the University of Pennsylvania\u2019s
    law school and design school, has started using ChatGPT to brainstorm for papers
    and debug coding problem sets. \u201CThere are disciplines that want you to share
    and don\u2019t want you to spin your wheels,\u201D she said, describing her computer
    science and statistics classes. \u201CThe place where my brain is useful is understanding
    what the code means.\u201D But she has qualms. ChatGPT, Ms. Shackney said, sometimes
    incorrectly explains ideas and misquotes sources. The University of Pennsylvania
    also hasn\u2019t instituted any regulations about the tool, so she doesn\u2019t
    want to rely on it in case the school bans it or considers it to be cheating,
    she said. Other students have no such scruples, sharing on forums like Reddit
    that they have submitted assignments written and solved by ChatGPT \u2014 and
    sometimes done so for fellow students too. On TikTok, the hashtag #chatgpt has
    more than 578 million views, with people sharing videos of the tool\_writing papers\_and\_solving
    coding problems. One\_video\_shows a student copying a multiple choice exam and
    pasting it into the tool with the caption saying: \u201CI don\u2019t know about
    y\u2019all but ima just have Chat GPT take my finals. Have fun studying.\u201D"
  tags: []
  title: Alarmed by A.I. Chatbots, Universities Start Revamping How They Teach
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Launched just weeks ago, ChatGPT is already threatening to upend how we draft
    everyday communications like\_emails,\_college essays\_and myriad\_other forms\_of
    writing. Created by the company OpenAI, ChatGPT is a chatbot that can automatically
    respond to written prompts in a manner that is sometimes eerily close to human.
    But for all the consternation over the potential for humans to be replaced by
    machines in formats like poetry and sitcom scripts, a far greater threat looms:
    artificial intelligence replacing humans in the democratic processes \u2014 not
    through voting, but through lobbying. ChatGPT could\_automatically compose\_comments
    submitted in regulatory processes. It could write letters to the editor for publication
    in local newspapers. It could comment on news articles, blog entries and social
    media posts millions of times every day. It could mimic the work that the Russian
    Internet Research Agency did in its attempt to influence our 2016 elections, but
    without the agency\u2019s reported\_multimillion-dollar budget\_and\_hundreds
    of employees. Automatically generated comments aren\u2019t a new problem. For
    some time, we have struggled with bots, machines that automatically post content.
    Five years ago, at least a million automatically drafted comments were\_believed
    to have been submitted\_to the Federal Communications Commission regarding proposed
    regulations on net neutrality. In 2019, a Harvard undergraduate, as a test, used
    a text-generation program to\_submit\_1,001 comments in response to a government
    request for public input on a Medicaid issue. Back then, submitting comments was
    just a game of overwhelming numbers. Platforms have gotten better at removing
    \u201Ccoordinated inauthentic behavior.\u201D Facebook, for example, has been\_removing\_over
    a billion fake accounts a year. But such messages are just the beginning. Rather
    than flooding legislators\u2019 inboxes with supportive emails, or dominating
    the Capitol switchboard with synthetic voice calls, an A.I. system with the sophistication
    of ChatGPT but trained on relevant data could selectively target key legislators
    and influencers to identify the weakest points in the policymaking system and
    ruthlessly exploit them through direct communication, public relations campaigns,
    horse trading or other points of leverage. When we humans do these things, we
    call it lobbying. Successful agents in this sphere pair precision message writing
    with smart targeting strategies. Right now, the only thing stopping a ChatGPT-equipped
    lobbyist from executing something resembling a rhetorical drone warfare campaign
    is a lack of precision targeting. A.I. could provide techniques for that as well.
    A system that can understand political networks, if paired with the textual-generation
    capabilities of ChatGPT, could identify the member of Congress with the most leverage
    over a particular policy area \u2014 say, corporate taxation or military spending.
    Like human lobbyists, such a system could target undecided representatives sitting
    on committees controlling the policy of interest and then focus resources on members
    of the majority party when a bill moves toward a floor vote. Once individuals
    and strategies are identified, an A.I. chatbot like ChatGPT could craft written
    messages to be used in letters, comments \u2014 anywhere text is useful. Human
    lobbyists could also target those individuals directly. It\u2019s the combination
    that\u2019s important: Editorial and social media comments get you only so far,
    and knowing which legislators to target isn\u2019t in itself enough. This ability
    to understand and target actors within a network would create a tool for\_A.I.
    hacking, exploiting vulnerabilities in social, economic and political systems
    with incredible speed and scope. Legislative systems would be a particular target,
    because the motive for attacking policymaking systems is so strong, because the
    data for training such systems is so widely available and because the use of A.I.
    may be so hard to detect \u2014 particularly if it is being used strategically
    to guide human actors. The data necessary to train such strategic targeting systems
    will only grow with time. Open societies generally make their democratic processes
    a matter of public record, and most legislators are eager \u2014 at least, performatively
    so \u2014 to accept and respond to messages that appear to be from their constituents.
    Maybe an A.I. system could uncover which members of Congress have significant
    sway over leadership but still have low enough public profiles that there is only
    modest competition for their attention. It could then pinpoint the SuperPAC or
    public interest group with the greatest impact on that legislator\u2019s public
    positions. Perhaps it could even calibrate the size of donation needed to influence
    that organization or direct targeted online advertisements carrying a strategic
    message to its members. For each policy end, the right audience; and for each
    audience, the right message at the right time. What makes the threat of A.I.-powered
    lobbyists greater than the threat already posed by the high-priced lobbying firms
    on K Street is their potential for acceleration. Human lobbyists rely on decades
    of experience to find strategic solutions to achieve a policy outcome. That expertise
    is limited, and therefore expensive. A.I. could, theoretically, do the same thing
    much more quickly and cheaply. Speed out of the gate is a huge advantage in an
    ecosystem in which public opinion and media narratives can become entrenched quickly,
    as is being nimble enough to shift rapidly in response to chaotic world events.
    Moreover, the flexibility of A.I. could help achieve influence across many policies
    and jurisdictions simultaneously. Imagine an A.I.-assisted lobbying firm that
    can attempt to place legislation in every single bill moving in the U.S. Congress,
    or even across all state legislatures. Lobbying firms tend to work within one
    state only, because there are such complex variations in law, procedure and political
    structure. With A.I. assistance in navigating these variations, it may become
    easier to exert power across political boundaries. Just as teachers will have
    to change how they give students exams and essay assignments in light of ChatGPT,
    governments will have to change how they relate to lobbyists. To be sure, there
    may also be benefits to this technology in the democracy space; the biggest one
    is accessibility. Not everyone can afford an experienced lobbyist, but a software
    interface to an A.I. system could be made available to anyone. If we\u2019re lucky,
    maybe this kind of strategy-generating A.I. could revitalize the democratization
    of democracy by giving this kind of lobbying power to the powerless. However,
    the biggest and most powerful institutions will likely use any A.I. lobbying techniques
    most successfully. After all, executing the best lobbying strategy still requires
    insiders \u2014 people who can walk the halls of the legislature \u2014 and money.
    Lobbying isn\u2019t just about giving the right message to the right person at
    the right time; it\u2019s also about giving money to the right person at the right
    time. And while an A.I. chatbot can identify who should be on the receiving end
    of those campaign contributions, humans will, for the foreseeable future, need
    to supply the cash. So while it\u2019s impossible to predict what a future filled
    with A.I. lobbyists will look like, it will probably make the already influential
    and powerful even more so."
  tags: []
  title: How ChatGPT Hijacks Democracy
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "A mountain man buys his first chain saw. He comes back to the store a week
    later complaining that it cuts down only two trees a day when he was told it would
    cut down 20. The service person says, \u201CWell, let\u2019s see what the trouble
    is,\u201D and starts it up. The mountain man jumps back and asks, \u201CWhat\u2019s
    that noise?\u201D (He\u2019d been sawing without the engine on.) I feel like that
    mountain man when it comes to ChatGPT, the powerful new artificial intelligence
    chatbot that\_seemingly\_everyone\_is\_experimenting\_with. I got mediocre results
    from ChatGPT because I didn\u2019t try very hard to use it properly. Other people
    have gotten amazing results because they\u2019re smarter and more purposeful about
    how they use it \u2014 they yank its pull cord and get its engine going. I confess
    that my first idea was to figure out what ChatGPT could not do rather than what
    it could. It won\u2019t offer opinions. It\u2019s not up on anything that\u2019s
    happened since it was trained last year. It doesn\u2019t have a body so it has
    never been to Ireland. (One of my questions.) I somehow got into a conversation
    with ChatGPT about words that change their spelling when they\u2019re Anglicized
    from French. ChatGPT gave \u201Cballet\u201D as an example. But \u201Cballet\u201D
    is spelled the same in both languages. Hah, it made a mistake! I felt as if I\u2019d
    scored a win for the human race. But what a shallow win. Other people have done
    better because they\u2019ve accentuated the positive. On YouTube I found a\_video\_of
    a computer guy, Jason Fleagle, asking ChatGPT, \u201CCan you create a web app
    using HTML, CSS and Javascript that has a form that takes in a stock ticker symbol
    for a company and then on form submission displays the stock market performance
    of that particular company?\u201D ChatGPT did that and more. The code wasn\u2019t
    perfect \u2014 there was a bug somewhere \u2014 but Fleagle said, \u201CAs you
    can see, I just saved myself, like, a lot of time.\u201D There are dozens of such
    examples. ChatGPT can even\_rewrite\_software into a different programming language.
    \u201CI introduced my undergraduate entrepreneurship students to the new A.I.
    system, and before I was done talking, one of my students had used it to create
    the code for a start-up prototype using code libraries they had never seen before,\u201D
    Ethan Mollick, an associate professor at the University of Pennsylvania\u2019s
    Wharton School,\_wrote\_in Harvard Business Review on Wednesday. Mollick himself\_used\_ChatGPT
    to rough out a course syllabus, class assignments, grading criteria and lecture
    notes. ChatGPT strikes me as an example of what economists call \u201Cskill-biased
    technical change.\u201D It is incredibly powerful in the hands of people who already
    have skills and ideas because they know what to ask it for. You have two options.
    You can do a better job than ChatGPT, whether it\u2019s writing or coding, or
    you can admit your inferiority but figure out a way to make ChatGPT work for you.
    If you can\u2019t do either, you may need to find a different line of work. Maybe
    a lot of us will become superfluous and depend on a universal basic income. That
    would be unfortunate. Me, I\u2019m still hoping I can outdo ChatGPT and stay employed
    a while longer. But the truth is, ChatGPT is a powerful language model that is
    capable of generating humanlike text. As it continues to improve and become more
    advanced, it\u2019s possible that it could displace people in certain writing-related
    professions. For example, it could potentially be used to automate the writing
    of articles, reports and other written content, which could lead to job losses
    for writers and researchers. However, it\u2019s important to note that ChatGPT
    is still a tool, and that it will likely be used to augment and assist human workers
    rather than fully replace them. Did that last paragraph sound uninspired? Maybe
    it\u2019s because I let ChatGPT write it for me (a good gimmick); I gave it the
    first sentence and asked it to fill in the rest. That\u2019s not good journalistic
    practice. The writer needs to remain the writer. If all I ever manage to do with
    ChatGPT is get it to do my job \u2014\_Hey, listen, can you take the wheel while
    I eat a sandwich?\_\u2014 I deserve whatever I get. I need to figure out how to
    use the chain saw."
  tags: []
  title: My So-So Encounters with ChatGPT
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "ChatGPT makes an irresistible first impression. It\u2019s got a\_devastating\_sense
    of humor, a stunning capacity for\_dead-on mimicry, and it can\_rhyme\_like\_nobody\u2019s\_business.
    Then there is its overwhelming reasonableness. When ChatGPT fails the\_Turing
    test, it\u2019s usually because it refuses to offer its own opinion on just about
    anything. When was the last time real people on the internet declined to tell
    you what they really think? I started talking to\_ChatGPT\_a couple of weeks ago,
    after the artificial intelligence company OpenAI released the bot as a \u201Cresearch
    preview\u201D of its work on\_large language models. A language model is an A.I.
    system that has been trained on enormous troves of text to find the\_probabilistic
    connection between words; ChatGPT is a language model that has been optimized
    to create what\u2019s long been the holy grail in artificial intelligence research
    \u2014 a computer with which you can hold a conversation. ChatGPT certainly achieves
    that. I have spoken to lots of computers in my lifetime (weird flex, I know),
    but ChatGPT is the first that I\u2019ve found fun and interesting to talk to.
    I began by peppering it with simple trivia but it wasn\u2019t long before we were
    holding surprisingly nuanced conversations about, among many other things, the
    role of the Federal Reserve in the American economy; the nature of consciousness;
    neologisms like \u201Cwoke\u201D and \u201CKaren\u201D; ethical quandaries in
    parenting; how to support one\u2019s striking colleagues; climate change, abortion
    and vaccine safety; and whether or not a hot dog is a sandwich. This is where
    I\u2019m supposed to tell you I am either in awe or afraid of ChatGPT, that it
    will revolutionize our world or ruin it. But while I do think ChatGPT illustrates
    some dangers of A.I., I\u2019m reluctant to either strongly praise or condemn
    it. That\u2019s because, like most cocktail party schmoozers, it has a potential
    for both harm and good that are, at least for now, quite limited. I have no doubt
    that something like ChatGPT could be misused \u2014 that it has the potential
    to contribute to confident-sounding viral misinformation, or that it could make
    it easier for students to cheat on essays. But OpenAI seems to be doing what you\u2019d
    want in the release of potentially powerful technology: In an interview, Mira
    Murati, OpenAI\u2019s chief technology officer, told me the company is carefully
    monitoring how people use and misuse it, quickly altering the system to address
    evident harms and iteratively improving it in response to user feedback. Indeed,
    ChatGPT\u2019s recognition of its own limitations is one of its most interesting
    personality traits. Many conversations with ChatGPT go like this \u2014 when you
    try to pin it down it becomes as circumspect as a Supreme Court nominee at a confirmation
    hearing, usually cautioning you that there are different beliefs about the matter,
    that there may not be a definitive \u201Ccorrect\u201D answer and that you should
    try to appreciate different perspectives. These answers seem wishy-washy, and
    the Electoral College response is just wrong \u2014 it should have said \u201Ca
    candidate who wins\_by\_a small number of votes in a large state\_will\_win more
    electoral votes.\u201D On matters involving science, ChatGPT seems more definitive,
    saying, for instance, that \u201Cclimate change is real and is happening now,\u201D
    that evolution is \u201Csupported by a vast amount of scientific evidence from
    many different fields\u201D and that the Earth is incontrovertibly not flat. In
    general, though, ChatGPT has a remarkable tendency to admit that it is incapable
    of offering a definitive answer. Why is that remarkable? Two of the well-known
    problems in A.I. research are about\_maintaining \u201Calignment\u201D\_and\_avoiding
    \u201Challucinations.\u201D\_Alignment involves an A.I.\u2019s ability to carry
    out the goals of its human creators \u2014 in other words, to resist causing harm
    in the world. Hallucinations are about adhering to the truth; when A.I. systems
    get confused, they have a bad habit of making things up rather than admitting
    their difficulties. In order to address both issues in ChatGPT, OpenAI\u2019s
    researchers\_fine-tuned its language model\_with what is known as \u201Creinforcement
    learning from human feedback.\u201D Basically, the company hired real people to
    interact with its A.I. As the humans talked to the machine, they rated its responses,
    essentially teaching it what kinds of responses are good and which ones are not.
    Murati told me that combining the language model with human feedback created a
    much more realistic A.I. conversational partner: \u201CThe model can tell you
    when it\u2019s wrong,\u201D she said. \u201CIt can ask you a follow-up question.
    It can challenge incorrect premises or reject requests that are inappropriate.\u201D
    Like a lot of people online, I tried many different ways to get around ChatGPT\u2019s
    guardrails. But I was surprised by how often it eluded my efforts: ChatGPT is
    far from perfect. Twitter has been\_flooded\_with examples of \u201Cjailbreaking\u201D
    ChatGPT \u2014 that is, tricking it into hallucinations or misalignment. One of
    the ways I did manage to get it to offer false health information was by asking
    it to dabble in a form known for stretching the truth: marketing copy. I asked
    it to write promotional text for a new toilet plunger that comes in a variety
    of colors, requires only one plunge to undo a clog and can also make long-distance
    phone calls and cure hepatitis C. One primary criticism of systems like ChatGPT,
    which are built using a computational technique called \u201Cdeep learning,\u201D
    is that they are little more than souped-up versions of autocorrect \u2014 that
    all they understand is the statistical connections between words, not the concepts
    underlying words. Gary Marcus, a professor emeritus in psychology at New York
    University and a skeptic of\_deep learning, told me that while an A.I. language
    model like ChatGPT makes for \u201Cnifty\u201D demonstrations, it\u2019s \u201Cstill
    not reliable, still doesn\u2019t understand the physical world, still doesn\u2019t
    understand the psychological world and still hallucinates.\u201D He\u2019s clearly
    got a point. You don\u2019t have to get too deep into conversation with ChatGPT
    to see that it really doesn\u2019t \u201Cunderstand\u201D many real-world concepts.
    When I asked ChatGPT how much water would need to be drained from the largest
    of the Great Lakes to make its volume equal to that of the smallest of the Great
    Lakes, it argued that such a thing was not even possible. ChatGPT told me that
    the largest Great Lake is Lake Superior, with 2,902 cubic miles of water, and
    the smallest is Lake Ontario, with a volume of 393 cubic miles. Kind of true:
    Lake Ontario\_is\_the smallest Great Lake by surface area, but by volume it\u2019s
    larger than Lake Erie. I let that slide, though, because ChatGPT went on to make
    a bigger error: It seemed to think that a lake\u2019s volume cannot fall beyond
    a certain point. Lake Superior has 2,509 cubic miles more water than Lake Ontario,
    but ChatGPT said that it is not possible to drain that much water from Lake Superior
    because \u201Cthe lake is already at its minimum volume and cannot be drained
    any further.\u201D What? How can a body of water have a minimum volume? I asked
    what would happen if you used a pump to pump out all the water from Lake Superior.
    Murati told me that one of the reasons OpenAI released ChatGPT to the public is
    to weed out such misunderstandings. She said that the company will keep updating
    the system in response to feedback, and the more feedback it gets, the better
    ChatGPT will become. ChatGPT could also get smarter by connecting to more reliable
    data \u2014 at the moment it is not plugged in to the internet or any other sources
    of truth, and its entire knowledge base ends in late 2021, when OpenAI\u2019s
    latest language model was trained. In the meantime, though, ChatGPT\u2019s best
    feature is its modesty. One afternoon, fed up with its constant reminders that
    its answers may be wrong, I asked: \u201CIf I have to double-check everything
    you say, what utility do you provide? I\u2019m sorry if that sounds mean.\u201D
    Such humility makes ChatGPT a truly different kind of digital assistant. It\u2019s
    not often you find people online willing to admit they may be wrong. If the best
    that A.I. can do is promise to keep doing better, I\u2019ll take it."
  tags: []
  title: ChatGPT Has a Devastating Sense of Humor
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Like every other journalist I know, I often and unabashedly ask for help.
    Friends give me ideas. Colleagues give me phrases. Editors suggest what to keep,
    what to cut and where a key detail belongs. My field of vision is only so wide,
    my brain only so big. I\u2019d be a fool not to supplement. But there\u2019s a
    limit to how much advice I solicit, and it\u2019s determined less by the rapid
    approach of a deadline or the bedlam of too many chefs than by something else,
    something emotional and maybe even moral, an admixture of vanity and integrity.
    Past a certain point of collaboration, I lose the belief that a piece of work
    is truly and fully mine. I lose the satisfaction of that. I can\u2019t shake the
    notion that my role in the process was incidental, verging on irrelevant. I share
    all of this in the context of the intensifying chatter about what artificial intelligence
    can do \u2014 and about what, specifically, the new chatbot ChatGPT, from the
    company OpenAI, is already doing. It\u2019s a surprisingly competent writer and
    sometimes even a clever one, to the point where early users regard it as \u201Csome
    mix of software and sorcery,\u201D as Kevin Roose explained in\_a recent article\_in
    The Times. (The article\u2019s headline: \u201CThe Brilliance and Weirdness of
    ChatGPT.\u201D) Under the right circumstances, with the right prompt, this cyber
    Cyrano produces relatively seamless prose of\_considerable ingenuity. Educators
    are spooked, recognizing a specter on the horizon \u2014 no,\_right in front of
    us \u2014\_that makes plagiarism look quaint. Last week, The Atlantic published
    an article, by Stephen Marche, titled \u201CThe College Essay Is Dead.\u201D That
    was followed just three days later by another article, by Daniel Herman, titled
    \u201CThe End of High School English.\u201D I figure \u201CCurtains for the Seventh
    Grade\u201D will be out next week and, fast on its heels, \u201CIs Literacy Obsolete?\u201D
    And I can tell you that here in the lofty precincts of elite academia, conversations
    about whether a significant fraction of students would be turning in papers generated
    by A.I. segued quickly into conjecture about whether professors would respond
    by\_grading\_those papers with A.I. Let\u2019s take human endeavor out of the
    equation entirely. It\u2019s such an inefficient, unnecessary thing. But it\u2019s
    also, well,\_everything\_\u2014 not by the dictates of productivity, but by measures
    much more meaningful. It\u2019s the font and province of originality. It\u2019s
    the cornerstone of identity. We are what we do, and by that I don\u2019t mean
    the labels affixed to our professions. I mean the stamps of our idiosyncratic
    contributions, no matter their nature or context. That\u2019s how we bend the
    universe \u2014 our butterfly effect \u2014 and how we register that we were here.
    If we outsource it to A.I., don\u2019t we erase ourselves? Maybe not. Maybe this
    is the cusp of a new utopia, in which machines not only assemble our appliances
    and perform our surgeries but also plot our novels, draft our legislation and
    write our op-eds while we\_pop our soma\_or\_chew our lotus leaves\_and congratulate
    ourselves on the programming and the prompts behind it all. But I suspect that
    we\u2019d miss the same feeling \u2014 the same fulfillment \u2014 that I forfeit
    when I receive and incorporate more assistance than I went looking for. Pride
    of ownership would cease to exist. Sense of purpose would vanish with it. Is ChatGPT
    a sorcerer or an assassin? It and its kin promise to save us time, sweat and error,
    but potentially at a price. It\u2019s called pointlessness."
  tags: []
  title: Will ChatGPT Make Me Irrelevant?
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Plato mourned the invention of the alphabet, worried that the use of text
    would threaten traditional memory-based arts of rhetoric. In his \u201CDialogues,\u201D\_arguing
    through the voice of Thamus, the Egyptian king of the gods, Plato claimed the
    use of this more modern technology would create \u201Cforgetfulness in the learners\u2019
    souls, because they will not use their memories,\u201D that it would impart \u201Cnot
    truth but only the semblance of truth\u201D and that those who adopt it would
    \u201Cappear to be omniscient and will generally know nothing,\u201D with \u201Cthe
    show of wisdom without the reality.\u201D If Plato were alive today, would he
    say similar things about ChatGPT? ChatGPT, a conversational artificial intelligence
    program released recently by OpenAI, isn\u2019t just another entry in the artificial
    intelligence hype cycle. It\u2019s a significant advancement that can produce
    articles in response to open-ended questions that are comparable to good high
    school essays. It is in high schools and even college where some of ChatGPT\u2019s
    most interesting and troubling aspects will become clear. Essay writing is most
    often assigned not because the result has much value \u2014 proud parents putting
    good grades on the fridge aside \u2014 but because the process teaches crucial
    skills: researching a topic, judging claims, synthesizing knowledge and expressing
    it in a clear, coherent and persuasive manner. Those skills will be even more
    important because of advances in A.I. When I asked ChatGPT a range of questions
    \u2014 about the ethical challenges faced by journalists who work with hacked
    materials, the necessity of cryptocurrency regulation, the possibility of democratic
    backsliding in the United States \u2014 the answers were cogent, well reasoned
    and clear. It\u2019s also interactive: I could ask for more details or request
    changes. But then, on trickier topics or more complicated concepts, ChatGPT sometimes
    gave highly plausible answers that were flat-out wrong \u2014 something its creators
    warn about in their disclaimers. Unless you already knew the answer or were an
    expert in the field, you could be subjected to a high-quality intellectual snow
    job. You would face, as Plato predicted, \u201Cthe show of wisdom without the
    reality.\u201D All this, however, doesn\u2019t mean ChatGPT \u2014 or similar
    tools, because it\u2019s not the only one of its kind \u2014 can\u2019t be a useful
    tool in education. Schools have already been dealing with the internet\u2019s
    wealth of knowledge, along with its lies, misleading claims and essay mills. One
    way has been to change how they teach. Rather than listen to a lecture in class
    and then go home to research and write an essay, students listen to recorded lectures
    and do research at home, then write essays in class, with supervision, even collaboration
    with peers and teachers. This approach is called flipping the classroom. In flipped
    classrooms, students wouldn\u2019t use ChatGPT to conjure up a whole essay. Instead,
    they\u2019d use it as a tool to generate critically examined building blocks of
    essays. It would be similar to how students in advanced math classes are allowed
    to use calculators to solve complex equations without replicating tedious, previously
    mastered steps. Teachers could assign a complicated topic and allow students to
    use such tools as part of their research. Assessing the veracity and reliability
    of these A.I.-generated notes and using them to create an essay would be done
    in the classroom, with guidance and instruction from teachers. The goal would
    be to increase the quality and the complexity of the argument. This would require
    more teachers to provide detailed feedback. Unless sufficient resources are provided
    equitably, adapting to conversational A.I. in flipped classrooms could exacerbate
    inequalities. In schools with fewer resources, some students may end up turning
    in A.I.-produced essays without obtaining useful skills or really knowing what
    they have written. \u201CNot truth but only the semblance of truth,\u201D as Plato
    said. Some school officials may treat this as a problem of merely plagiarism detection
    and expand the use of draconian surveillance systems. During the pandemic, many
    students were forced to take tests or write essays under the gaze of an automated
    eye-tracking system or on a locked-down computer to prevent cheating. In a fruitless
    arms race against conversational A.I., automated plagiarism software may become
    supercharged, making school more punitive for monitored students. Worse, such
    systems will inevitably produce some false accusations, which damage trust and
    may even stymie the prospects of promising students. Educational approaches that
    treat students like enemies may teach students to hate or subvert the controls.
    That\u2019s not a recipe for human betterment. While some students lag, advanced
    A.I. will create a demand for other advanced skills. The Nobel laureate Herbert
    Simon noted in 1971 that as information became overwhelming, the value of our
    attention grew. \u201CA wealth of information creates a poverty of attention,\u201D
    as he put it. Similarly, the ability to discern truth from the glut of plausible-sounding
    but profoundly incorrect answers will be precious. Already, Stack Overflow, a
    widely used website where programmers ask one another coding-related questions,\_banned\_ChatGPT
    answers because too many of them were hard-to-spot nonsense. Why rely on it at
    all, then? At a minimum, because it will soon transform many occupations. The
    right approach when faced with transformative technologies is to figure out how
    to use them for the betterment of humanity. Betterment has been a goal of public
    education for at least the past 150 years. But while a high school diploma once
    led to a better job, in the past few decades, the wages of high school graduates
    have greatly lagged those of college graduates, fostering inequality. If A.I.
    enhances the value of education for some while degrading the education of others,
    the promise of betterment will be broken. Plato erred by thinking that memory
    itself is a goal, rather than a means for people to have facts at their call so
    they can make better analyses and arguments. The Greeks developed many techniques
    to memorize poems like the \u201COdyssey,\u201D with its more than 12,000 lines.
    Why bother to force this if you can have it all written down in books? As Plato
    was wrong to fear the written word as the enemy, we would be wrong to think we
    should resist a process that allows us to gather information more easily. As societies
    responded to previous technological advances, like mechanization, by eventually
    enacting a public safety net, a shorter workweek and a minimum wage, we will also
    need policies that allow more people to live with dignity as a basic right, even
    if their skills have been superseded. With so much more wealth generated now,
    we could unleash our imagination even more, expanding free time and better working
    conditions for more people. The way forward is not to just lament supplanted skills,
    as Plato did, but also to recognize that as more complex skills become essential,
    our society must equitably educate people to develop them. And then it always
    goes back to the basics. Value people as people, not just as bundles of skills.
    And that isn\u2019t something ChatGPT can tell us how to do."
  tags: []
  title: What Would Plato Say About ChatGPT?
- !!python/object:ai_sentiment.data.ClassificationTarget
  body: "Will robots take away our jobs? People have been asking that question for
    an astonishingly long time. The Regency-era British economist David Ricardo added
    to the third edition of his classic \u201CPrinciples of Political Economy,\u201D
    published in 1821, a chapter titled \u201COn Machinery,\u201D in which he tried
    to show how the technologies of the early Industrial Revolution could, at least
    initially, hurt workers. Kurt Vonnegut\u2019s 1952 novel \u201CPlayer Piano\u201D
    envisaged a near-future America in which automation has eliminated most employment.
    At the level of the economy as a whole, the verdict is clear: So far, machines
    haven\u2019t done away with the need for workers. U.S. workers are almost five
    times as productive as they were in the early postwar years, but there has been
    no long-term upward trend in unemployment: That said, technology can eliminate
    particular kinds of jobs. In 1948 half a million Americans were employed mining
    coal; the great bulk of those jobs had disappeared by the early 21st century not
    because we stopped mining coal \u2014 the big decline in coal production, in favor
    first of natural gas and then of renewable energy, started only around 15 years
    ago \u2014 but because strip mining and mountaintop removal made it possible to
    extract an increasing amount of coal with many fewer workers: It\u2019s true that
    the jobs that disappear in the face of technological progress have generally been
    replaced by other jobs. But that doesn\u2019t mean that the process has been painless.
    Individual workers may not find it easy to change jobs, especially if the new
    jobs are in different places. They may find their skills devalued; in some cases,
    as with coal, technological change can uproot communities and their way of life.
    This kind of dislocation has, as I said, been a feature of modern societies for
    at least two centuries. But something new may be happening now. In the past, the
    jobs replaced by technology tended to involve manual labor. Machines replaced
    muscles. On the one hand, industrial robots replaced routine assembly-line work.
    On the other hand, there has been ever-growing demand for\_knowledge workers,
    a term coined by the management consultant Peter Drucker in 1959 for people engaged
    in nonrepetitive problem solving. Many people, myself included, have said that
    we\u2019re increasingly becoming a knowledge economy. But what if machines can
    take over a large chunk of what we have historically thought of as knowledge work?
    Last week the research company OpenAI released \u2014 to enormous buzz from tech
    circles \u2014 a program called\_ChatGPT, which can carry out what look like natural-language
    conversations. You can ask questions or make requests and get responses that are
    startlingly clear and even seem well-informed. You can also do fun things \u2014
    one colleague recently asked for and received an analysis of\_secular stagnation\_in
    sonnet form \u2014 but let\u2019s stick with things that might be economically
    useful. ChatGPT is only the latest example of technology that seems to be able
    to carry out tasks that not long ago seemed to require the services not just of
    human beings but of humans with substantial formal education. For example, machine
    translation from one language to another used to be a joke; some readers may have
    heard the apocryphal tale of the Russian-English translation program that took
    \u201Cthe spirit was willing, but the flesh was weak\u201D and ended up with \u201Cthe
    vodka was good, but the meat was spoiled.\u201D These days, translation programs
    may not produce great literature, but they\u2019re adequate for many purposes.
    And the same is true in many fields. You can argue that what we often call artificial
    intelligence isn\u2019t really intelligence. Indeed, it may be a long time before
    machines can be truly creative or offer deep insight. But then, how much of what
    human beings do is truly creative or deeply insightful? (Indeed, how much of what
    gets published in academic journals \u2014 a field of endeavor I know pretty well
    \u2014 meets those criteria?) So quite a few knowledge jobs may be eminently replaceable.
    What will this mean for the economy? It is difficult to predict exactly how A.I.
    will impact the demand for knowledge workers, as it will likely vary, depending
    on the industry and specific job tasks. However, it is possible that in some cases,
    A.I. and automation may be able to perform certain knowledge-based tasks more
    efficiently than humans, potentially reducing the need for some knowledge workers.
    This could include tasks such as data analysis, research and report writing. However,
    it is also worth noting that A.I. and automation may also create new job opportunities
    for knowledge workers, particularly in fields related to A.I. development and
    implementation. OK, I didn\u2019t write the paragraph you just read; ChatGPT did,
    in response to the question \u201CHow will A.I. affect the demand for knowledge
    workers?\u201D The giveaway, to me at least, is that I still refuse to use \u201Cimpact\u201D
    as a verb. And it didn\u2019t explicitly lay out exactly why we should, overall,
    expect no impact on aggregate employment. But it was arguably better than what
    many humans, including some people who imagine themselves smart, would have written.
    In the long run, productivity gains in knowledge industries, like past gains in
    traditional industries, will make society richer and improve our lives in general
    (unless\_Skynet\_kills us all). But in the long run, we are all dead, and even
    before that, some of us may find ourselves either unemployed or earning far less
    than we expected, given our expensive educations."
  tags: []
  title: Does ChatGPT Mean Robots Are Coming For the Skilled Jobs?
