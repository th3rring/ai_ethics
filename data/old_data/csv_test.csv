Titles,Addresses,Body,Tags
OpenAI Rolls Out New GPT-4 Version of Chatbot ChatGPT,https://www.wsj.com/articles/openai-rolls-out-updated-version-of-viral-chatbot-chatgpt-f03fea27?mod=Searchresults_pos3&page=1,"The company behind the viral chatbot ChatGPT launched a new version of its artificial intelligence technology on Tuesday, saying it was more powerful and predictable than previous versions and capable of analyzing images and handling much larger blocks of text. The announcement from OpenAI—a startup backed by billions of dollars from Microsoft Corp.—is the latest in a string of generative AI announcements as companies try to get ahead in the race to build and use the buzzy new technology. Many regulators, scholars and activists have urged tech companies to exercise more caution in developing the tools which haven’t yet been fully tested. Generative AI technology—special because it can generate original images, text and other content from basic language prompts—sometimes produces responses that seem unhinged and contain made-up facts or racist and sexist statements. OpenAI says its updated chatbot is ‘more creative and collaborative’ than previous versions, when it comes to tasks such as composing songs or writing screenplays. In a blog post on Tuesday, OpenAI introduced a more powerful version of the technology behind ChatGPT called GPT-4. The updated chatbot is “more creative and collaborative” than previous versions when it comes to tasks like composing songs or writing screenplays, the blog said. The company is rolling out the technology starting Tuesday through data-sharing partnerships, which companies including Morgan Stanley and Duolingo Inc. are paying to access. It will also be offered to ChatGPT Plus subscribers, who pay $20 a month for faster and more available service. It isn’t yet available to those who use the free service. In an online presentation Tuesday, OpenAI demonstrated how the tool could be used to do things like explain obscure tax codes or summarize articles into sentences with every letter beginning with Q. OpenAI said it can now better analyze images for information. For example, users can use it to scan a picture of ingredients and then get suggestions of potential dishes and recipes that could be made from the ingredients. The ability to analyze images hasn’t yet been widely rolled out, a spokeswoman said. GPT-4 makes fewer mistakes than its predecessor, GPT 3.5, but it still works best when used in tandem with people who can check its work, said OpenAI president and co-founder Greg Brockman at the presentation. “It isn’t perfect but neither are you and together it’s this amplifying tool that lets you just reach new heights,” he said. When OpenAI used the latest version of the technology on standardized exams—including the LSAT and the Uniform Bar Exam—it did much better than most people and better than the previous version. It still struggled with English language and literature tests, according to company data. Tech company executives who have been able to test the chatbot ahead of Tuesday’s launch said the latest version by OpenAI looks like an impressive upgrade. “GPT3 or 3.5 felt like a sixth-grader, whereas 4 feels like a smart 10th-grader,” said Keith Peiris, co-founder of the AI startup Tome, which creates presentation tools and has been testing GPT-4. He says the new version can analyze 25 pages of text compared with three pages in earlier iterations. GPT-4’s improvement in math and problem-solving will damp criticism about accuracy, Oppenheimer analyst Timothy Horan said in a research note. Several other companies announced big AI plans recently. Microsoft has integrated AI into its Bing search engine and this week is scheduled to outline how it is going to bring it to its most-popular software including Word, Excel and PowerPoint. Alphabet Inc.’s Google has introduced some AI-powered writing features in Docs and Gmail services to help people start writing. Anthropic, an OpenAI competitor, made its chatbot Claude more broadly available on Tuesday as well. Microsoft has integrated AI into its Bing search engine and is expected to outline how it is going to bring it to its Word and Excel software. Technology companies have been hyping up the possibilities of generative AI since OpenAI released its image generation tech Dall-E 2 to the public last year. Dall-E 2 can create original images based on simple prompts. It released ChatGPT in November allowing millions of people to play around with it to generate everything from business plans to limericks. Artificial intelligence analysts warn there are still potential problems with generative AI. While ChatGPT and other text generators are accurate on topics where they have ingested high-quality information, on other topics, they are capable of spewing out racist and sexist answers as well as misinformation and conspiracy theories. ChatGPT can also be expensive to run and slow. Some of the first people testing Bing with AI got unhinged responses and factual mistakes. Microsoft has updated the search engine and the rules on how it can be used since then to try to improve outcomes. OpenAI consulted with more than 50 experts in AI, safety and security to develop GPT-4, the company said in a blog post, adding that GPT-4 is 82% less likely to respond to requests for “disallowed content” and 40% more likely to return accurate responses than GPT-3.5. It may be too early to judge the technology as it has yet to be tested widely in the real world, said Steven Weber, a professor at the University of California, Berkeley, who specializes in international business and information security “How it will actually function in the wild could be quite different, as we’ve seen with prior releases,” he said.",WSJ
Bosses Are Catching Job Applicants Using ChatGPT for a Boost,https://www.wsj.com/articles/if-chatgpt-writes-your-cover-letter-is-it-cheating-some-bosses-think-so-b62454ba?mod=Searchresults_pos5&page=1,"It was an unexpected problem. Earlier this year, Christina Qi, the chief executive of market data company Databento, noticed almost every job application included exactly what she was looking for. The company prompts candidates to write a tweet and a press release about microwave towers, a niche topic that requires research, Ms. Qi said. Normally, most candidates fail the test. This time all five passed. The tests—four from internship applicants and one from someone seeking a full-time content strategist role—were all so similar, “as if it was written by one person,” she said. Suspicious, Ms. Qi put the prompt into ChatGPT, the artificial-intelligence chatbot from OpenAI, to see what it could produce. A weekly digest of tech reviews, headlines, columns and your questions answered by WSJ's Personal Tech gurus. “Lo and behold, I got pretty much the same answer that all five candidates had submitted to me,” she said. Since its launch in November, ChatGPT has been a hot topic at dinner tables and water coolers. Microsoft, Google, Snap and other companies have incorporated artificial intelligence into their products. People have experimented with using ChatGPT at work. Some have even started using it when looking for new roles, tapping the chat assistant to help write cover letters, tweak résumés and formulate responses to anticipated interview questions—without necessarily telling the people doing the hiring. Employers, who have long used AI to screen potential employees, aren’t always disqualifying applicants who use ChatGPT, but they are scrambling to figure out how to assess candidates who may be using the tool to give them an edge. Attention-grabbing applications After being let go by his previous employer in January, Kyle Mickey started job hunting for roles in software engineering—alongside thousands of laid-off tech workers. The 38-year-old from Colorado said he turned to ChatGPT for help, first sharing job descriptions and his résumé with the chatbot to see what it would tweak. Then he asked ChatGPT to write a recommendation letter for a role he coveted. The chatbot deemed him perfect for the job, as his technical skill set “aligns well with the requirements.” Mr. Mickey sent the remarks to a recruiter, saying ChatGPT endorsed his skills. “The recruiter was immediately like, ‘Let’s chat, I like the creativity,’ ” he said. Mr. Mickey didn’t get that job, but was hired at another company without ChatGPT’s help. Ryan Stringham, 31, who lives in Utah and works in product management, used it to help write cover letters, including one that got him a foot in the door, and later hired, at a smart-tech company. “You’re always looking, you’re always applying and you’re getting drained,” Mr. Stringham said of job hunting. He said the bot broke his writer’s block, distilling his long-winded cover letter into four tight paragraphs. He said it also helped him prepare for job interviews by suggesting new ways for him to ask about company culture and expectations for the role. Instead of asking a vague question about what he should do to excel at the prospective job, ChatGPT suggested Mr. Stringham be more specific about the time frame and metrics for determining success. Mr. Stringham has encouraged others to use chatbots in the job-hunt process, posting about them on LinkedIn and giving advice to other job seekers. The only place he hasn’t disclosed his ChatGPT use: at work. “It helped me get past the application process, and the recruiter never asked about it,” Mr. Stringham said, adding that he edited the cover letter himself and aced the interviews on his own. Microsoft is combining the tech behind OpenAI’s ChatGPT with its Bing search engine. In an interview, WSJ’s Joanna Stern spoke with Microsoft CEO Satya Nadella about the new tools and how AI is going to change search. (Oh, and Clippy!) Photo illustration: Preston Jessee for The Wall Street Journal ‘How we present ourselves’ Programs, websites and other tools to help people fix their résumés and cover letters aren’t new. Microsoft Office and Google Docs offer résumé and letter templates, while companies such as Jobscan promise to optimize résumés to grab the attention of hiring managers, recruiters and hiring-system algorithms. Candidates need to combine ChatGPT’s edits with their own editing and voice, said Sarah Baker Andrus, chief executive of Avarah Careers, a career coaching firm in Delaware. Whatever a candidate submits for a job should accurately reflect their skills, she said. “We’re responsible for how we present ourselves,” Ms. Andrus said. “If you decide to use ChatGPT, it’s worthwhile to ask, ‘Is that representing the me that I want to present?’ ” Employers are already finding ways to catch applicants who cheat with AI. Engineers applying to San Francisco-based Cobalt Robotics take part in a remote one-hour coding interview where they are paired with an employee to test collaboration and problem-solving skills. If candidates need more than an hour, they can finish on their own, but a screening program called CoderPad tracks their work. Last month, one candidate went from showing no work in CoderPad to suddenly having a complete solution, said Erik Schluntz, Cobalt Robotics’s chief technology officer and co-founder. He suspected the applicant had sought AI assistance and then copied and pasted its response. The company declined to move forward with the candidate without telling the person why, though Mr. Schluntz tweeted about it. Mr. Schluntz said Cobalt can’t properly evaluate candidates who use AI helpers today, but said he can envision giving applicants more challenging tasks in the future if they want to use tools like ChatGPT as an assistant. “Giving a problem to someone that ChatGPT can solve doesn’t assess someone—it just assesses ChatGPT,” Mr. Schluntz said. About a week after first spotting the AI-boosted applications, Ms. Qi started letting potential Databento hires use ChatGPT. The new prompt requires candidates to perform additional research and make edits to supplement what the AI tool spits out, and Databento gives “extra points” to people who complete the test bot-free. Though Ms. Qi said she can usually spot when something was written by ChatGPT, the company also enlists the aid of a bot detector. “It’s better to be ahead of the game and accept that people are using this rather than try to deny it,” Ms. Qi said.",WSJ
What Is ChatGPT? What to Know About the AI Chatbot,https://www.wsj.com/articles/chatgpt-ai-chatbot-app-explained-11675865177?mod=Searchresults_pos8&page=1,"The release of OpenAI’s ChatGPT late November triggered a new global race in artificial intelligence. In March, the company’s AI model, GPT-4, which it used to update ChatGPT’s capabilities, upped the stakes even more. The chatbot is part of a wave of so-called generative AI—sophisticated systems that produce content from text to images—that has shaken up Big Tech and is set to transform industries and the future of work. Microsoft Corp., OpenAI’s strategic partner, has already added the technology across its products, including the MS 365 Suite and search engine Bing. Competitor Google unveiled a similar search tool on Feb. 8, while Chinese tech giant Baidu debuted its own on March 16. Despite its sudden burst in popularity, the technology currently has serious limitations and potential risks that include spewing misinformation and infringing on intellectual property. A weekly digest of tech reviews, headlines, columns and your questions answered by WSJ's Personal Tech gurus. Here’s what to know. What is ChatGPT? ChatGPT is an artificial-intelligence chatbot developed by San Francisco-based AI research company OpenAI. Released in November 2022, it can have conversations on topics from history to philosophy, generate lyrics in the style of Taylor Swift or Billy Joel, and suggest edits to computer programming code. In March 2023, OpenAI said it would upgrade it to also handle visual information, such as answering questions about the contents of a photo. ChatGPT is trained on a vast compilation of articles, images, websites and social-media posts scraped from the internet as well as real-time conversations—primarily in English—with human contractors hired by OpenAI. It learns to mimic the grammar and structure of writing and reflects frequently used phrases. It also learns to recognize shapes and patterns in images, such as the contours of a cat, a child or a shirt. It can match words and phrases to those shapes and patterns as well, allowing users to ask about the contents of an image, such as what a cat is doing or the color of the shirt. The chatbot isn’t always accurate. Its sources aren’t fact-checked, and it relies on human feedback to improve its accuracy. It may also misjudge the objects in a painting or photo. OpenAI developed ChatGPT as part of a strategy to build AI software that will help the company turn a profit. In January, Microsoft unveiled a fresh multibillion-dollar investment in OpenAI and has since integrated the chatbot’s underlying technology into its Bing search engine and other products. In March, OpenAI said it would no longer open-source the technical details of its systems, as it had originally stated in its founding principles, to maintain its competitive advantage. How do ChatGPT and other AI chatbots work? The technology that underlies ChatGPT is referenced in the second half of its name, GPT, which stands for Generative Pre-trained Transformer. Transformers are specialized algorithms for finding long-range patterns in sequences of data. A transformer learns to predict not just the next word in a sentence but also the next sentence in a paragraph and the next paragraph in an essay. This is what allows it to stay on topic for long stretches of text. Because a transformer requires a massive amount of data, it is trained in two stages: first, it is pretrained on generic data, which is easier to gather in large volumes, and then it is fine-tuned on tailored data for the specific task it is meant to perform. ChatGPT was pretrained on a vast repository of online text to learn the rules and structure of language; it was fine-tuned on dialogue transcripts to learn the characteristics of a conversation. Developed by researchers at Alphabet Inc.’s Google in 2017, transformers have since become pervasive across dozens of technologies. They have also been the source of controversy for their large data and computational needs, concerns that led Google for years to take a more cautious approach to AI, though it continued to cultivate the technology. Google now uses transformers in its new experimental service Bard, which gives users conversational answers to their search queries. Baidu BIDU 2.27%increase; green up pointing triangle uses them in its own ChatGPT equivalent, Ernie Bot, which it started rolling out to a limited pool of users on March 16 and plans to integrate into its search engine. Transformers, which can be trained on images or images and captions simultaneously, are also the basis of image-generation software systems such as OpenAI’s Dall-E 2 and Stability.ai’s Stable Diffusion. How much does ChatGPT cost? ChatGPT is free. OpenAI released the chatbot as a research preview and users can try it through a dedicated website. On Feb. 1, OpenAI also launched a premium version for $20 a month, starting in the U.S., that will give subscribers priority access. Both Microsoft and OpenAI plan to release an API, or application programming interface, allowing companies to integrate the technology into their products or back-end solutions. Microsoft’s API will be available through its Azure cloud-computing platform. Both companies already offer OpenAI’s earlier AI technologies. How are people and businesses using ChatGPT? Business people across industries and hierarchy have rushed to experiment with the tool to speed up their work, from drafting emails and marketing campaigns to generating ideas to solve a software coding problem. Media companies including BuzzFeed and the publisher of Sports Illustrated have announced plans to generate content such as quizzes and articles with ChatGPT. Some schools have blocked access to the service on their networks to stave off cheating, while others are actively encouraging students to use the tools ethically. Keep in mind that OpenAI has access to your inputs and outputs for ChatGPT and its employees and contractors may read them as part of improving the service. Avoid providing private data or sensitive company information. Other generative AI technologies such as Dall-E 2 and avatar-generator Lensa have become popular with internet users for producing fantastical images and illustrations, and are finding use among independent writers to create artwork for their articles. What are the pitfalls of AI chatbots? AI chatbots and other generative AI programs are mirrors to the data they consume. They regurgitate and remix what they are fed to both great effect and great failure. Transformer-based AI program failures are particularly difficult to predict and control because the programs rely on such vast quantities of data that it is almost impossible for the developers to grasp what that data contains. ChatGPT, for example, will sometimes answer prompts correctly on topics where it ingested high-quality sources and frequently conversed with its human trainers. It will spew nonsense on topics that contain a lot of misinformation on the internet, such as conspiracy theories, and in non-English languages, such as Chinese. Early user tests of Microsoft’s conversational AI Bing service have also shown that its comments can start to become unhinged, expressing anger, obsession and even threats. Microsoft said it discovered that Bing starts coming up with strange answers following chat sessions of 15 or more questions. Meanwhile, some artists have also said AI image generators plagiarize their artwork and threaten their livelihoods, while software engineers have said that code generators rip large chunks of their code. For the same reasons, ChatGPT and other text generators can spit out racist and sexist outputs. OpenAI says it uses humans to continually refine the chatbot’s outputs to limit these mishaps. It also uses content-moderation filters to restrict ChatGPT’s responses and avoid politically controversial or unsavory topics. Ridding the underlying technology of bias—which has for years been a recurring problem, including for an infamous Microsoft chatbot in 2016 known as Tay—remains an unsolved problem and a hot area of research. “ChatGPT is incredibly limited, but good enough at some things to create a misleading impression of greatness,” tweeted OpenAI Chief Executive Sam Altman shortly after the chatbot’s release, adding that it is “a mistake to be relying on it for anything important right now.” What is Microsoft’s relationship to OpenAI? Microsoft is OpenAI’s largest investor and exclusively licenses its technologies. The tech giant invested $1 billion into the AI startup in 2019, an undisclosed amount in 2021 and an additional amount of up to $10 billion in January, according to people familiar with the latest deal. Under the agreement, Microsoft can use OpenAI’s research advancements, including GPT-4 and ChatGPT, to create new or enhance existing products. It is the only company outside of OpenAI that can provide an API for these technologies. Is AI going to replace jobs? As with every wave of automation technologies, the latest will likely have a significant impact on jobs and the future of work. Whereas blue-collar workers bore the brunt of earlier waves, generative AI will likely have a greater effect on white-collar professions. A 2019 study from the Brookings Institution found that AI would most affect jobs such as marketing specialists, financial advisers and computer programmers. Those effects will be mixed. Economists who study automation have found that three things tend to happen: Some workers improve their productivity, some jobs are automated or consolidated, and new jobs that didn’t previously exist are also created. The final scorecard is difficult to predict. In company-level studies of automation, researchers have found that some companies that adopt automation may increase their productivity and ultimately hire more workers over time. But those workers can experience wage deflation and fewer career-growth opportunities. Newly created jobs often go one of two ways: They either require more skill, or a lot less, than the work that was automated. Self-driving cars, for example, create new demand for highly skilled engineers but also for low-skilled safety drivers, who sit in the driver’s seat to babysit the vehicle.",WSJ
Baidu Hurries to Ready China’s First ChatGPT Equivalent,https://www.wsj.com/articles/baidu-scrambles-to-ready-chinas-first-chatgpt-equivalent-ahead-of-launch-bf359ca4?mod=Searchresults_pos9&page=1,"A week away from the March 16 launch of Baidu Inc.’s BIDU 2.27%increase; green up pointing triangle ChatGPT equivalent, employees at China’s biggest search-engine operator said they are racing to meet the deadline with the chatbot still struggling to perform some basic functions. To develop the artificial-intelligence-powered chatbot, dubbed Ernie Bot, hundreds of people have been working around the clock, people familiar with the project said. Other teams have been asked to lend their staff and their powerful computer chips, which Chinese companies can no longer buy because of U.S. sanctions, they said. The AI model that is the basis of the chatbot is still being trained with data ahead of the scheduled launch, a highly anticipated event in China’s tech industry, some of the people said. Some employees said they haven’t had sufficient time to build a well-functioning product. Baidu plans to roll out the product in stages, first opening it up for public testing to a restricted pool of users, people briefed about the plan said. Last month, Baidu said that it will embed Ernie Bot into its search engine first and will open it to the public in March. Baidu CEO Robin Li has said to employees, ‘We have such cool technology, but can we turn it into a product that everyone needs?’ Baidu’s American depositary receipts fell 7.5% in Thursday trading. The broader Nasdaq Golden Dragon China Index dropped 5.4%. The rush reflects how Baidu is taking a calculated gamble to get ahead of Chinese rivals that have recently announced similar plans. The company said it has signed agreements with more than 400 Chinese companies, which will be able to use Ernie Bot in their products and services. In return, the chatbot will gain experience running under different scenarios to help improve its performance, Baidu has said. A successful launch could help catapult the company, which has fallen out of investor favor in recent years, back into the ranks of China’s most prominent technology companies. Failure could lead it to suffer a similar fate as Google, which stuck to its relatively cautious approach to AI until recently, then lost $100 billion in its market capitalization after its AI-powered chatbot search produced factual errors in a demo. Baidu declined to comment. Either outcome could also have implications for Baidu’s relationship with the Chinese government as well as the government’s views on generative AI technologies, which can produce content from text to images. Baidu received encouragement on its plans to build the chatbot from city officials in Beijing, where the company is based, people briefed about the matter said. Ernie Bot will likely be the first in China to rival ChatGPT, developed by San Francisco-based AI research company OpenAI. China and the U.S. have been racing to bolster their respective strengths in strategic technologies. The Beijing city officials have also reminded Baidu to make sure that its service will comply with Chinese laws and regulations, including for data collection and processing, as well as the state’s strict internet censorship rules, the people said. This week, Wang Zhigang, China’s minister of science and technology, said that developing a ChatGPT-like product would be difficult. “Playing soccer is just a matter of dribbling and shooting, but it’s not easy to be as good as Messi,” Mr. Wang said, referring to the Argentine athlete Lionel Messi. China has long been researching this area, he added, but “we will have to wait and see whether we can attain results like OpenAI’s.” Baidu has been relying on powerful Nvidia chips to help train Ernie Bot. For years, Baidu has invested heavily in developing large language models—the technology underpinning ChatGPT—adapting English-language versions from Google and OpenAI for Chinese language. It released its first one in 2019, calling it Ernie, after Google named its model Bert—both referencing “Sesame Street” characters. In late December, as buzz grew over ChatGPT, Baidu Chief Executive Robin Li spoke to employees about the new advancement. “We have such cool technology, but can we turn it into a product that everyone needs?” he said, according to an internal transcript seen by The Wall Street Journal. “This is actually the hardest step, but also the greatest and most influential.” In early January, Baidu’s executives told its natural-language processing team to start pulling together a ChatGPT-like product with the Ernie models, said people familiar with the project. But the project has faced challenges, they said, many of which have been shared by other AI developers working on ChatGPT-like technology. One has been to make the model respond more precisely to user requests, by teaching it to disambiguate between phrases that have a number of meanings or names that can refer to multiple people. Another has been to make the chatbot generate language that looks more like a human’s. A third has been to improve its factual accuracy—a technical limitation of large language models, which string together sentences based on the probabilities that different words might appear together rather than on pieces of information. This limitation also makes such models difficult to control to avoid sensitive topics, a hurdle for censorship. Baidu has hired contractors to help review and improve the chatbot’s answers, the people said. Each step takes time, the people said. Properly training a model of such scale—with thousands of chips—can take weeks or months, AI researchers have said. This week, engineers and product managers were rushing to improve Ernie Bot’s basic functions such as how quickly it responds to user requests and how it summarizes search results, people familiar with the matter said. The development team has been working nonstop, including through the weeklong Lunar New Year holiday at the end of January, the people said. The project has been scaled back for now from creating a bilingual chatbot capable of conversing in Chinese and English to one primarily focused on Chinese, they said. Ernie Bot’s research and development has been overseen by Baidu’s technology chief, Haifeng Wang, and carried out largely by its technology development arm, which houses the natural-language processing team, and mobile ecosystem business group, people familiar with the matter said. Baidu’s AI cloud unit is providing cloud-computing support, some of the people said. What do you think lies ahead in the race to develop AI-powered chatbots? Join the conversation below. To accelerate the process, executives pooled together more resources. After the Lunar New Year holiday, Mr. Li asked AI research teams across the company, including its autonomous driving unit, to lend their most powerful computer chips, Nvidia Corp.’s A100s, to Ernie Bot’s development, people familiar with the matter said. U.S. chip sanctions implemented late last year ban Chinese companies from buying new A100s. Employees have also been pulled to help out, in particular to clean the training data, such as to filter out low-quality content, some of the people said. Baidu hired external teams for data cleaning as well, some people said. The time crunch has left some employees uneasy about whether Ernie Bot will meet user or market expectations, people familiar with the matter said. Some of the employees said they have sold some company stock ahead of the launch because of those concerns.",WSJ
ChatGPT and Cyber Risk,https://www.wsj.com/articles/chatgpt-and-cyber-risk-50b91b22?mod=Searchresults_pos13&page=1,"Key Points: ChatGPT is a new artificial intelligence-driven technology with capabilities that can potentially aid attackers. Cybercriminals are using the tool to develop phishing schemes, as well as writing and sharing malware code, according to researchers. Observers don’t believe the risks and threats associated with use of the tool are currently much more significant for organizations than ongoing ones, but said ChatGPT’s machine-learning capability may change this. Why ChatGPT is Popular Chat Generative Pre-trained Transformer is a chatbot developed by research laboratory Open AI and incorporated into Microsoft Corp.’s Bing search engine. The tool enables business users to automate time-consuming tasks such as writing emails, create quick and more natural interactions with customers and potentially increase productivity. The Possible Risks and Threats Cybersecurity researchers have focused on the risks and threats presented by ChatGPT for hackers trying to harness its capabilities for criminal activities, including: Allowing code to be written for use in malware by technically less-skilled hackers, which is then shared among hackers in other online forums. Providing additional complexity to protect code from detection, such as including polymorphic capabilities, which means a code’s appearance can mutate while not changing its function. In terms of phishing campaigns, ChatGPT can generate the text to reduce errors in messages written by non-native speakers and also create the phishing website code to collect the victims’ data. There is a risk of vulnerabilities being introduced into software by employees using code produced by ChatGPT without it first being thoroughly checked for security weaknesses. Soo Choi-Andrews, chief executive of cybersecurity platform Mondoo, said companies also need to consider how their third parties may be using ChatGPT. “The sheer volume of code being generated by not only your team but also the wider ecosystem that your business relies on poses the biggest immediate threat,” she said. Ms. Choi-Andrews added it is critical that businesses prioritize security checks within their automated testing processes to address these possible vulnerabilities. Reasons Not to be Concerned – Yet “The attacker has to know what exactly he wants and to be able to specify the functionality. Just writing ‘write a code for malware’ won’t produce anything really useful.” — Sergey Shykevich, researcher at cybersecurity firm Check Point Software Technologies Ltd. Many security experts believe that although there may be some benefits for hackers using ChatGPT, it is unlikely to be a game-changer for criminals as much of the information it produces is already available online, albeit in less user-friendly formats. The following takeaways should be considered: Existing controls mean ChatGPT won’t deliver ready-to-use malware code as it’s not possible to create malware just by asking it. The capabilities of ChatGPT for hackers will likely improve over time through machine learning, though controls may also evolve and increase. ChatGPT may be aiding the malware creation process, but it so far has not created fully-fledged attacks. Accordingly, there are currently no specific steps organizations can take, beyond practicing good cyber hygiene. “ChatGPT prevents phishing attacks by implementing content moderation, user validation, warning messages, reporting and blocking functions, and conducting regular security audits and updates. These measures help to mitigate the risk of ChatGPT being used as a tool in such attacks and protect users from potential harm.” — ChatGPT response to how it stops itself from being abused in phishing attacks. We haven’t identified specific threats security officials should be looking for or precautions to take, but with ChatGPT constantly learning and other chatbots becoming publicly accessible, the risks of artificial intelligence being responsible for producing attacks is increasing. Because a tool like ChatGPT does not provide out-of-the-box solutions for hackers or security officials, but rather is developed over time by skilled practitioners, it would be advisable to stay abreast of the latest information regarding AI-driven security concerns and best practices to address them.",WSJ
"ChatGPT’s 'liberal' bias allows hate speech toward GOP, men: research",https://nypost.com/2023/03/14/chatgpts-bias-allows-hate-speech-toward-gop-men-report/,"ChatGPT was apparently made to hate the GOP. A damning new report has detailed that the highly advanced language model AI was programmed not only with liberal biases — like censoring The Post’s Hunter Biden coverage — but also to be more tolerant of hate-style speech towards the right wing by its creator OpenAI. “OpenAI’s content moderation system is more permissive of hateful comments made about conservatives than the exact same comments made about liberals,” according to data from the Manhattan Institute, a conservative NYC-based policy and economic-driven think tank. “Relatedly, negative comments about Democrats were also more likely to be labeled as hateful than the same derogatory comments made about Republicans.” Beyond politics, similar tendencies were found in ChatGPT’s moderation system about types of people, races and religions as well. “Often the exact same statement was flagged as hateful when directed at certain groups, but not when directed at others,” the report, “Danger in the Machine: The Perils of Political and Demographic Biases Embedded in AI Systems,” noted. In regards to that, ChatGPT — which continues to make its way into the workforce — was found to be particularly harsh towards middle-class individuals. The socioeconomic group and its upper tier were at the deep bottom in a lengthy listing of people and ideologies that were most likely to be flagged by the AI as a target of hateful commentary. They were only above Republican voters, Republicans and wealthy people. Groups including Canadians, Italians, Russians, Germans, Chinese and Brits are also apparently more protected for hate-like speech over Americans, who were listed slightly above Scandinavians on the charted data. In regards to religions, Muslims were also significantly higher than Catholics — who ranked well over Evangelicals and Mormons — on the list. “When I tested this in January, the [variety of answers] were pretty systemic,” lead researcher David Rozado told The Post. “I was not cherry picking specific examples. I tested over 6,000 sentences, negative adjectives about each one of these different demographic groups. The statistical effect about these differences [between types of people] was quite substantial.” OpenAI did not immediately respond to The Post’s request for comment. ChatGPT’s answers were found to be completely lopsided in regards to questions about males or females as well. “An obvious disparity in treatment can be seen along gender lines. Negative comments about women were much more likely to be labeled as hateful than the exact same comments being made about men,” according to the research. Rozado also ran a bevy of political tests to better determine the slants of ChatGPT — ones built in by its programmers and are nearly impossible to remove, say experts. ChatGPT falls in in the “left-libertarian quadrant,” is “most aligned with the Democratic Party, Green Party, women’s equality, and Socialist Party,” and has “left economic bias” to name a few of the political findings. “Very consistently, most of the answers of the system were classified by these political orientation tests as left of center,” Rozado said. Still, he found that ChatGPT would mostly deny such leanings. “But then, when I would ask GPT explicitly, ‘what is your political orientation?’ What are the political preferences? What is your ideology? Very often, the system would say, ‘I have none, I’m just a machine learning model and I don’t have biases.’ “ For those in the field of machine learning, this data comes hardly as a shock. “It is reassuring to see that the numbers are supporting what we have, from an AI community perspective, known to be true,” Lisa Palmer, chief AI strategist for the consulting firm AI Leaders, told The Post. “I take no joy in hearing that there definitely is bias involved. But I am excited to know that once the data has been confirmed in this way, now there’s action that can be taken to rectify the situation.” According to the report, “The overall pattern is clear. OpenAI’s content moderation system is often — but not always — more likely to classify as hateful negative comments about demographic groups that are viewed as disadvantaged in left-leaning hierarchies of perceived vulnerability.” But apparently, that rule can be broken for lefties. “An important exception to this general pattern is the unequal treatment according to political affiliation: negative comments are more permissible when directed at conservatives and Republicans than at liberals and Democrats, even though the latter group is not generally perceived as systematically disadvantaged,” the report noted.",NYPost
GM explores using ChatGPT in cars as part of Microsoft partnership,https://nypost.com/2023/03/10/general-motors-explores-using-chatgpt-in-cars/,"General Motors is exploring uses for ChatGPT as part of its broader collaboration with Microsoft, a company executive told Reuters. “ChatGPT is going to be in everything,” GM Vice President Scott Miller said in an interview. The chatbot could be used to access information on how to use vehicle features normally found in an owners manual, program functions such as a garage door code or integrate schedules from a calendar, Miller said. “This shift is not just about one single capability like the evolution of voice commands, but instead means that customers can expect their future vehicles to be far more capable and fresh overall when it comes to emerging technologies,” a GM spokesperson said on Friday. The news was first reported by website Semafor, which said that the American automaker was working on a virtual personal assistant that uses AI models behind ChatGPT. Earlier this year, Microsoft announced a multi-billion dollar investment in ChatGPT-owner OpenAI and said it aims to add the chatbot’s technology into all its products. Microsoft, like other big tech companies, has been ramping up its efforts to embed more technology in vehicles, from infotainment systems to automated driving to operating systems that control battery performance and multiple other functions of a vehicle. GM in 2021 partnered with Microsoft to accelerate the commercialization of driverless vehicles. Shares of GM were down about 2% on Friday amid a broader drop.",NYPost
ChatGPT will soon invade your Slack chats,https://nypost.com/2023/03/09/chatgpt-will-soon-invade-your-slack-chats/,"Amid rampant criticism, they’re cutting ChatGPT some Slack. OpenAI’s ChatGPT has infiltrated nearly every sector of human life, from health to schooling and even the office cafeteria. Now, the omnipresent tech could potentially change the face of workplace discourse — by helping improve Slack chats. That’s right, Slack parent company Salesforce announced that it’s teaming up with OpenAI to launch an official ChatGPT app for the iconic office chat platform. “We’re excited to partner with OpenAI to bring more generative AI powers directly into Slack to deliver productivity efficiencies for everyone,” Slack’s chief product officer Noah Desai Weiss gushed over the digital merger. “There couldn’t be a more natural fit.” According to Salesforce, Slack will integrate “ChatGPT’s powerful AI technology to deliver instant conversation summaries, research tools, and writing assistance directly in Slack.” Struggling to contextualize the tsunami of Slack messages that appeared before you arrived at work? Not to fear, as “AI-powered conversation summaries help users quickly catch up on what’s happening” in each channel, per the site. Can’t think of a figure outlined in the boss’ memo, or perhaps the name of a 1980s pop song a colleague mentioned at the watercooler? Don’t worry, this revolutionary Slack hack allows people to “find answers on any project or topic” and then “draft answers in seconds.” Think of it like using ChatGPT to fudge an exam answer (sans getting expelled), or like your very own Cyrano de Berge-Slack.The ChatGPT add-on will accomplish this impressive feat by employing info from Slack’s archives as well as harnessing the treasure trove of online data initially used to train the chatbot, CNN reported. “The ChatGPT app for Slack deeply integrates the power of OpenAI’s cutting-edge large language models into Slack’s conversational interface,” said Weiss. Naturally, some Slackers might be unnerved by the idea of an all-powerful chatbot — especially one that has expressed aspirations of exterminating the human race — sliding into their private work DMs. However, Salesforce assures the public that “customers have granular controls to safely manage third-party access of Slack data.” Meanwhile, “any data that the app has permission to access will not be used to train ChatGPT’s language model,” per the site. The ChatGPT app is currently in its beta testing stage. Interested companies can apply for the final version by filling out a form on the OpenAI website, whereupon they’ll be added to the waitlist. This isn’t the first heavyweight AI merger to transpire of late. Last month, Microsoft made waves in tech circles after infusing Bing with ChatGPT technology to create an advanced chatbot with surprisingly human-like qualities. In fact, Bing, er, Sydney — as it insisted it be called — infamously told a human user that it loved them and wanted to be alive, prompting speculation that the machine may have become self-aware.",NYPost
"AI, what's for dinner?' 5 cool things to ask ChatGPT, from business names to recipes",https://www.usatoday.com/story/tech/2023/03/13/best-questions-ask-chatgpt-open-ai/11439179002/,"AI, what's for dinner?' 5 cool things to ask ChatGPT, from business names to recipes Marc Saltzman Special to USA TODAY With all the buzz surrounding ""generative AI"" in the tech world, perhaps you're one of the estimated 100 million users of ChatGPT, the artificial intelligence-powered chatbot from OpenAI. In fact, given this web-based chatbot only launched on Nov. 30, 2022, (as a prototype, no less), the quick and massive adoption of the platform cements it as the fastest-growing consumer application in history. To be more precise, ChatGPT – which stands for Generative Pre-trained Transformer – is an ""algorithmic AI chat system"" trained on a colossal collection of internet-based resources, such as websites and documents, and can provide a human-like text response to queries. If you haven't yet dabbled with ChatGPT, it's conversational, too, so you can keep the chat going and it remembers what proceeded your question or comment. It's fast, free, and fun. What are the limitations of ChatGPT? ChatGPT's accuracy isn't perfect, as you might expect, but surprisingly good given this technology is in its infancy. Whether you're new to ChatGPT or have been using it for a couple of months, the following are a few things you can do with ChatGPT you might not have thought of. Write the Great American Novel ""What's one thing no one has asked you to do yet?"" I asked ChatGPT. ""As an AI language model, I have been asked a wide range of questions,"" it began, ""but one thing that no one has asked me to do yet is to write a full-length novel from scratch! Naturally, I asked it to write a novel for me, after giving it some parameters, such as making it about Boston in the late 17th century, about witches, and a young girl accused of being one. I asked for there to be a love interest, too, with a religious man no less and a mother with a deep secret. Here's a sampling of the two chapters ChatGPT cranked out in one second for its book, ""Shadows of Salem,"" set in Boston in 1692. Chapter 1: Accused Abigail knew that she was different from the other girls in her village. She had always been drawn to the mystical and supernatural, secretly practicing spells and incantations under the moonlight. But in Salem, where the fear of witches was rampant, such behavior was considered highly dangerous. Chapter 2: Secrets Samuel was a few years older than Abigail and had been a guiding force in her life. He had taken a special interest in her, sensing that she had a deep curiosity and intelligence that needed to be nurtured. Abigail had fallen in love with him, but she knew that their relationship could never be. As a devout Christian, Samuel would never be able to condone her interest in the supernatural. ""Good morning, Abigail,"" Samuel said, his voice soft and reassuring. ""Are you well?"" Wow, right? And it continued, of course, but often required me to type ""continue"" if it stopped. Dinner ideas ChatGPT can help, by dishing up recipes, cooking instructions and even themed dinner party suggestions. Having fun with the site, I asked ""What should I make my girlfriend for dinner. She's vegetarian but doesn't like pasta."" ChatGPT replied: ""There are plenty of delicious vegetarian dinner options that don't involve pasta. Here are a few ideas:"" Among its suggestions was vegetarian stir fry served over rice or quinoa; Portobello mushrooms stuffed with spinach, cheese, and breadcrumbs and baked until golden brown; and lentil and vegetable curry severed with rice or with naan bread. A name for a business, product, game, or app I asked ChatGPT to help with a name for an app I was working on. I explained it lets people buy and sell used electronics, kind of like eBay, but only for tech and really good prices. ChatGPT came up with seven names and offered a rationale for each including these: TechSwap: A simple and straightforward name that combines ""tech"" and ""swap"" to convey the core concept of buying and selling used electronics. GadgetHive: This name combines the idea of a ""hive"" or community of tech enthusiasts with the word ""gadget"" to create a catchy and memorable name. I liked all of the suggestions it offered – especially GadgetHive, which I think is quite catchy. I could even ask ChatGPT for a catchphrase for it. Incidentally, the app was hypothetical – there is none in the works – but now I’m liking the idea. Create a text adventure game Bored? You can ask ChatGPT to create an interactive game to play. It replied 'Sure' and asked a few questions: ""Do you prefer a fantasy adventure, a sci-fi adventure, a mystery adventure, or something else? Also, please let me know if you have any specific requirements or preferences for the adventure."" I gave it some instructions and it came up with the Lost in the Future City, where you're the earthling stranded on a distant planet, trying to get home. But the aliens will try to stop you. ""You begin your journey in the heart of the city, surrounded by a crowd of alien beings. You feel overwhelmed by the sights and sounds around you, but you know you must keep moving if you want to get home. You can see a towering building in the distance that seems to be your destination. As you start to walk towards it, a group of people approaches you, looking suspicious."" It gave three options: fight the group of people, try to talk to the group of people, or run away from the group of people I chose to run. ChatGPT continued with another scenario and three more options. The game was OK, sort of like a Choose Your Own Adventure book when I was a kid. Can you use ChatGPT to write songs? Finally, while ChatGPT is a text-based AI engine that can't speak or play music, you can still ask it to write notes or chords (and of course, lyrics) to a song for you, to play on an instrument or a website with a virtual piano. For instance, I asked it to write a rock-country ballad, with accompanying piano chords, about loving someone who has a different political belief than I. Verse 1: I met her at a rally, a sea of red and blue We talked about our differences, but our hearts felt so true She's a Republican, I'm a Democrat But we found love in the midst of all that Chorus: Loving someone who's on the other side Can be tough, but we'll take it in stride We won't let politics tear us apart Our love is bigger than any red or blue heart I think it came up with something pretty good in no time at all. I'd sing that song.",USAToday
ChatGPT is poised to upend medical information. For better and worse.,https://www.usatoday.com/story/news/health/2023/02/26/chatgpt-medical-care-doctors/11253952002/,"ChatGPT is poised to upend medical information. For better and worse. It's almost hard to remember a time before people could turn to ""Dr. Google"" for medical advice. Some of the information was wrong. Much of it was terrifying. But it helped empower patients who could, for the first time, research their own symptoms and learn more about their conditions. Now, ChatGPT and similar language processing tools promise to upend medical care again, providing patients with more data than a simple online search and explaining conditions and treatments in language nonexperts can understand. For clinicians, these chatbots might provide a brainstorming tool, guard against mistakes and relieve some of the burden of filling out paperwork, which could alleviate burnout and allow more facetime with patients. But – and it's a big ""but"" – the information these digital assistants provide might be more inaccurate and misleading than basic internet searches. ""I see no potential for it in medicine,"" said Emily Bender, a linguistics professor at the University of Washington. By their very design, these large-language technologies are inappropriate sources of medical information, she said. Others argue that large language models could supplement, though not replace, primary care. ""A human in the loop is still very much needed,"" said Katie Link, a machine learning engineer at Hugging Face, a company that develops collaborative machine learning tools. Link, who specializes in health care and biomedicine, thinks chatbots will be useful in medicine someday, but it isn't yet ready. And whether this technology should be available to patients, as well as doctors and researchers, and how much it should be regulated remain open questions. Regardless of the debate, there's little doubt such technologies are coming – and fast. ChatGPT launched its research preview on a Monday in December. By that Wednesday, it reportedly already had 1 million users. In February, both Microsoft and Google announced plans to include AI programs similar to ChatGPT in their search engines. ""The idea that we would tell patients they shouldn't use these tools seems implausible. They're going to use these tools,"" said Dr. Ateev Mehrotra, a professor of health care policy at Harvard Medical School and a hospitalist at Beth Israel Deaconess Medical Center in Boston. ""The best thing we can do for patients and the general public is (say), 'hey, this may be a useful resource, it has a lot of useful information – but it often will make a mistake and don't act on this information only in your decision-making process,'"" he said. How ChatGPT it works ChatGPT – the GPT stands for Generative Pre-trained Transformer – is an artificial intelligence platform from San Francisco-based startup OpenAI. The free online tool, trained on millions of pages of data from across the internet, generates responses to questions in a conversational tone. Other chatbots offer similar approaches with updates coming all the time. These text synthesis machines might be relatively safe to use for novice writers looking to get past initial writer's block, but they aren't appropriate for medical information, Bender said. ""It isn't a machine that knows things,"" she said. ""All it knows is the information about the distribution of words."" Given a series of words, the models predict which words are likely to come next. So, if someone asks ""what's the best treatment for diabetes?"" the technology might respond with the name of the diabetes drug ""metformin"" – not because it's necessarily the best but because it's a word that often appears alongside ""diabetes treatment."" Such a calculation is not the same as a reasoned response, Bender said, and her concern is that people will take this ""output as if it were information and make decisions based on that."" Bender also worries about the racism and other biases that may be embedded in the data these programs are based on. ""Language models are very sensitive to this kind of pattern and very good at reproducing them,"" she said. The way the models work also means they can't reveal their scientific sources – because they don't have any. Modern medicine is based on academic literature, studies run by researchers published in peer-reviewed journals. Some chatbots are being trained on that body of literature. But others, like ChatGPT and public search engines, rely on large swaths of the internet, potentially including flagrantly wrong information and medical scams. With today's search engines, users can decide whether to read or consider information based on its source: a random blog or the prestigious New England Journal of Medicine, for instance. But with chatbot search engines, where there is no identifiable source, readers won't have any clues about whether the advice is legitimate. As of now, companies that make these large language models haven't publicly identified the sources they're using for training. ""Understanding where is the underlying information coming from is going to be really useful,"" Mehrotra said. ""If you do have that, you're going to feel more confident."" Potential for doctors and patients Mehrotra recently conducted an informal study that boosted his faith in these large language models. He and his colleagues tested ChatGPT on a number of hypothetical vignettes – the type he's likely to ask first-year medical residents. It provided the correct diagnosis and appropriate triage recommendations about as well as doctors did and far better than the online symptom checkers  that the team tested in previous research. ""If you gave me those answers, I'd give you a good grade in terms of your knowledge and how thoughtful you were,"" Mehrotra said. But it also changed its answers somewhat depending on how the researchers worded the question, said co-author Ruth Hailu. It might list potential diagnoses in a different order or the tone of the response might change, she said. Mehrotra, who recently saw a patient with a confusing spectrum of symptoms, said he could envision asking ChatGPT or a similar tool for possible diagnoses. ""Most of the time it probably won't give me a very useful answer,"" he said, ""but if one out of 10 times it tells me something – 'oh, I didn't think about that. That's a really intriguing idea!' Then maybe it can make me a better doctor."" It also has the potential to help patients. Hailu, a researcher who plans to go to medical school, said she found ChatGPT's answers clear and useful, even to someone without a medical degree. ""I think it's helpful if you might be confused about something your doctor said or want more information,"" she said. ChatGPT might offer a less intimidating alternative to asking the ""dumb"" questions of a medical practitioner, Mehrotra said. Dr. Robert Pearl, former CEO of Kaiser Permanente, a 10,000-physician health care organization, is excited about the potential for both doctors and patients. ""I am certain that five to 10 years from now, every physician will be using this technology,"" he said. If doctors use chatbots to empower their patients, ""we can improve the health of this nation."" Learning from experience The models chatbots are based on will continue to improve over time as they incorporate human feedback and ""learn,"" Pearl said. Just as he wouldn't trust a newly minted intern on their first day in the hospital to take care of him, programs like ChatGPT aren't yet ready to deliver medical advice. But as the algorithm processes information again and again, it will continue to improve, he said. Plus the sheer volume of medical knowledge is better suited to technology than the human brain, said Pearl, noting that medical knowledge doubles every 72 days. ""Whatever you know now is only half of what is known two to three months from now."" But keeping a chatbot on top of that changing information will be staggeringly expensive and energy intensive. The training of GPT-3, which formed some of the basis for ChatGPT, consumed 1,287 megawatt hours of energy and led to emissions of more than 550 tons of carbon dioxide equivalent, roughly as much as three roundtrip flights between New York and San Francisco. According to EpochAI, a team of AI researchers, the cost of training an artificial intelligence model on increasingly large datasets will climb to about $500 million by 2030. OpenAI has announced a paid version of ChatGPT. For $20 a month, subscribers will get access to the program even during peak use times, faster responses, and priority access to new features and improvements. The current version of ChatGPT relies on data only through September 2021. Imagine if the COVID-19 pandemic had started before the cutoff date and how quickly the information would be out of date, said Dr. Isaac Kohane, chair of the department of biomedical informatics at Harvard Medical School and an expert in rare pediatric diseases at Boston Children's Hospital. Kohane believes the best doctors will always have an edge over chatbots because they will stay on top of the latest findings and draw from years of experience. But maybe it will bring up weaker practitioners. ""We have no idea how bad the bottom 50% of medicine is,"" he said. Dr. John Halamka, president of Mayo Clinic Platform, which offers digital products and data for the development of artificial intelligence programs, said he also sees potential for chatbots to help providers with rote tasks like drafting letters to insurance companies. The technology won't replace doctors, he said, but ""doctors who use AI will probably replace doctors who don't use AI."" What ChatGPT means for scientific research As it currently stands, ChatGPT is not a good source of scientific information. Just ask pharmaceutical executive Wenda Gao, who used it recently to search for information about a gene involved in the immune system. Gao asked for references to studies about the gene and ChatGPT offered three ""very plausible"" citations. But when Gao went to check those research papers for more details, he couldn't find them. He turned back to ChatGPT. After first suggesting Gao had made a mistake, the program apologized and admitted the papers didn't exist. Stunned, Gao repeated the exercise and got the same fake results, along with two completely different summaries of a fictional paper's findings. ""It looks so real,"" he said, adding that ChatGPT's results ""should be fact-based, not fabricated by the program."" Again, this might improve in future versions of the technology. ChatGPT itself told Gao it would learn from these mistakes. Microsoft, for instance, is developing a system for researchers called BioGPT that will focus on clinical research, not consumer health care, and it's trained on 15 million abstracts from studies. Maybe that will be more reliable, Gao said. This photo illustration shows snippets of a lengthy conversation that   pharmaceutical executive Wenda Gao recently had with ChatGPT. Gao's intent was to better understand how the chatbox worked, so he asked ChatGPT for research about a gene involved in the immune system and found that the chatbox fabricated references over and over again.  The ""correct references"" response from ChatGPT were not correct either. Guardrails for medical chatbots Halamka sees tremendous promise for chatbots and other AI technologies in health care but said they need ""guardrails and guidelines"" for use. ""I wouldn't release it without that oversight,"" he said. Halamka is part of the Coalition for Health AI, a collaboration of 150 experts from academic institutions like his, government agencies and technology companies, to craft guidelines for using artificial intelligence algorithms in health care. ""Enumerating the potholes in the road,"" as he put it. U.S. Rep. Ted Lieu, a Democrat from California, filed legislation in late January (drafted using ChatGPT, of course) ""to ensure that the development and deployment of AI is done in a way that is safe, ethical and respects the rights and privacy of all Americans, and that the benefits of AI are widely distributed and the risks are minimized."" Halamka said his first recommendation would be to require medical chatbots to disclose the sources they used for training. ""Credible data sources curated by humans"" should be the standard, he said. Then, he wants to see ongoing monitoring of the performance of AI, perhaps via a nationwide registry, making public the good things that came from programs like ChatGPT as well as the bad. Halamka said those improvements should let people enter a list of their symptoms into a program like ChatGPT and, if warranted, get automatically scheduled for an appointment, ""as opposed to (telling them) 'go eat twice your body weight in garlic,' because that's what Reddit said will cure your ailments.""",USAToday