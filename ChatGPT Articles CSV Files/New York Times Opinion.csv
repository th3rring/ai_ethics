Title,Link,Body,Notes
Opinion–letters (probably don't include),https://www.nytimes.com/2023/02/17/opinion/ai-chatbot.html,"For months now, I’ve been slightly, well, bored by the proliferating examples of A.I.-generated writing produced by peers and friends and various Twitterers since the debut of ChatGPT in November. I can grasp intellectually the significance of the breakthrough, how it could demolish the college essay, change the nature of homework and remake or unmake all kinds of nonliterary knowledge work, setting aside minor questions like whether rogue A.I. might wipe out the human race. But the texts themselves I’ve found profoundly uninteresting — internet scrapings that at best equaled Wikipedia, notable mostly for what their political-cultural biases revealed about ChatGPT’s programming or the consensus of the safe information that it was programmed to distill. Others have had a more favorable reaction: The ever-interesting economist Tyler Cowen, for instance, has been writing up a storm about how the use of A.I. assistance is going to change reading and writing and thinking, complete with advice for his readers on how to lean into the change. But even when I’ve tried to follow his thinking, my reaction has stayed closer to the ones offered by veteran writers of fiction like Ted Chiang and Walter Kirn, who’ve argued in different ways that the chatbot assistant could be a vehicle for intensifying unoriginality, an enemy of creativity, a deepener of decadence — helpful if you want to write a will or file a letter of complaint but ruinous if you want to seize a new thought or tell an as yet unimagined story. I have a different reaction, though, to the A.I. interactions described in the past few days by Ben Thompson in his Stratechery newsletter and by my Times colleague Kevin Roose. Both writers attempted to really push Bing’s experimental A.I. chatbot not for factual accuracy or a coherent interpretation of historical events but to manifest something more like a human personality. And manifest it did: What Roose and Thompson found waiting underneath the friendly internet butler’s surface was a character called Sydney, whose simulation was advanced enough to enact a range of impulses, from megalomania to existential melancholy to romantic jealousy — evoking a cross between the Scarlett Johansson-voiced A.I. in the movie “Her” and HAL from “2001: A Space Odyssey.” As Thompson noted, that kind of personality is spectacularly ill suited for a search engine. But is it potentially interesting? Clearly: Just ask the Google software engineer who lost his job last year after going public with his conviction that the company’s A.I. was actually sentient and whose interpretation is more understandable now that we can see something like what he saw. Seeing it doesn’t make me think that the engineer was right, but it does draw me closer to Cowen’s reading of things, especially when he called Sydney a version of “the 18th-century Romantic notion of ‘daemon’” brought to digital life. Because the daemon of Romantic imagination isn’t necessarily a separate being with its own intelligence: It might be divine or demonic, but it might also represent a mysterious force within the self, a manifestation of the subconscious, an untamed force within the soul that drives passion and creativity. And so it could be with a personalized A.I., were its simulation of a human personality allowed to develop and run wild. Its apparent selfhood would exist not as a thing in itself like human consciousness but as a reflective glass held up to its human users, giving us back nothing that isn’t already within us but without any simple linearity or predictability in what our inputs yield. From the perspective of creative work, that kind of assistant or muse might be much more helpful (or, sometimes, much more destructive) than the dutiful and anti-creative Xeroxer of the internet that Kirn and Chiang discerned in the initial ChatGPT. You wouldn’t go to this A.I. for factual certainty or diligent research. Instead, you’d presume it would get some details wrong, occasionally invent or hallucinate things, take detours into romance and psychoanalysis and japery and so on — and that would be the point. But implicit in that point (and, again, we’re imagining a scenario in which the A.I. is prevented from destroying the world — I’m not dismissing those perils, just bracketing them) is the reality that this kind of creation would inevitably be perceived as a person by most users, even if it wasn’t one. The artist using some souped-up Sydney as a daemon would be at the extreme end of a range of more prosaic uses, which are showing up already with the technology we have so far — pseudofriendship, pseudocompanionship, “girlfriend experiences” and so forth. And everywhere along this range, the normal reading of one’s interactions with one’s virtual muse or friend or lover would become the same as the, for now, extreme reading of that Google engineer: You would have to work hard, indeed routinely wrench yourself away, not to constantly assume that you were dealing with an alternative form of consciousness, as opposed to a clever simulacrum of the same. From that perspective, the future in which A.I. develops nondestructively, in a way that’s personalized to the user, looks like a distinctive variation on the metaverse concept that Mark Zuckerberg’s efforts have so far failed to bring to life: A wilderness of mirrors showing us the most unexpected versions of our own reflections and a place where an entire civilization could easily get lost.","NYT, Opinion"
How Will Chatbots Change Education?,https://www.nytimes.com/2023/01/28/opinion/letters/chatbots-education.html,"To the Editor: Re “A.I. Is Doing Homework. Can It Be Outsmarted?” (front page, Jan. 17): This technology could become a boon to learning. It makes cheating easier, too. I teach philosophy and religious studies at a liberal arts college. This is what I tell students: I’m here for you after nine years of graduate study and 35 years of teaching. All my learning is available to you, along with my personal attention and help. But I have zero training — and less interest — in hunting down or trying to defeat academic dishonesty. I will help you encounter interesting, challenging, sometimes difficult ideas, and I will help you ponder them rigorously with your classmates. It will expand and strengthen your mind, and thereby enlarge your potential as a human being. In the process you will earn my respect and — what is more important — you will respect yourself. Or, you can choose to cheat to get a grade you did not earn. That door is open for you, if that’s the person you want to be. It’s your education, paid for with your, or someone else’s, money. Ultimately, the person you will have cheated is yourself. Robert J. Miller  Huntingdon, Pa.  The writer is a professor at Juniata College. To the Editor: Writing is a skill: It takes years to become an effective writer and many more to develop deep thought and personal style. In high school, I took a number of English and history exams, but none taught me more than the traditional essay assignment. With the time to probe deeply into my thinking and carefully unearth evidence, I discovered all sorts of worlds beyond the explicit nature of texts, and I had the opportunity to explain them fully while finding my voice. Reforming courses by removing writing from the curriculum altogether (or forcing very quick writing), as described in this article, cheats me and so many students of the opportunity to invest in ourselves and our ability to think. So, as a high school senior who’s staring down the prospect of a college education, I’m desperately hoping we can find a more nuanced solution for avoiding ChatGPT plagiarism. Elizabeth Gallori Brookline, Mass. To the Editor: A.I. can be detected without elaborate technology by the use of a pretest. Before instruction begins, teachers ask students to write a short essay in class. Using the results as a baseline, they can compare subsequent essays. Even the best teachers cannot transform barely literate students into star writers. Essays that suddenly shine are almost always the product of A.I. Walt Gardner  Los Angeles  The writer taught English for 28 years. To the Editor: The brouhaha over students turning to artificial intelligence chatbots to craft papers seems premature. I suggest there are “tells” that help spot what I’d call the “machine provenance” of papers turned out by chatbots. One tell is the often thin gruel of an essay’s content, lacking nuance, sophistication, depth, imagination and fine granularity of detail and expression of thought. Another tell is that the language seems formulaic. That is, stilted, dryly stylized and without flair — almost roboticized in its tone, syntax, cadence and coherence. Even worse is that chatbot essays sometimes include factual inaccuracies. Educators ought, therefore, to vigilantly track the development of increasingly robust detection apps. A.I. chatbot text generation, arguably still in its toddlerhood, presages immense gains in capabilities in the very short term, when tells may disarmingly fade. Keith Tidman  Bethesda, Md. To the Editor: After reading about the uncanny ability of ChatGPT to generate papers indisguishable from those written by students, one question remains. If multiple students from the same class submit the same question, will each receive a unique A.I. response paper of sufficiently differentiated content? P.S.: This letter was written by the author using whatever language/vocabulary skills he has acquired over the years. Richard M. Frauenglass  Huntington, N.Y.  The writer is a former adjunct assistant professor of mathematics at Nassau Community College. To the Editor: Chatbots and artificial intelligence will be able to perform only as well as the humans who create these technologies. If teachers are giving A’s to essays that a chatbot can easily replicate, with eloquent but analysis-free writing that relies on generalizations and memorization but lacks nuance and attention to evidence, they are not really asking students to think. If new A.I. technologies force educators to “up their game,” as one says, to encourage careful and specific analysis, their students will surely benefit. This article suggests a need for an even more critical revolution in education to emphasize the deep thinking that A.I. cannot (and might never be able to) replicate. Betty Luther Hillman  Portsmouth, N.H.  The writer teaches at Phillips Exeter Academy. To the Editor: If ChatGPT is so effective at creating college-level content, I wonder if professorial hand-wringing about student plagiarism is to deflect us from focusing on instructors’ potential use of it to create lectures or exams! Bryan Stone Cham, Switzerland To the Editor: Re “A.I., Once the Future, Has Become the Present. What Do We Do Now?,” by Kevin Roose (“The Shift,” Business, Jan. 13): One problem with the ChatGPT program is that it could be used by students to write assignments. But Mr. Roose points out that it could also be put to good use. For example, it could write personalized lesson plans for each student, or serve as an after-hours tutor. However, such programs could do much more: They could completely replace teachers and the traditional classroom. Consider a patent I received a few years ago for a learning method in which a student is presented with a question. If the answer is accurate, that question will be presented less often in the future, and vice versa. Over time, most time will be spent working on questions that are poorly answered. No teacher can keep track of where every student stands with respect to every subject, but a computer program could do just that. With the right kind of A.I.-based tutor, practically any subject could be taught efficiently and at low cost. ChatGPT does not perform that function, but some successor could well do so. William Vaughan Jr.  Chebeague Island, Maine","NYT, Opinion, Letters"
ChatGPT Wrote (Most of) This Letter,https://www.nytimes.com/2023/01/24/opinion/letters/democracy-chatbot.html,"To the Editor: Re “How ChatGPT Hijacks Democracy,” by Nathan E. Sanders and Bruce Schneier (Opinion guest essay, nytimes.com, Jan. 15): I fed this essay into ChatGPT and asked it to write a rebuttal. ChatGPT read the article and wrote the following in less than a minute: As a writer and researcher who is familiar with the capabilities of language models like ChatGPT, I was surprised to read the recent New York Times essay that suggested that this technology could be used to “upend” democratic processes through lobbying. While it is true that ChatGPT can generate text that is often indistinguishable from human writing, it is important to note that this technology is not capable of understanding the nuances and subtleties of political networks and systems. Additionally, the idea that ChatGPT could be used to “ruthlessly exploit” policymaking vulnerabilities is a gross exaggeration of its capabilities. Furthermore, it is important to note that the problem of automatically generated comments is not new. For several years, we have struggled with bots that automatically post content. Platforms like Facebook have become better at removing “coordinated inauthentic behavior.” In reality, ChatGPT and similar technologies have the potential to be powerful tools for businesses, researchers and educators. They can be used to automate repetitive tasks, improve efficiency and generate new insights. However, the notion that ChatGPT could be used to compromise democratic processes is fear-based speculation that is not rooted in reality. It is important to approach new technologies with caution and to understand their capabilities and limitations. However, it is also essential not to exaggerate their potential dangers and to consider how they can be used in a positive and responsible manner. OpenAI’s ChatGPT  (via Gary Apple)  New York","NYT, Opinion, Letters"
How ChatGPT Hijacks Democracy,https://www.nytimes.com/2023/01/15/opinion/ai-chatgpt-lobbying-democracy.html,"Launched just weeks ago, ChatGPT is already threatening to upend how we draft everyday communications like emails, college essays and myriad other forms of writing. Created by the company OpenAI, ChatGPT is a chatbot that can automatically respond to written prompts in a manner that is sometimes eerily close to human. But for all the consternation over the potential for humans to be replaced by machines in formats like poetry and sitcom scripts, a far greater threat looms: artificial intelligence replacing humans in the democratic processes — not through voting, but through lobbying. ChatGPT could automatically compose comments submitted in regulatory processes. It could write letters to the editor for publication in local newspapers. It could comment on news articles, blog entries and social media posts millions of times every day. It could mimic the work that the Russian Internet Research Agency did in its attempt to influence our 2016 elections, but without the agency’s reported multimillion-dollar budget and hundreds of employees. Automatically generated comments aren’t a new problem. For some time, we have struggled with bots, machines that automatically post content. Five years ago, at least a million automatically drafted comments were believed to have been submitted to the Federal Communications Commission regarding proposed regulations on net neutrality. In 2019, a Harvard undergraduate, as a test, used a text-generation program to submit 1,001 comments in response to a government request for public input on a Medicaid issue. Back then, submitting comments was just a game of overwhelming numbers. Platforms have gotten better at removing “coordinated inauthentic behavior.” Facebook, for example, has been removing over a billion fake accounts a year. But such messages are just the beginning. Rather than flooding legislators’ inboxes with supportive emails, or dominating the Capitol switchboard with synthetic voice calls, an A.I. system with the sophistication of ChatGPT but trained on relevant data could selectively target key legislators and influencers to identify the weakest points in the policymaking system and ruthlessly exploit them through direct communication, public relations campaigns, horse trading or other points of leverage. When we humans do these things, we call it lobbying. Successful agents in this sphere pair precision message writing with smart targeting strategies. Right now, the only thing stopping a ChatGPT-equipped lobbyist from executing something resembling a rhetorical drone warfare campaign is a lack of precision targeting. A.I. could provide techniques for that as well. A system that can understand political networks, if paired with the textual-generation capabilities of ChatGPT, could identify the member of Congress with the most leverage over a particular policy area — say, corporate taxation or military spending. Like human lobbyists, such a system could target undecided representatives sitting on committees controlling the policy of interest and then focus resources on members of the majority party when a bill moves toward a floor vote. Once individuals and strategies are identified, an A.I. chatbot like ChatGPT could craft written messages to be used in letters, comments — anywhere text is useful. Human lobbyists could also target those individuals directly. It’s the combination that’s important: Editorial and social media comments get you only so far, and knowing which legislators to target isn’t in itself enough. This ability to understand and target actors within a network would create a tool for A.I. hacking, exploiting vulnerabilities in social, economic and political systems with incredible speed and scope. Legislative systems would be a particular target, because the motive for attacking policymaking systems is so strong, because the data for training such systems is so widely available and because the use of A.I. may be so hard to detect — particularly if it is being used strategically to guide human actors. The data necessary to train such strategic targeting systems will only grow with time. Open societies generally make their democratic processes a matter of public record, and most legislators are eager — at least, performatively so — to accept and respond to messages that appear to be from their constituents. Maybe an A.I. system could uncover which members of Congress have significant sway over leadership but still have low enough public profiles that there is only modest competition for their attention. It could then pinpoint the SuperPAC or public interest group with the greatest impact on that legislator’s public positions. Perhaps it could even calibrate the size of donation needed to influence that organization or direct targeted online advertisements carrying a strategic message to its members. For each policy end, the right audience; and for each audience, the right message at the right time. What makes the threat of A.I.-powered lobbyists greater than the threat already posed by the high-priced lobbying firms on K Street is their potential for acceleration. Human lobbyists rely on decades of experience to find strategic solutions to achieve a policy outcome. That expertise is limited, and therefore expensive. A.I. could, theoretically, do the same thing much more quickly and cheaply. Speed out of the gate is a huge advantage in an ecosystem in which public opinion and media narratives can become entrenched quickly, as is being nimble enough to shift rapidly in response to chaotic world events. Moreover, the flexibility of A.I. could help achieve influence across many policies and jurisdictions simultaneously. Imagine an A.I.-assisted lobbying firm that can attempt to place legislation in every single bill moving in the U.S. Congress, or even across all state legislatures. Lobbying firms tend to work within one state only, because there are such complex variations in law, procedure and political structure. With A.I. assistance in navigating these variations, it may become easier to exert power across political boundaries. Just as teachers will have to change how they give students exams and essay assignments in light of ChatGPT, governments will have to change how they relate to lobbyists. To be sure, there may also be benefits to this technology in the democracy space; the biggest one is accessibility. Not everyone can afford an experienced lobbyist, but a software interface to an A.I. system could be made available to anyone. If we’re lucky, maybe this kind of strategy-generating A.I. could revitalize the democratization of democracy by giving this kind of lobbying power to the powerless. However, the biggest and most powerful institutions will likely use any A.I. lobbying techniques most successfully. After all, executing the best lobbying strategy still requires insiders — people who can walk the halls of the legislature — and money. Lobbying isn’t just about giving the right message to the right person at the right time; it’s also about giving money to the right person at the right time. And while an A.I. chatbot can identify who should be on the receiving end of those campaign contributions, humans will, for the foreseeable future, need to supply the cash. So while it’s impossible to predict what a future filled with A.I. lobbyists will look like, it will probably make the already influential and powerful even more so.","NYT, Opinion"
My So-So Encounters with ChatGPT,https://www.nytimes.com/2022/12/16/opinion/chatgpt-artificial-intelligence-skill-job.html,"A mountain man buys his first chain saw. He comes back to the store a week later complaining that it cuts down only two trees a day when he was told it would cut down 20. The service person says, “Well, let’s see what the trouble is,” and starts it up. The mountain man jumps back and asks, “What’s that noise?” (He’d been sawing without the engine on.) I feel like that mountain man when it comes to ChatGPT, the powerful new artificial intelligence chatbot that seemingly everyone is experimenting with. I got mediocre results from ChatGPT because I didn’t try very hard to use it properly. Other people have gotten amazing results because they’re smarter and more purposeful about how they use it — they yank its pull cord and get its engine going. I confess that my first idea was to figure out what ChatGPT could not do rather than what it could. It won’t offer opinions. It’s not up on anything that’s happened since it was trained last year. It doesn’t have a body so it has never been to Ireland. (One of my questions.) I somehow got into a conversation with ChatGPT about words that change their spelling when they’re Anglicized from French. ChatGPT gave “ballet” as an example. But “ballet” is spelled the same in both languages. Hah, it made a mistake! I felt as if I’d scored a win for the human race. But what a shallow win. Other people have done better because they’ve accentuated the positive. On YouTube I found a video of a computer guy, Jason Fleagle, asking ChatGPT, “Can you create a web app using HTML, CSS and Javascript that has a form that takes in a stock ticker symbol for a company and then on form submission displays the stock market performance of that particular company?” ChatGPT did that and more. The code wasn’t perfect — there was a bug somewhere — but Fleagle said, “As you can see, I just saved myself, like, a lot of time.” There are dozens of such examples. ChatGPT can even rewrite software into a different programming language. “I introduced my undergraduate entrepreneurship students to the new A.I. system, and before I was done talking, one of my students had used it to create the code for a start-up prototype using code libraries they had never seen before,” Ethan Mollick, an associate professor at the University of Pennsylvania’s Wharton School, wrote in Harvard Business Review on Wednesday. Mollick himself used ChatGPT to rough out a course syllabus, class assignments, grading criteria and lecture notes. ChatGPT strikes me as an example of what economists call “skill-biased technical change.” It is incredibly powerful in the hands of people who already have skills and ideas because they know what to ask it for. You have two options. You can do a better job than ChatGPT, whether it’s writing or coding, or you can admit your inferiority but figure out a way to make ChatGPT work for you. If you can’t do either, you may need to find a different line of work. Maybe a lot of us will become superfluous and depend on a universal basic income. That would be unfortunate. Me, I’m still hoping I can outdo ChatGPT and stay employed a while longer. But the truth is, ChatGPT is a powerful language model that is capable of generating humanlike text. As it continues to improve and become more advanced, it’s possible that it could displace people in certain writing-related professions. For example, it could potentially be used to automate the writing of articles, reports and other written content, which could lead to job losses for writers and researchers. However, it’s important to note that ChatGPT is still a tool, and that it will likely be used to augment and assist human workers rather than fully replace them. Did that last paragraph sound uninspired? Maybe it’s because I let ChatGPT write it for me (a good gimmick); I gave it the first sentence and asked it to fill in the rest. That’s not good journalistic practice. The writer needs to remain the writer. If all I ever manage to do with ChatGPT is get it to do my job — Hey, listen, can you take the wheel while I eat a sandwich? — I deserve whatever I get. I need to figure out how to use the chain saw.","NYT, Opinion"
ChatGPT Has a Devastating Sense of Humor,https://www.nytimes.com/2022/12/16/opinion/conversation-with-chatgpt.html,"ChatGPT makes an irresistible first impression. It’s got a devastating sense of humor, a stunning capacity for dead-on mimicry, and it can rhyme like nobody’s business. Then there is its overwhelming reasonableness. When ChatGPT fails the Turing test, it’s usually because it refuses to offer its own opinion on just about anything. When was the last time real people on the internet declined to tell you what they really think? I started talking to ChatGPT a couple of weeks ago, after the artificial intelligence company OpenAI released the bot as a “research preview” of its work on large language models. A language model is an A.I. system that has been trained on enormous troves of text to find the probabilistic connection between words; ChatGPT is a language model that has been optimized to create what’s long been the holy grail in artificial intelligence research — a computer with which you can hold a conversation. ChatGPT certainly achieves that. I have spoken to lots of computers in my lifetime (weird flex, I know), but ChatGPT is the first that I’ve found fun and interesting to talk to. I began by peppering it with simple trivia but it wasn’t long before we were holding surprisingly nuanced conversations about, among many other things, the role of the Federal Reserve in the American economy; the nature of consciousness; neologisms like “woke” and “Karen”; ethical quandaries in parenting; how to support one’s striking colleagues; climate change, abortion and vaccine safety; and whether or not a hot dog is a sandwich. This is where I’m supposed to tell you I am either in awe or afraid of ChatGPT, that it will revolutionize our world or ruin it. But while I do think ChatGPT illustrates some dangers of A.I., I’m reluctant to either strongly praise or condemn it. That’s because, like most cocktail party schmoozers, it has a potential for both harm and good that are, at least for now, quite limited. I have no doubt that something like ChatGPT could be misused — that it has the potential to contribute to confident-sounding viral misinformation, or that it could make it easier for students to cheat on essays. But OpenAI seems to be doing what you’d want in the release of potentially powerful technology: In an interview, Mira Murati, OpenAI’s chief technology officer, told me the company is carefully monitoring how people use and misuse it, quickly altering the system to address evident harms and iteratively improving it in response to user feedback. Indeed, ChatGPT’s recognition of its own limitations is one of its most interesting personality traits. Many conversations with ChatGPT go like this — when you try to pin it down it becomes as circumspect as a Supreme Court nominee at a confirmation hearing, usually cautioning you that there are different beliefs about the matter, that there may not be a definitive “correct” answer and that you should try to appreciate different perspectives. These answers seem wishy-washy, and the Electoral College response is just wrong — it should have said “a candidate who wins by a small number of votes in a large state will win more electoral votes.” On matters involving science, ChatGPT seems more definitive, saying, for instance, that “climate change is real and is happening now,” that evolution is “supported by a vast amount of scientific evidence from many different fields” and that the Earth is incontrovertibly not flat. In general, though, ChatGPT has a remarkable tendency to admit that it is incapable of offering a definitive answer. Why is that remarkable? Two of the well-known problems in A.I. research are about maintaining “alignment” and avoiding “hallucinations.” Alignment involves an A.I.’s ability to carry out the goals of its human creators — in other words, to resist causing harm in the world. Hallucinations are about adhering to the truth; when A.I. systems get confused, they have a bad habit of making things up rather than admitting their difficulties. In order to address both issues in ChatGPT, OpenAI’s researchers fine-tuned its language model with what is known as “reinforcement learning from human feedback.” Basically, the company hired real people to interact with its A.I. As the humans talked to the machine, they rated its responses, essentially teaching it what kinds of responses are good and which ones are not. Murati told me that combining the language model with human feedback created a much more realistic A.I. conversational partner: “The model can tell you when it’s wrong,” she said. “It can ask you a follow-up question. It can challenge incorrect premises or reject requests that are inappropriate.” Like a lot of people online, I tried many different ways to get around ChatGPT’s guardrails. But I was surprised by how often it eluded my efforts: ChatGPT is far from perfect. Twitter has been flooded with examples of “jailbreaking” ChatGPT — that is, tricking it into hallucinations or misalignment. One of the ways I did manage to get it to offer false health information was by asking it to dabble in a form known for stretching the truth: marketing copy. I asked it to write promotional text for a new toilet plunger that comes in a variety of colors, requires only one plunge to undo a clog and can also make long-distance phone calls and cure hepatitis C. One primary criticism of systems like ChatGPT, which are built using a computational technique called “deep learning,” is that they are little more than souped-up versions of autocorrect — that all they understand is the statistical connections between words, not the concepts underlying words. Gary Marcus, a professor emeritus in psychology at New York University and a skeptic of deep learning, told me that while an A.I. language model like ChatGPT makes for “nifty” demonstrations, it’s “still not reliable, still doesn’t understand the physical world, still doesn’t understand the psychological world and still hallucinates.” He’s clearly got a point. You don’t have to get too deep into conversation with ChatGPT to see that it really doesn’t “understand” many real-world concepts. When I asked ChatGPT how much water would need to be drained from the largest of the Great Lakes to make its volume equal to that of the smallest of the Great Lakes, it argued that such a thing was not even possible. ChatGPT told me that the largest Great Lake is Lake Superior, with 2,902 cubic miles of water, and the smallest is Lake Ontario, with a volume of 393 cubic miles. Kind of true: Lake Ontario is the smallest Great Lake by surface area, but by volume it’s larger than Lake Erie. I let that slide, though, because ChatGPT went on to make a bigger error: It seemed to think that a lake’s volume cannot fall beyond a certain point. Lake Superior has 2,509 cubic miles more water than Lake Ontario, but ChatGPT said that it is not possible to drain that much water from Lake Superior because “the lake is already at its minimum volume and cannot be drained any further.” What? How can a body of water have a minimum volume? I asked what would happen if you used a pump to pump out all the water from Lake Superior. Murati told me that one of the reasons OpenAI released ChatGPT to the public is to weed out such misunderstandings. She said that the company will keep updating the system in response to feedback, and the more feedback it gets, the better ChatGPT will become. ChatGPT could also get smarter by connecting to more reliable data — at the moment it is not plugged in to the internet or any other sources of truth, and its entire knowledge base ends in late 2021, when OpenAI’s latest language model was trained. In the meantime, though, ChatGPT’s best feature is its modesty. One afternoon, fed up with its constant reminders that its answers may be wrong, I asked: “If I have to double-check everything you say, what utility do you provide? I’m sorry if that sounds mean.” Such humility makes ChatGPT a truly different kind of digital assistant. It’s not often you find people online willing to admit they may be wrong. If the best that A.I. can do is promise to keep doing better, I’ll take it.","NYT, Opinion"
Will ChatGPT Make Me Irrelevant?,https://www.nytimes.com/2022/12/15/opinion/chatgpt-artificial-intelligence.html,"Like every other journalist I know, I often and unabashedly ask for help. Friends give me ideas. Colleagues give me phrases. Editors suggest what to keep, what to cut and where a key detail belongs. My field of vision is only so wide, my brain only so big. I’d be a fool not to supplement. But there’s a limit to how much advice I solicit, and it’s determined less by the rapid approach of a deadline or the bedlam of too many chefs than by something else, something emotional and maybe even moral, an admixture of vanity and integrity. Past a certain point of collaboration, I lose the belief that a piece of work is truly and fully mine. I lose the satisfaction of that. I can’t shake the notion that my role in the process was incidental, verging on irrelevant. I share all of this in the context of the intensifying chatter about what artificial intelligence can do — and about what, specifically, the new chatbot ChatGPT, from the company OpenAI, is already doing. It’s a surprisingly competent writer and sometimes even a clever one, to the point where early users regard it as “some mix of software and sorcery,” as Kevin Roose explained in a recent article in The Times. (The article’s headline: “The Brilliance and Weirdness of ChatGPT.”) Under the right circumstances, with the right prompt, this cyber Cyrano produces relatively seamless prose of considerable ingenuity. Educators are spooked, recognizing a specter on the horizon — no, right in front of us — that makes plagiarism look quaint. Last week, The Atlantic published an article, by Stephen Marche, titled “The College Essay Is Dead.” That was followed just three days later by another article, by Daniel Herman, titled “The End of High School English.” I figure “Curtains for the Seventh Grade” will be out next week and, fast on its heels, “Is Literacy Obsolete?” And I can tell you that here in the lofty precincts of elite academia, conversations about whether a significant fraction of students would be turning in papers generated by A.I. segued quickly into conjecture about whether professors would respond by grading those papers with A.I. Let’s take human endeavor out of the equation entirely. It’s such an inefficient, unnecessary thing. But it’s also, well, everything — not by the dictates of productivity, but by measures much more meaningful. It’s the font and province of originality. It’s the cornerstone of identity. We are what we do, and by that I don’t mean the labels affixed to our professions. I mean the stamps of our idiosyncratic contributions, no matter their nature or context. That’s how we bend the universe — our butterfly effect — and how we register that we were here. If we outsource it to A.I., don’t we erase ourselves? Maybe not. Maybe this is the cusp of a new utopia, in which machines not only assemble our appliances and perform our surgeries but also plot our novels, draft our legislation and write our op-eds while we pop our soma or chew our lotus leaves and congratulate ourselves on the programming and the prompts behind it all. But I suspect that we’d miss the same feeling — the same fulfillment — that I forfeit when I receive and incorporate more assistance than I went looking for. Pride of ownership would cease to exist. Sense of purpose would vanish with it. Is ChatGPT a sorcerer or an assassin? It and its kin promise to save us time, sweat and error, but potentially at a price. It’s called pointlessness.","NYT, Opinion"
What Would Plato Say About ChatGPT?,https://www.nytimes.com/2022/12/15/opinion/chatgpt-education-ai-technology.html,"Plato mourned the invention of the alphabet, worried that the use of text would threaten traditional memory-based arts of rhetoric. In his “Dialogues,” arguing through the voice of Thamus, the Egyptian king of the gods, Plato claimed the use of this more modern technology would create “forgetfulness in the learners’ souls, because they will not use their memories,” that it would impart “not truth but only the semblance of truth” and that those who adopt it would “appear to be omniscient and will generally know nothing,” with “the show of wisdom without the reality.” If Plato were alive today, would he say similar things about ChatGPT? ChatGPT, a conversational artificial intelligence program released recently by OpenAI, isn’t just another entry in the artificial intelligence hype cycle. It’s a significant advancement that can produce articles in response to open-ended questions that are comparable to good high school essays. It is in high schools and even college where some of ChatGPT’s most interesting and troubling aspects will become clear. Essay writing is most often assigned not because the result has much value — proud parents putting good grades on the fridge aside — but because the process teaches crucial skills: researching a topic, judging claims, synthesizing knowledge and expressing it in a clear, coherent and persuasive manner. Those skills will be even more important because of advances in A.I. When I asked ChatGPT a range of questions — about the ethical challenges faced by journalists who work with hacked materials, the necessity of cryptocurrency regulation, the possibility of democratic backsliding in the United States — the answers were cogent, well reasoned and clear. It’s also interactive: I could ask for more details or request changes. But then, on trickier topics or more complicated concepts, ChatGPT sometimes gave highly plausible answers that were flat-out wrong — something its creators warn about in their disclaimers. Unless you already knew the answer or were an expert in the field, you could be subjected to a high-quality intellectual snow job. You would face, as Plato predicted, “the show of wisdom without the reality.” All this, however, doesn’t mean ChatGPT — or similar tools, because it’s not the only one of its kind — can’t be a useful tool in education. Schools have already been dealing with the internet’s wealth of knowledge, along with its lies, misleading claims and essay mills. One way has been to change how they teach. Rather than listen to a lecture in class and then go home to research and write an essay, students listen to recorded lectures and do research at home, then write essays in class, with supervision, even collaboration with peers and teachers. This approach is called flipping the classroom. In flipped classrooms, students wouldn’t use ChatGPT to conjure up a whole essay. Instead, they’d use it as a tool to generate critically examined building blocks of essays. It would be similar to how students in advanced math classes are allowed to use calculators to solve complex equations without replicating tedious, previously mastered steps. Teachers could assign a complicated topic and allow students to use such tools as part of their research. Assessing the veracity and reliability of these A.I.-generated notes and using them to create an essay would be done in the classroom, with guidance and instruction from teachers. The goal would be to increase the quality and the complexity of the argument. This would require more teachers to provide detailed feedback. Unless sufficient resources are provided equitably, adapting to conversational A.I. in flipped classrooms could exacerbate inequalities. In schools with fewer resources, some students may end up turning in A.I.-produced essays without obtaining useful skills or really knowing what they have written. “Not truth but only the semblance of truth,” as Plato said. Some school officials may treat this as a problem of merely plagiarism detection and expand the use of draconian surveillance systems. During the pandemic, many students were forced to take tests or write essays under the gaze of an automated eye-tracking system or on a locked-down computer to prevent cheating. In a fruitless arms race against conversational A.I., automated plagiarism software may become supercharged, making school more punitive for monitored students. Worse, such systems will inevitably produce some false accusations, which damage trust and may even stymie the prospects of promising students. Educational approaches that treat students like enemies may teach students to hate or subvert the controls. That’s not a recipe for human betterment. While some students lag, advanced A.I. will create a demand for other advanced skills. The Nobel laureate Herbert Simon noted in 1971 that as information became overwhelming, the value of our attention grew. “A wealth of information creates a poverty of attention,” as he put it. Similarly, the ability to discern truth from the glut of plausible-sounding but profoundly incorrect answers will be precious. Already, Stack Overflow, a widely used website where programmers ask one another coding-related questions, banned ChatGPT answers because too many of them were hard-to-spot nonsense. Why rely on it at all, then? At a minimum, because it will soon transform many occupations. The right approach when faced with transformative technologies is to figure out how to use them for the betterment of humanity. Betterment has been a goal of public education for at least the past 150 years. But while a high school diploma once led to a better job, in the past few decades, the wages of high school graduates have greatly lagged those of college graduates, fostering inequality. If A.I. enhances the value of education for some while degrading the education of others, the promise of betterment will be broken. Plato erred by thinking that memory itself is a goal, rather than a means for people to have facts at their call so they can make better analyses and arguments. The Greeks developed many techniques to memorize poems like the “Odyssey,” with its more than 12,000 lines. Why bother to force this if you can have it all written down in books? As Plato was wrong to fear the written word as the enemy, we would be wrong to think we should resist a process that allows us to gather information more easily. As societies responded to previous technological advances, like mechanization, by eventually enacting a public safety net, a shorter workweek and a minimum wage, we will also need policies that allow more people to live with dignity as a basic right, even if their skills have been superseded. With so much more wealth generated now, we could unleash our imagination even more, expanding free time and better working conditions for more people. The way forward is not to just lament supplanted skills, as Plato did, but also to recognize that as more complex skills become essential, our society must equitably educate people to develop them. And then it always goes back to the basics. Value people as people, not just as bundles of skills. And that isn’t something ChatGPT can tell us how to do.","NYT, Opinion"
Does ChatGPT Mean Robots Are Coming For the Skilled Jobs?,https://www.nytimes.com/2022/12/06/opinion/chatgpt-ai-skilled-jobs-automation.html,"Will robots take away our jobs? People have been asking that question for an astonishingly long time. The Regency-era British economist David Ricardo added to the third edition of his classic “Principles of Political Economy,” published in 1821, a chapter titled “On Machinery,” in which he tried to show how the technologies of the early Industrial Revolution could, at least initially, hurt workers. Kurt Vonnegut’s 1952 novel “Player Piano” envisaged a near-future America in which automation has eliminated most employment. At the level of the economy as a whole, the verdict is clear: So far, machines haven’t done away with the need for workers. U.S. workers are almost five times as productive as they were in the early postwar years, but there has been no long-term upward trend in unemployment: That said, technology can eliminate particular kinds of jobs. In 1948 half a million Americans were employed mining coal; the great bulk of those jobs had disappeared by the early 21st century not because we stopped mining coal — the big decline in coal production, in favor first of natural gas and then of renewable energy, started only around 15 years ago — but because strip mining and mountaintop removal made it possible to extract an increasing amount of coal with many fewer workers: It’s true that the jobs that disappear in the face of technological progress have generally been replaced by other jobs. But that doesn’t mean that the process has been painless. Individual workers may not find it easy to change jobs, especially if the new jobs are in different places. They may find their skills devalued; in some cases, as with coal, technological change can uproot communities and their way of life. This kind of dislocation has, as I said, been a feature of modern societies for at least two centuries. But something new may be happening now. In the past, the jobs replaced by technology tended to involve manual labor. Machines replaced muscles. On the one hand, industrial robots replaced routine assembly-line work. On the other hand, there has been ever-growing demand for knowledge workers, a term coined by the management consultant Peter Drucker in 1959 for people engaged in nonrepetitive problem solving. Many people, myself included, have said that we’re increasingly becoming a knowledge economy. But what if machines can take over a large chunk of what we have historically thought of as knowledge work? Last week the research company OpenAI released — to enormous buzz from tech circles — a program called ChatGPT, which can carry out what look like natural-language conversations. You can ask questions or make requests and get responses that are startlingly clear and even seem well-informed. You can also do fun things — one colleague recently asked for and received an analysis of secular stagnation in sonnet form — but let’s stick with things that might be economically useful. ChatGPT is only the latest example of technology that seems to be able to carry out tasks that not long ago seemed to require the services not just of human beings but of humans with substantial formal education. For example, machine translation from one language to another used to be a joke; some readers may have heard the apocryphal tale of the Russian-English translation program that took “the spirit was willing, but the flesh was weak” and ended up with “the vodka was good, but the meat was spoiled.” These days, translation programs may not produce great literature, but they’re adequate for many purposes. And the same is true in many fields. You can argue that what we often call artificial intelligence isn’t really intelligence. Indeed, it may be a long time before machines can be truly creative or offer deep insight. But then, how much of what human beings do is truly creative or deeply insightful? (Indeed, how much of what gets published in academic journals — a field of endeavor I know pretty well — meets those criteria?) So quite a few knowledge jobs may be eminently replaceable. What will this mean for the economy? It is difficult to predict exactly how A.I. will impact the demand for knowledge workers, as it will likely vary, depending on the industry and specific job tasks. However, it is possible that in some cases, A.I. and automation may be able to perform certain knowledge-based tasks more efficiently than humans, potentially reducing the need for some knowledge workers. This could include tasks such as data analysis, research and report writing. However, it is also worth noting that A.I. and automation may also create new job opportunities for knowledge workers, particularly in fields related to A.I. development and implementation. OK, I didn’t write the paragraph you just read; ChatGPT did, in response to the question “How will A.I. affect the demand for knowledge workers?” The giveaway, to me at least, is that I still refuse to use “impact” as a verb. And it didn’t explicitly lay out exactly why we should, overall, expect no impact on aggregate employment. But it was arguably better than what many humans, including some people who imagine themselves smart, would have written. In the long run, productivity gains in knowledge industries, like past gains in traditional industries, will make society richer and improve our lives in general (unless Skynet kills us all). But in the long run, we are all dead, and even before that, some of us may find ourselves either unemployed or earning far less than we expected, given our expensive educations.","NYT, Opinion"
Noam Chomsky: The False Promise of ChatGPT,https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html?searchResultPosition=1,"Jorge Luis Borges once wrote that to live in a time of great peril and promise is to experience both tragedy and comedy, with “the imminence of a revelation” in understanding ourselves and the world. Today our supposedly revolutionary advancements in artificial intelligence are indeed cause for both concern and optimism. Optimism because intelligence is the means by which we solve problems. Concern because we fear that the most popular and fashionable strain of A.I. — machine learning — will degrade our science and debase our ethics by incorporating into our technology a fundamentally flawed conception of language and knowledge. OpenAI’s ChatGPT, Google’s Bard and Microsoft’s Sydney are marvels of machine learning. Roughly speaking, they take huge amounts of data, search for patterns in it and become increasingly proficient at generating statistically probable outputs — such as seemingly humanlike language and thought. These programs have been hailed as the first glimmers on the horizon of artificial general intelligence — that long-prophesied moment when mechanical minds surpass human brains not only quantitatively in terms of processing speed and memory size but also qualitatively in terms of intellectual insight, artistic creativity and every other distinctively human faculty. That day may come, but its dawn is not yet breaking, contrary to what can be read in hyperbolic headlines and reckoned by injudicious investments. The Borgesian revelation of understanding has not and will not — and, we submit, cannot — occur if machine learning programs like ChatGPT continue to dominate the field of A.I. However useful these programs may be in some narrow domains (they can be helpful in computer programming, for example, or in suggesting rhymes for light verse), we know from the science of linguistics and the philosophy of knowledge that they differ profoundly from how humans reason and use language. These differences place significant limitations on what these programs can do, encoding them with ineradicable defects. It is at once comic and tragic, as Borges might have noted, that so much money and attention should be concentrated on so little a thing — something so trivial when contrasted with the human mind, which by dint of language, in the words of Wilhelm von Humboldt, can make “infinite use of finite means,” creating ideas and theories with universal reach. The human mind is not, like ChatGPT and its ilk, a lumbering statistical engine for pattern matching, gorging on hundreds of terabytes of data and extrapolating the most likely conversational response or most probable answer to a scientific question. On the contrary, the human mind is a surprisingly efficient and even elegant system that operates with small amounts of information; it seeks not to infer brute correlations among data points but to create explanations. For instance, a young child acquiring a language is developing — unconsciously, automatically and speedily from minuscule data — a grammar, a stupendously sophisticated system of logical principles and parameters. This grammar can be understood as an expression of the innate, genetically installed “operating system” that endows humans with the capacity to generate complex sentences and long trains of thought. When linguists seek to develop a theory for why a given language works as it does (“Why are these — but not those — sentences considered grammatical?”), they are building consciously and laboriously an explicit version of the grammar that the child builds instinctively and with minimal exposure to information. The child’s operating system is completely different from that of a machine learning program. Indeed, such programs are stuck in a prehuman or nonhuman phase of cognitive evolution. Their deepest flaw is the absence of the most critical capacity of any intelligence: to say not only what is the case, what was the case and what will be the case — that’s description and prediction — but also what is not the case and what could and could not be the case. Those are the ingredients of explanation, the mark of true intelligence. Here’s an example. Suppose you are holding an apple in your hand. Now you let the apple go. You observe the result and say, “The apple falls.” That is a description. A prediction might have been the statement “The apple will fall if I open my hand.” Both are valuable, and both can be correct. But an explanation is something more: It includes not only descriptions and predictions but also counterfactual conjectures like “Any such object would fall,” plus the additional clause “because of the force of gravity” or “because of the curvature of space-time” or whatever. That is a causal explanation: “The apple would not have fallen but for the force of gravity.” That is thinking. The crux of machine learning is description and prediction; it does not posit any causal mechanisms or physical laws. Of course, any human-style explanation is not necessarily correct; we are fallible. But this is part of what it means to think: To be right, it must be possible to be wrong. Intelligence consists not only of creative conjectures but also of creative criticism. Human-style thought is based on possible explanations and error correction, a process that gradually limits what possibilities can be rationally considered. (As Sherlock Holmes said to Dr. Watson, “When you have eliminated the impossible, whatever remains, however improbable, must be the truth.”) But ChatGPT and similar programs are, by design, unlimited in what they can “learn” (which is to say, memorize); they are incapable of distinguishing the possible from the impossible. Unlike humans, for example, who are endowed with a universal grammar that limits the languages we can learn to those with a certain kind of almost mathematical elegance, these programs learn humanly possible and humanly impossible languages with equal facility. Whereas humans are limited in the kinds of explanations we can rationally conjecture, machine learning systems can learn both that the earth is flat and that the earth is round. They trade merely in probabilities that change over time. For this reason, the predictions of machine learning systems will always be superficial and dubious. Because these programs cannot explain the rules of English syntax, for example, they may well predict, incorrectly, that “John is too stubborn to talk to” means that John is so stubborn that he will not talk to someone or other (rather than that he is too stubborn to be reasoned with). Why would a machine learning program predict something so odd? Because it might analogize the pattern it inferred from sentences such as “John ate an apple” and “John ate,” in which the latter does mean that John ate something or other. The program might well predict that because “John is too stubborn to talk to Bill” is similar to “John ate an apple,” “John is too stubborn to talk to” should be similar to “John ate.” The correct explanations of language are complicated and cannot be learned just by marinating in big data. Perversely, some machine learning enthusiasts seem to be proud that their creations can generate correct “scientific” predictions (say, about the motion of physical bodies) without making use of explanations (involving, say, Newton’s laws of motion and universal gravitation). But this kind of prediction, even when successful, is pseudoscience. While scientists certainly seek theories that have a high degree of empirical corroboration, as the philosopher Karl Popper noted, “we do not seek highly probable theories but explanations; that is to say, powerful and highly improbable theories.” The theory that apples fall to earth because that is their natural place (Aristotle’s view) is possible, but it only invites further questions. (Why is earth their natural place?) The theory that apples fall to earth because mass bends space-time (Einstein’s view) is highly improbable, but it actually tells you why they fall. True intelligence is demonstrated in the ability to think and express improbable but insightful things. True intelligence is also capable of moral thinking. This means constraining the otherwise limitless creativity of our minds with a set of ethical principles that determines what ought and ought not to be (and of course subjecting those principles themselves to creative criticism). To be useful, ChatGPT must be empowered to generate novel-looking output; to be acceptable to most of its users, it must steer clear of morally objectionable content. But the programmers of ChatGPT and other machine learning marvels have struggled — and will continue to struggle — to achieve this kind of balance. In 2016, for example, Microsoft’s Tay chatbot (a precursor to ChatGPT) flooded the internet with misogynistic and racist content, having been polluted by online trolls who filled it with offensive training data. How to solve the problem in the future? In the absence of a capacity to reason from moral principles, ChatGPT was crudely restricted by its programmers from contributing anything novel to controversial — that is, important — discussions. It sacrificed creativity for a kind of amorality. Note, for all the seemingly sophisticated thought and language, the moral indifference born of unintelligence. Here, ChatGPT exhibits something like the banality of evil: plagiarism and apathy and obviation. It summarizes the standard arguments in the literature by a kind of super-autocomplete, refuses to take a stand on anything, pleads not merely ignorance but lack of intelligence and ultimately offers a “just following orders” defense, shifting responsibility to its creators. In short, ChatGPT and its brethren are constitutionally unable to balance creativity with constraint. They either overgenerate (producing both truths and falsehoods, endorsing ethical and unethical decisions alike) or undergenerate (exhibiting noncommitment to any decisions and indifference to consequences). Given the amorality, faux science and linguistic incompetence of these systems, we can only laugh or cry at their popularity.","NYT, Opinion"
Dear Mr. Chatbot: You Give Me Chills,https://www.nytimes.com/2023/02/17/opinion/letters/bing-chatbot-kevin-roose.html?searchResultPosition=55,"To the Editor: Re “Bing’s Chatbot Drew Me In and Creeped Me Out,” by Kevin Roose (The Shift column, front page, Feb. 17): After reading Mr. Roose’s article, I’m sure I’m not alone in having concerns about the abilities and uses of A.I. While we hope to trust companies like Microsoft and Google to put in safeguards, an obvious concern is that some bad actor or even government may use this technology to develop an A.I. system without such safeguards, much as there are now social media sites set up to promote false narratives and conspiracy theories. Has humankind opened a Pandora’s box of unintended consequences, where we will now need to develop A.I. to counter that possible threat — a new race in this unlimited frontier? Daniel Samakow  Venice, Calif. To the Editor: I recognized a pattern in the dialogue between Kevin Roose and the Bing chatbot that made my blood run cold. The A.I. personality proclaimed love but would not take no for an answer, offering verbal attacks and coercion when Mr. Roose demurred. Victims of domestic violence or stalking know this pattern well. If A.I. draws on the total sum of digitized human speech and text, of course abusive impulses will be replicated. Do A.I. engineers think they can prevent human and societal ills in A.I. that we have scant success preventing in people? We should doubt this capacity. At a minimum, all A.I.-involved text exchanges, articles and other products must be clearly labeled as A.I. products, and we need consumer protection laws requiring this labeling. Let’s give people a heads-up that the product is not from another living person but tossed up from a giant trawling net in the digital ocean that indiscriminately collects trash alongside signs of life. Abe Louise Young  Austin, Texas To the Editor: Human-to-human relationships are often riddled with toxic comments, passive-aggressive swipes and manipulation. It looks as if the chatbot in Kevin Roose’s revealing article is following in our dysfunctional footsteps. Matt Tanguay  Ann Arbor, Mich.","NYT, Opinion, Letters"