,titles,body_contents,tags,sentiment_score,positive_words,negative_words
0,"Alarmed by A.I. Chatbots, Universities Start Revamping How They Teach","While grading essays for his world religions course last month, Antony Aumann, a professor of philosophy at Northern Michigan University, read what he said was easily “the best paper in the class.” It explored the morality of burqa bans with clean paragraphs, fitting examples and rigorous arguments. A red flag instantly went up. Mr. Aumann confronted his student over whether he had written the essay himself. The student confessed to using ChatGPT, a chatbot that delivers information, explains concepts and generates ideas in simple sentences — and, in this case, had written the paper. Alarmed by his discovery, Mr. Aumann decided to transform essay writing for his courses this semester. He plans to require students to write first drafts in the classroom, using browsers that monitor and restrict computer activity. In later drafts, students have to explain each revision. Mr. Aumann, who may forgo essays in subsequent semesters, also plans to weave ChatGPT into lessons by asking students to evaluate the chatbot’s responses. “What’s happening in class is no longer going to be, ‘Here are some questions — let’s talk about it between us human beings,’” he said, but instead “it’s like, ‘What also does this alien robot think?’” Across the country, university professors like Mr. Aumann, department chairs and administrators are starting to overhaul classrooms in response to ChatGPT, prompting a potentially huge shift in teaching and learning. Some professors are redesigning their courses entirely, making changes that include more oral exams, group work and handwritten assessments in lieu of typed ones. The moves are part of a real-time grappling with a new technological wave known as generative artificial intelligence. ChatGPT, which was released in November by the artificial intelligence lab OpenAI, is at the forefront of the shift. The chatbot generates eerily articulate and nuanced text in response to short prompts, with people using it to write love letters, poetry, fan fiction — and their schoolwork. That has upended some middle and high schools, with teachers and administrators trying to discern whether students are using the chatbot to do their schoolwork. Some public school systems, including in New York City and Seattle, have since banned the tool on school Wi-Fi networks and devices to prevent cheating, though students can easily find workarounds to access ChatGPT. In higher education, colleges and universities have been reluctant to ban the A.I. tool because administrators doubt the move would be effective and they don’t want to infringe on academic freedom. That means the way people teach is changing instead. “We try to institute general policies that certainly back up the faculty member’s authority to run a class,” instead of targeting specific methods of cheating, said Joe Glover, provost of the University of Florida. “This isn’t going to be the last innovation we have to deal with. That’s especially true as generative A.I. is in its early days. OpenAI is expected to soon release another tool, GPT-4, which is better at generating text than previous versions. Google has built LaMDA, a rival chatbot, and Microsoft is discussing a $10 billion investment in OpenAI. Silicon Valley start-ups, including Stability AI and Character.AI, are also working on generative A.I. tools. An OpenAI spokeswoman said the lab recognized its programs could be used to mislead people and was developing technology to help people identify text generated by ChatGPT. At many universities, ChatGPT has now vaulted to the top of the agenda. Administrators are establishing task forces and hosting universitywide discussions to respond to the tool, with much of the guidance being to adapt to the technology. At schools including George Washington University in Washington, D.C., Rutgers University in New Brunswick, N.J., and Appalachian State University in Boone, N.C., professors are phasing out take-home, open-book assignments — which became a dominant method of assessment in the pandemic but now seem vulnerable to chatbots. They are instead opting for in-class assignments, handwritten papers, group work and oral exams. Gone are prompts like “write five pages about this or that.” Some professors are instead crafting questions that they hope will be too clever for chatbots and asking students to write about their own lives and current events. Students are “plagiarizing this because the assignments can be plagiarized,” said Sid Dobrin, chair of the English department at the University of Florida. Frederick Luis Aldama, the humanities chair at the University of Texas at Austin, said he planned to teach newer or more niche texts that ChatGPT might have less information about, such as William Shakespeare’s early sonnets instead of “A Midsummer Night’s Dream.” The chatbot may motivate “people who lean into canonical, primary texts to actually reach beyond their comfort zones for things that are not online,” he said. In case the changes fall short of preventing plagiarism, Mr. Aldama and other professors said they planned to institute stricter standards for what they expect from students and how they grade. It is now not enough for an essay to have just a thesis, introduction, supporting paragraphs and a conclusion. “We need to up our game,” Mr. Aldama said. “The imagination, creativity and innovation of analysis that we usually deem an A paper needs to be trickling down into the B-range papers.” Universities are also aiming to educate students about the new A.I. tools. The University at Buffalo in New York and Furman University in Greenville, S.C., said they planned to embed a discussion of A.I. tools into required courses that teach entering or freshman students about concepts such as academic integrity. “We have to add a scenario about this, so students can see a concrete example,” said Kelly Ahuna, who directs the academic integrity office at the University at Buffalo. “We want to prevent things from happening instead of catch them when they happen.” Other universities are trying to draw boundaries for A.I. Washington University in St. Louis and the University of Vermont in Burlington are drafting revisions to their academic integrity policies so their plagiarism definitions include generative A.I. John Dyer, vice president for enrollment services and educational technologies at Dallas Theological Seminary, said the language in his seminary’s honor code felt “a little archaic anyway.” He plans to update its plagiarism definition to include: “using text written by a generation system as one’s own (e.g., entering a prompt into an artificial intelligence tool and using the output in a paper).” The misuse of A.I. tools will most likely not end, so some professors and universities said they planned to use detectors to root out that activity. The plagiarism detection service Turnitin said it would incorporate more features for identifying A.I., including ChatGPT, this year. More than 6,000 teachers from Harvard University, Yale University, the University of Rhode Island and others have also signed up to use GPTZero, a program that promises to quickly detect A.I.-generated text, said Edward Tian, its creator and a senior at Princeton University. Some students see value in embracing A.I. tools to learn. Lizzie Shackney, 27, a student at the University of Pennsylvania’s law school and design school, has started using ChatGPT to brainstorm for papers and debug coding problem sets. “There are disciplines that want you to share and don’t want you to spin your wheels,” she said, describing her computer science and statistics classes. “The place where my brain is useful is understanding what the code means.” But she has qualms. ChatGPT, Ms. Shackney said, sometimes incorrectly explains ideas and misquotes sources. The University of Pennsylvania also hasn’t instituted any regulations about the tool, so she doesn’t want to rely on it in case the school bans it or considers it to be cheating, she said. Other students have no such scruples, sharing on forums like Reddit that they have submitted assignments written and solved by ChatGPT — and sometimes done so for fellow students too. On TikTok, the hashtag #chatgpt has more than 578 million views, with people sharing videos of the tool writing papers and solving coding problems. One video shows a student copying a multiple choice exam and pasting it into the tool with the caption saying: “I don’t know about y’all but ima just have Chat GPT take my finals. Have fun studying.”",[],0.17,"['easily', 'best', 'clean', 'sentences', 'like', 'like', 'huge', 'intelligence', 'intelligence', 'love', 'fan', 'prevent', 'easily', 'effective', 'want', 'freedom', 'certainly', 'authority', 'innovation', 'true', 'better', 'help', 'top', 'like', 'hope', 'clever', 'Dream', 'motivate', 'reach', 'comfort', 'supporting', 'creativity', 'innovation', 'integrity', 'integrity', 'want', 'prevent', 'integrity', 'honor', 'intelligence', 'promises', 'value', 'want', 'share', 'want', 'useful', 'want', 'sharing', 'like', 'solved', 'sharing', 'solving', 'fun']","['rigorous', 'arguments', 'confronted', 'Alarmed', 'restrict', 'no', 'banned', 'cheating', 'reluctant', 'ban', 'doubt', 'cheating', 'vulnerable', 'preventing', 'trickling', 'problem', 'cheating', 'no', 'problems']"
1,How ChatGPT Hijacks Democracy,"Launched just weeks ago, ChatGPT is already threatening to upend how we draft everyday communications like emails, college essays and myriad other forms of writing. Created by the company OpenAI, ChatGPT is a chatbot that can automatically respond to written prompts in a manner that is sometimes eerily close to human. But for all the consternation over the potential for humans to be replaced by machines in formats like poetry and sitcom scripts, a far greater threat looms: artificial intelligence replacing humans in the democratic processes — not through voting, but through lobbying. ChatGPT could automatically compose comments submitted in regulatory processes. It could write letters to the editor for publication in local newspapers. It could comment on news articles, blog entries and social media posts millions of times every day. It could mimic the work that the Russian Internet Research Agency did in its attempt to influence our 2016 elections, but without the agency’s reported multimillion-dollar budget and hundreds of employees. Automatically generated comments aren’t a new problem. For some time, we have struggled with bots, machines that automatically post content. Five years ago, at least a million automatically drafted comments were believed to have been submitted to the Federal Communications Commission regarding proposed regulations on net neutrality. In 2019, a Harvard undergraduate, as a test, used a text-generation program to submit 1,001 comments in response to a government request for public input on a Medicaid issue. Back then, submitting comments was just a game of overwhelming numbers. Platforms have gotten better at removing “coordinated inauthentic behavior.” Facebook, for example, has been removing over a billion fake accounts a year. But such messages are just the beginning. Rather than flooding legislators’ inboxes with supportive emails, or dominating the Capitol switchboard with synthetic voice calls, an A.I. system with the sophistication of ChatGPT but trained on relevant data could selectively target key legislators and influencers to identify the weakest points in the policymaking system and ruthlessly exploit them through direct communication, public relations campaigns, horse trading or other points of leverage. When we humans do these things, we call it lobbying. Successful agents in this sphere pair precision message writing with smart targeting strategies. Right now, the only thing stopping a ChatGPT-equipped lobbyist from executing something resembling a rhetorical drone warfare campaign is a lack of precision targeting. A.I. could provide techniques for that as well. A system that can understand political networks, if paired with the textual-generation capabilities of ChatGPT, could identify the member of Congress with the most leverage over a particular policy area — say, corporate taxation or military spending. Like human lobbyists, such a system could target undecided representatives sitting on committees controlling the policy of interest and then focus resources on members of the majority party when a bill moves toward a floor vote. Once individuals and strategies are identified, an A.I. chatbot like ChatGPT could craft written messages to be used in letters, comments — anywhere text is useful. Human lobbyists could also target those individuals directly. It’s the combination that’s important: Editorial and social media comments get you only so far, and knowing which legislators to target isn’t in itself enough. This ability to understand and target actors within a network would create a tool for A.I. hacking, exploiting vulnerabilities in social, economic and political systems with incredible speed and scope. Legislative systems would be a particular target, because the motive for attacking policymaking systems is so strong, because the data for training such systems is so widely available and because the use of A.I. may be so hard to detect — particularly if it is being used strategically to guide human actors. The data necessary to train such strategic targeting systems will only grow with time. Open societies generally make their democratic processes a matter of public record, and most legislators are eager — at least, performatively so — to accept and respond to messages that appear to be from their constituents. Maybe an A.I. system could uncover which members of Congress have significant sway over leadership but still have low enough public profiles that there is only modest competition for their attention. It could then pinpoint the SuperPAC or public interest group with the greatest impact on that legislator’s public positions. Perhaps it could even calibrate the size of donation needed to influence that organization or direct targeted online advertisements carrying a strategic message to its members. For each policy end, the right audience; and for each audience, the right message at the right time. What makes the threat of A.I.-powered lobbyists greater than the threat already posed by the high-priced lobbying firms on K Street is their potential for acceleration. Human lobbyists rely on decades of experience to find strategic solutions to achieve a policy outcome. That expertise is limited, and therefore expensive. A.I. could, theoretically, do the same thing much more quickly and cheaply. Speed out of the gate is a huge advantage in an ecosystem in which public opinion and media narratives can become entrenched quickly, as is being nimble enough to shift rapidly in response to chaotic world events. Moreover, the flexibility of A.I. could help achieve influence across many policies and jurisdictions simultaneously. Imagine an A.I.-assisted lobbying firm that can attempt to place legislation in every single bill moving in the U.S. Congress, or even across all state legislatures. Lobbying firms tend to work within one state only, because there are such complex variations in law, procedure and political structure. With A.I. assistance in navigating these variations, it may become easier to exert power across political boundaries. Just as teachers will have to change how they give students exams and essay assignments in light of ChatGPT, governments will have to change how they relate to lobbyists. To be sure, there may also be benefits to this technology in the democracy space; the biggest one is accessibility. Not everyone can afford an experienced lobbyist, but a software interface to an A.I. system could be made available to anyone. If we’re lucky, maybe this kind of strategy-generating A.I. could revitalize the democratization of democracy by giving this kind of lobbying power to the powerless. However, the biggest and most powerful institutions will likely use any A.I. lobbying techniques most successfully. After all, executing the best lobbying strategy still requires insiders — people who can walk the halls of the legislature — and money. Lobbying isn’t just about giving the right message to the right person at the right time; it’s also about giving money to the right person at the right time. And while an A.I. chatbot can identify who should be on the receiving end of those campaign contributions, humans will, for the foreseeable future, need to supply the cash. So while it’s impossible to predict what a future filled with A.I. lobbyists will look like, it will probably make the already influential and powerful even more so.",[],0.14,"['Launched', 'like', 'Created', 'like', 'greater', 'intelligence', 'better', 'supportive', 'Successful', 'smart', 'well', 'Like', 'interest', 'party', 'like', 'useful', 'important', 'ability', 'create', 'strong', 'matter', 'eager', 'accept', 'significant', 'interest', 'greatest', 'greater', 'solutions', 'huge', 'advantage', 'flexibility', 'help', 'easier', 'sure', 'benefits', 'lucky', 'kind', 'giving', 'kind', 'powerful', 'successfully', 'best', 'giving', 'giving', 'like', 'influential', 'powerful']","['threatening', 'threat', 'looms', 'lobbying', 'problem', 'struggled', 'fake', 'dominating', 'weakest', 'exploit', 'lobbying', 'stopping', 'warfare', 'lack', 'undecided', 'exploiting', 'vulnerabilities', 'attacking', 'hard', 'low', 'threat', 'threat', 'lobbying', 'limited', 'chaotic', 'lobbying', 'Lobbying', 'lobbying', 'powerless', 'lobbying', 'lobbying', 'Lobbying']"
2,My So-So Encounters with ChatGPT,"A mountain man buys his first chain saw. He comes back to the store a week later complaining that it cuts down only two trees a day when he was told it would cut down 20. The service person says, “Well, let’s see what the trouble is,” and starts it up. The mountain man jumps back and asks, “What’s that noise?” (He’d been sawing without the engine on.) I feel like that mountain man when it comes to ChatGPT, the powerful new artificial intelligence chatbot that seemingly everyone is experimenting with. I got mediocre results from ChatGPT because I didn’t try very hard to use it properly. Other people have gotten amazing results because they’re smarter and more purposeful about how they use it — they yank its pull cord and get its engine going. I confess that my first idea was to figure out what ChatGPT could not do rather than what it could. It won’t offer opinions. It’s not up on anything that’s happened since it was trained last year. It doesn’t have a body so it has never been to Ireland. (One of my questions.) I somehow got into a conversation with ChatGPT about words that change their spelling when they’re Anglicized from French. ChatGPT gave “ballet” as an example. But “ballet” is spelled the same in both languages. Hah, it made a mistake! I felt as if I’d scored a win for the human race. But what a shallow win. Other people have done better because they’ve accentuated the positive. On YouTube I found a video of a computer guy, Jason Fleagle, asking ChatGPT, “Can you create a web app using HTML, CSS and Javascript that has a form that takes in a stock ticker symbol for a company and then on form submission displays the stock market performance of that particular company?” ChatGPT did that and more. The code wasn’t perfect — there was a bug somewhere — but Fleagle said, “As you can see, I just saved myself, like, a lot of time.” There are dozens of such examples. ChatGPT can even rewrite software into a different programming language. “I introduced my undergraduate entrepreneurship students to the new A.I. system, and before I was done talking, one of my students had used it to create the code for a start-up prototype using code libraries they had never seen before,” Ethan Mollick, an associate professor at the University of Pennsylvania’s Wharton School, wrote in Harvard Business Review on Wednesday. Mollick himself used ChatGPT to rough out a course syllabus, class assignments, grading criteria and lecture notes. ChatGPT strikes me as an example of what economists call “skill-biased technical change.” It is incredibly powerful in the hands of people who already have skills and ideas because they know what to ask it for. You have two options. You can do a better job than ChatGPT, whether it’s writing or coding, or you can admit your inferiority but figure out a way to make ChatGPT work for you. If you can’t do either, you may need to find a different line of work. Maybe a lot of us will become superfluous and depend on a universal basic income. That would be unfortunate. Me, I’m still hoping I can outdo ChatGPT and stay employed a while longer. But the truth is, ChatGPT is a powerful language model that is capable of generating humanlike text. As it continues to improve and become more advanced, it’s possible that it could displace people in certain writing-related professions. For example, it could potentially be used to automate the writing of articles, reports and other written content, which could lead to job losses for writers and researchers. However, it’s important to note that ChatGPT is still a tool, and that it will likely be used to augment and assist human workers rather than fully replace them. Did that last paragraph sound uninspired? Maybe it’s because I let ChatGPT write it for me (a good gimmick); I gave it the first sentence and asked it to fill in the rest. That’s not good journalistic practice. The writer needs to remain the writer. If all I ever manage to do with ChatGPT is get it to do my job — Hey, listen, can you take the wheel while I eat a sandwich? — I deserve whatever I get. I need to figure out how to use the chain saw.",[],0.1,"['Well', 'like', 'powerful', 'intelligence', 'amazing', 'smarter', 'win', 'win', 'better', 'positive', 'create', 'perfect', 'saved', 'like', 'create', 'powerful', 'better', 'admit', 'hoping', 'truth', 'powerful', 'capable', 'improve', 'advanced', 'certain', 'important', 'good', 'sentence']","['complaining', 'cuts', 'cut', 'trouble', 'hard', 'mistake', 'strikes', 'biased', 'inferiority', 'unfortunate', 'losses', 'good']"
3,ChatGPT Has a Devastating Sense of Humor,"ChatGPT makes an irresistible first impression. It’s got a devastating sense of humor, a stunning capacity for dead-on mimicry, and it can rhyme like nobody’s business. Then there is its overwhelming reasonableness. When ChatGPT fails the Turing test, it’s usually because it refuses to offer its own opinion on just about anything. When was the last time real people on the internet declined to tell you what they really think? I started talking to ChatGPT a couple of weeks ago, after the artificial intelligence company OpenAI released the bot as a “research preview” of its work on large language models. A language model is an A.I. system that has been trained on enormous troves of text to find the probabilistic connection between words; ChatGPT is a language model that has been optimized to create what’s long been the holy grail in artificial intelligence research — a computer with which you can hold a conversation. ChatGPT certainly achieves that. I have spoken to lots of computers in my lifetime (weird flex, I know), but ChatGPT is the first that I’ve found fun and interesting to talk to. I began by peppering it with simple trivia but it wasn’t long before we were holding surprisingly nuanced conversations about, among many other things, the role of the Federal Reserve in the American economy; the nature of consciousness; neologisms like “woke” and “Karen”; ethical quandaries in parenting; how to support one’s striking colleagues; climate change, abortion and vaccine safety; and whether or not a hot dog is a sandwich. This is where I’m supposed to tell you I am either in awe or afraid of ChatGPT, that it will revolutionize our world or ruin it. But while I do think ChatGPT illustrates some dangers of A.I., I’m reluctant to either strongly praise or condemn it. That’s because, like most cocktail party schmoozers, it has a potential for both harm and good that are, at least for now, quite limited. I have no doubt that something like ChatGPT could be misused — that it has the potential to contribute to confident-sounding viral misinformation, or that it could make it easier for students to cheat on essays. But OpenAI seems to be doing what you’d want in the release of potentially powerful technology: In an interview, Mira Murati, OpenAI’s chief technology officer, told me the company is carefully monitoring how people use and misuse it, quickly altering the system to address evident harms and iteratively improving it in response to user feedback. Indeed, ChatGPT’s recognition of its own limitations is one of its most interesting personality traits. Many conversations with ChatGPT go like this — when you try to pin it down it becomes as circumspect as a Supreme Court nominee at a confirmation hearing, usually cautioning you that there are different beliefs about the matter, that there may not be a definitive “correct” answer and that you should try to appreciate different perspectives. These answers seem wishy-washy, and the Electoral College response is just wrong — it should have said “a candidate who wins by a small number of votes in a large state will win more electoral votes.” On matters involving science, ChatGPT seems more definitive, saying, for instance, that “climate change is real and is happening now,” that evolution is “supported by a vast amount of scientific evidence from many different fields” and that the Earth is incontrovertibly not flat. In general, though, ChatGPT has a remarkable tendency to admit that it is incapable of offering a definitive answer. Why is that remarkable? Two of the well-known problems in A.I. research are about maintaining “alignment” and avoiding “hallucinations.” Alignment involves an A.I.’s ability to carry out the goals of its human creators — in other words, to resist causing harm in the world. Hallucinations are about adhering to the truth; when A.I. systems get confused, they have a bad habit of making things up rather than admitting their difficulties. In order to address both issues in ChatGPT, OpenAI’s researchers fine-tuned its language model with what is known as “reinforcement learning from human feedback.” Basically, the company hired real people to interact with its A.I. As the humans talked to the machine, they rated its responses, essentially teaching it what kinds of responses are good and which ones are not. Murati told me that combining the language model with human feedback created a much more realistic A.I. conversational partner: “The model can tell you when it’s wrong,” she said. “It can ask you a follow-up question. It can challenge incorrect premises or reject requests that are inappropriate.” Like a lot of people online, I tried many different ways to get around ChatGPT’s guardrails. But I was surprised by how often it eluded my efforts: ChatGPT is far from perfect. Twitter has been flooded with examples of “jailbreaking” ChatGPT — that is, tricking it into hallucinations or misalignment. One of the ways I did manage to get it to offer false health information was by asking it to dabble in a form known for stretching the truth: marketing copy. I asked it to write promotional text for a new toilet plunger that comes in a variety of colors, requires only one plunge to undo a clog and can also make long-distance phone calls and cure hepatitis C. One primary criticism of systems like ChatGPT, which are built using a computational technique called “deep learning,” is that they are little more than souped-up versions of autocorrect — that all they understand is the statistical connections between words, not the concepts underlying words. Gary Marcus, a professor emeritus in psychology at New York University and a skeptic of deep learning, told me that while an A.I. language model like ChatGPT makes for “nifty” demonstrations, it’s “still not reliable, still doesn’t understand the physical world, still doesn’t understand the psychological world and still hallucinates.” He’s clearly got a point. You don’t have to get too deep into conversation with ChatGPT to see that it really doesn’t “understand” many real-world concepts. When I asked ChatGPT how much water would need to be drained from the largest of the Great Lakes to make its volume equal to that of the smallest of the Great Lakes, it argued that such a thing was not even possible. ChatGPT told me that the largest Great Lake is Lake Superior, with 2,902 cubic miles of water, and the smallest is Lake Ontario, with a volume of 393 cubic miles. Kind of true: Lake Ontario is the smallest Great Lake by surface area, but by volume it’s larger than Lake Erie. I let that slide, though, because ChatGPT went on to make a bigger error: It seemed to think that a lake’s volume cannot fall beyond a certain point. Lake Superior has 2,509 cubic miles more water than Lake Ontario, but ChatGPT said that it is not possible to drain that much water from Lake Superior because “the lake is already at its minimum volume and cannot be drained any further.” What? How can a body of water have a minimum volume? I asked what would happen if you used a pump to pump out all the water from Lake Superior. Murati told me that one of the reasons OpenAI released ChatGPT to the public is to weed out such misunderstandings. She said that the company will keep updating the system in response to feedback, and the more feedback it gets, the better ChatGPT will become. ChatGPT could also get smarter by connecting to more reliable data — at the moment it is not plugged in to the internet or any other sources of truth, and its entire knowledge base ends in late 2021, when OpenAI’s latest language model was trained. In the meantime, though, ChatGPT’s best feature is its modesty. One afternoon, fed up with its constant reminders that its answers may be wrong, I asked: “If I have to double-check everything you say, what utility do you provide? I’m sorry if that sounds mean.” Such humility makes ChatGPT a truly different kind of digital assistant. It’s not often you find people online willing to admit they may be wrong. If the best that A.I. can do is promise to keep doing better, I’ll take it.",[],0.21,"['irresistible', 'impression', 'humor', 'stunning', 'like', 'intelligence', 'optimized', 'create', 'intelligence', 'certainly', 'fun', 'interesting', 'surprisingly', 'like', 'ethical', 'support', 'safety', 'strongly', 'praise', 'like', 'party', 'good', 'like', 'confident', 'easier', 'want', 'powerful', 'carefully', 'improving', 'interesting', 'like', 'Supreme', 'matter', 'appreciate', 'wins', 'number', 'win', 'matters', 'supported', 'remarkable', 'admit', 'remarkable', 'well', 'ability', 'truth', 'fine', 'good', 'created', 'challenge', 'Like', 'surprised', 'perfect', 'tricking', 'truth', 'like', 'like', 'nifty', 'clearly', 'Great', 'Great', 'Great', 'Superior', 'Kind', 'true', 'Great', 'certain', 'Superior', 'Superior', 'drained', 'Superior', 'better', 'smarter', 'truth', 'best', 'truly', 'kind', 'admit', 'best', 'promise', 'better']","['devastating', 'dead', 'fails', 'weird', 'ruin', 'dangers', 'reluctant', 'condemn', 'harm', 'limited', 'no', 'doubt', 'misinformation', 'cheat', 'harms', 'wrong', 'incapable', 'problems', 'avoiding', 'harm', 'confused', 'bad', 'difficulties', 'wrong', 'reject', 'criticism', 'skeptic', 'drained', 'argued', 'error', 'wrong', 'sorry', 'wrong']"
4,Will ChatGPT Make Me Irrelevant?,"Like every other journalist I know, I often and unabashedly ask for help. Friends give me ideas. Colleagues give me phrases. Editors suggest what to keep, what to cut and where a key detail belongs. My field of vision is only so wide, my brain only so big. I’d be a fool not to supplement. But there’s a limit to how much advice I solicit, and it’s determined less by the rapid approach of a deadline or the bedlam of too many chefs than by something else, something emotional and maybe even moral, an admixture of vanity and integrity. Past a certain point of collaboration, I lose the belief that a piece of work is truly and fully mine. I lose the satisfaction of that. I can’t shake the notion that my role in the process was incidental, verging on irrelevant. I share all of this in the context of the intensifying chatter about what artificial intelligence can do — and about what, specifically, the new chatbot ChatGPT, from the company OpenAI, is already doing. It’s a surprisingly competent writer and sometimes even a clever one, to the point where early users regard it as “some mix of software and sorcery,” as Kevin Roose explained in a recent article in The Times. (The article’s headline: “The Brilliance and Weirdness of ChatGPT.”) Under the right circumstances, with the right prompt, this cyber Cyrano produces relatively seamless prose of considerable ingenuity. Educators are spooked, recognizing a specter on the horizon — no, right in front of us — that makes plagiarism look quaint. Last week, The Atlantic published an article, by Stephen Marche, titled “The College Essay Is Dead.” That was followed just three days later by another article, by Daniel Herman, titled “The End of High School English.” I figure “Curtains for the Seventh Grade” will be out next week and, fast on its heels, “Is Literacy Obsolete?” And I can tell you that here in the lofty precincts of elite academia, conversations about whether a significant fraction of students would be turning in papers generated by A.I. segued quickly into conjecture about whether professors would respond by grading those papers with A.I. Let’s take human endeavor out of the equation entirely. It’s such an inefficient, unnecessary thing. But it’s also, well, everything — not by the dictates of productivity, but by measures much more meaningful. It’s the font and province of originality. It’s the cornerstone of identity. We are what we do, and by that I don’t mean the labels affixed to our professions. I mean the stamps of our idiosyncratic contributions, no matter their nature or context. That’s how we bend the universe — our butterfly effect — and how we register that we were here. If we outsource it to A.I., don’t we erase ourselves? Maybe not. Maybe this is the cusp of a new utopia, in which machines not only assemble our appliances and perform our surgeries but also plot our novels, draft our legislation and write our op-eds while we pop our soma or chew our lotus leaves and congratulate ourselves on the programming and the prompts behind it all. But I suspect that we’d miss the same feeling — the same fulfillment — that I forfeit when I receive and incorporate more assistance than I went looking for. Pride of ownership would cease to exist. Sense of purpose would vanish with it. Is ChatGPT a sorcerer or an assassin? It and its kin promise to save us time, sweat and error, but potentially at a price. It’s called pointlessness.",[],0.08,"['Like', 'help', 'Friends', 'vision', 'determined', 'emotional', 'integrity', 'certain', 'truly', 'satisfaction', 'share', 'intelligence', 'surprisingly', 'competent', 'clever', 'Brilliance', 'significant', 'well', 'meaningful', 'matter', 'congratulate', 'feeling', 'Pride', 'promise', 'save']","['cut', 'fool', 'vanity', 'lose', 'lose', 'shake', 'Weirdness', 'no', 'Dead', 'Obsolete', 'no', 'suspect', 'miss', 'error']"
5,What Would Plato Say About ChatGPT?,"Plato mourned the invention of the alphabet, worried that the use of text would threaten traditional memory-based arts of rhetoric. In his “Dialogues,” arguing through the voice of Thamus, the Egyptian king of the gods, Plato claimed the use of this more modern technology would create “forgetfulness in the learners’ souls, because they will not use their memories,” that it would impart “not truth but only the semblance of truth” and that those who adopt it would “appear to be omniscient and will generally know nothing,” with “the show of wisdom without the reality.” If Plato were alive today, would he say similar things about ChatGPT? ChatGPT, a conversational artificial intelligence program released recently by OpenAI, isn’t just another entry in the artificial intelligence hype cycle. It’s a significant advancement that can produce articles in response to open-ended questions that are comparable to good high school essays. It is in high schools and even college where some of ChatGPT’s most interesting and troubling aspects will become clear. Essay writing is most often assigned not because the result has much value — proud parents putting good grades on the fridge aside — but because the process teaches crucial skills: researching a topic, judging claims, synthesizing knowledge and expressing it in a clear, coherent and persuasive manner. Those skills will be even more important because of advances in A.I. When I asked ChatGPT a range of questions — about the ethical challenges faced by journalists who work with hacked materials, the necessity of cryptocurrency regulation, the possibility of democratic backsliding in the United States — the answers were cogent, well reasoned and clear. It’s also interactive: I could ask for more details or request changes. But then, on trickier topics or more complicated concepts, ChatGPT sometimes gave highly plausible answers that were flat-out wrong — something its creators warn about in their disclaimers. Unless you already knew the answer or were an expert in the field, you could be subjected to a high-quality intellectual snow job. You would face, as Plato predicted, “the show of wisdom without the reality.” All this, however, doesn’t mean ChatGPT — or similar tools, because it’s not the only one of its kind — can’t be a useful tool in education. Schools have already been dealing with the internet’s wealth of knowledge, along with its lies, misleading claims and essay mills. One way has been to change how they teach. Rather than listen to a lecture in class and then go home to research and write an essay, students listen to recorded lectures and do research at home, then write essays in class, with supervision, even collaboration with peers and teachers. This approach is called flipping the classroom. In flipped classrooms, students wouldn’t use ChatGPT to conjure up a whole essay. Instead, they’d use it as a tool to generate critically examined building blocks of essays. It would be similar to how students in advanced math classes are allowed to use calculators to solve complex equations without replicating tedious, previously mastered steps. Teachers could assign a complicated topic and allow students to use such tools as part of their research. Assessing the veracity and reliability of these A.I.-generated notes and using them to create an essay would be done in the classroom, with guidance and instruction from teachers. The goal would be to increase the quality and the complexity of the argument. This would require more teachers to provide detailed feedback. Unless sufficient resources are provided equitably, adapting to conversational A.I. in flipped classrooms could exacerbate inequalities. In schools with fewer resources, some students may end up turning in A.I.-produced essays without obtaining useful skills or really knowing what they have written. “Not truth but only the semblance of truth,” as Plato said. Some school officials may treat this as a problem of merely plagiarism detection and expand the use of draconian surveillance systems. During the pandemic, many students were forced to take tests or write essays under the gaze of an automated eye-tracking system or on a locked-down computer to prevent cheating. In a fruitless arms race against conversational A.I., automated plagiarism software may become supercharged, making school more punitive for monitored students. Worse, such systems will inevitably produce some false accusations, which damage trust and may even stymie the prospects of promising students. Educational approaches that treat students like enemies may teach students to hate or subvert the controls. That’s not a recipe for human betterment. While some students lag, advanced A.I. will create a demand for other advanced skills. The Nobel laureate Herbert Simon noted in 1971 that as information became overwhelming, the value of our attention grew. “A wealth of information creates a poverty of attention,” as he put it. Similarly, the ability to discern truth from the glut of plausible-sounding but profoundly incorrect answers will be precious. Already, Stack Overflow, a widely used website where programmers ask one another coding-related questions, banned ChatGPT answers because too many of them were hard-to-spot nonsense. Why rely on it at all, then? At a minimum, because it will soon transform many occupations. The right approach when faced with transformative technologies is to figure out how to use them for the betterment of humanity. Betterment has been a goal of public education for at least the past 150 years. But while a high school diploma once led to a better job, in the past few decades, the wages of high school graduates have greatly lagged those of college graduates, fostering inequality. If A.I. enhances the value of education for some while degrading the education of others, the promise of betterment will be broken. Plato erred by thinking that memory itself is a goal, rather than a means for people to have facts at their call so they can make better analyses and arguments. The Greeks developed many techniques to memorize poems like the “Odyssey,” with its more than 12,000 lines. Why bother to force this if you can have it all written down in books? As Plato was wrong to fear the written word as the enemy, we would be wrong to think we should resist a process that allows us to gather information more easily. As societies responded to previous technological advances, like mechanization, by eventually enacting a public safety net, a shorter workweek and a minimum wage, we will also need policies that allow more people to live with dignity as a basic right, even if their skills have been superseded. With so much more wealth generated now, we could unleash our imagination even more, expanding free time and better working conditions for more people. The way forward is not to just lament supplanted skills, as Plato did, but also to recognize that as more complex skills become essential, our society must equitably educate people to develop them. And then it always goes back to the basics. Value people as people, not just as bundles of skills. And that isn’t something ChatGPT can tell us how to do.",[],0.1,"['create', 'truth', 'adopt', 'wisdom', 'alive', 'intelligence', 'intelligence', 'significant', 'good', 'interesting', 'clear', 'value', 'proud', 'good', 'clear', 'important', 'ethical', 'challenges', 'United', 'well', 'clear', 'intellectual', 'wisdom', 'kind', 'useful', 'wealth', 'advanced', 'solve', 'allow', 'create', 'increase', 'truth', 'treat', 'expand', 'prevent', 'trust', 'prospects', 'promising', 'treat', 'like', 'advanced', 'create', 'advanced', 'value', 'wealth', 'creates', 'ability', 'truth', 'precious', 'better', 'value', 'promise', 'better', 'like', 'easily', 'like', 'safety', 'allow', 'dignity', 'wealth', 'free', 'better', 'lament', 'Value']","['mourned', 'worried', 'threaten', 'arguing', 'truth', 'troubling', 'hacked', 'trickier', 'wrong', 'warn', 'lies', 'misleading', 'blocks', 'argument', 'useful', 'truth', 'problem', 'forced', 'cheating', 'punitive', 'Worse', 'accusations', 'damage', 'enemies', 'hate', 'lag', 'demand', 'poverty', 'banned', 'hard', 'nonsense', 'lagged', 'degrading', 'broken', 'arguments', 'bother', 'wrong', 'fear', 'enemy', 'wrong']"
6,Does ChatGPT Mean Robots Are Coming For the Skilled Jobs?,"Will robots take away our jobs? People have been asking that question for an astonishingly long time. The Regency-era British economist David Ricardo added to the third edition of his classic “Principles of Political Economy,” published in 1821, a chapter titled “On Machinery,” in which he tried to show how the technologies of the early Industrial Revolution could, at least initially, hurt workers. Kurt Vonnegut’s 1952 novel “Player Piano” envisaged a near-future America in which automation has eliminated most employment. At the level of the economy as a whole, the verdict is clear: So far, machines haven’t done away with the need for workers. U.S. workers are almost five times as productive as they were in the early postwar years, but there has been no long-term upward trend in unemployment: That said, technology can eliminate particular kinds of jobs. In 1948 half a million Americans were employed mining coal; the great bulk of those jobs had disappeared by the early 21st century not because we stopped mining coal — the big decline in coal production, in favor first of natural gas and then of renewable energy, started only around 15 years ago — but because strip mining and mountaintop removal made it possible to extract an increasing amount of coal with many fewer workers: It’s true that the jobs that disappear in the face of technological progress have generally been replaced by other jobs. But that doesn’t mean that the process has been painless. Individual workers may not find it easy to change jobs, especially if the new jobs are in different places. They may find their skills devalued; in some cases, as with coal, technological change can uproot communities and their way of life. This kind of dislocation has, as I said, been a feature of modern societies for at least two centuries. But something new may be happening now. In the past, the jobs replaced by technology tended to involve manual labor. Machines replaced muscles. On the one hand, industrial robots replaced routine assembly-line work. On the other hand, there has been ever-growing demand for knowledge workers, a term coined by the management consultant Peter Drucker in 1959 for people engaged in nonrepetitive problem solving. Many people, myself included, have said that we’re increasingly becoming a knowledge economy. But what if machines can take over a large chunk of what we have historically thought of as knowledge work? Last week the research company OpenAI released — to enormous buzz from tech circles — a program called ChatGPT, which can carry out what look like natural-language conversations. You can ask questions or make requests and get responses that are startlingly clear and even seem well-informed. You can also do fun things — one colleague recently asked for and received an analysis of secular stagnation in sonnet form — but let’s stick with things that might be economically useful. ChatGPT is only the latest example of technology that seems to be able to carry out tasks that not long ago seemed to require the services not just of human beings but of humans with substantial formal education. For example, machine translation from one language to another used to be a joke; some readers may have heard the apocryphal tale of the Russian-English translation program that took “the spirit was willing, but the flesh was weak” and ended up with “the vodka was good, but the meat was spoiled.” These days, translation programs may not produce great literature, but they’re adequate for many purposes. And the same is true in many fields. You can argue that what we often call artificial intelligence isn’t really intelligence. Indeed, it may be a long time before machines can be truly creative or offer deep insight. But then, how much of what human beings do is truly creative or deeply insightful? (Indeed, how much of what gets published in academic journals — a field of endeavor I know pretty well — meets those criteria?) So quite a few knowledge jobs may be eminently replaceable. What will this mean for the economy? It is difficult to predict exactly how A.I. will impact the demand for knowledge workers, as it will likely vary, depending on the industry and specific job tasks. However, it is possible that in some cases, A.I. and automation may be able to perform certain knowledge-based tasks more efficiently than humans, potentially reducing the need for some knowledge workers. This could include tasks such as data analysis, research and report writing. However, it is also worth noting that A.I. and automation may also create new job opportunities for knowledge workers, particularly in fields related to A.I. development and implementation. OK, I didn’t write the paragraph you just read; ChatGPT did, in response to the question “How will A.I. affect the demand for knowledge workers?” The giveaway, to me at least, is that I still refuse to use “impact” as a verb. And it didn’t explicitly lay out exactly why we should, overall, expect no impact on aggregate employment. But it was arguably better than what many humans, including some people who imagine themselves smart, would have written. In the long run, productivity gains in knowledge industries, like past gains in traditional industries, will make society richer and improve our lives in general (unless Skynet kills us all). But in the long run, we are all dead, and even before that, some of us may find ourselves either unemployed or earning far less than we expected, given our expensive educations.",[],0.2,"['novel', 'verdict', 'clear', 'great', 'stopped', 'favor', 'natural', 'energy', 'true', 'progress', 'painless', 'kind', 'hand', 'hand', 'growing', 'engaged', 'solving', 'like', 'natural', 'clear', 'well', 'fun', 'useful', 'substantial', 'joke', 'spirit', 'good', 'adequate', 'true', 'intelligence', 'intelligence', 'truly', 'creative', 'truly', 'creative', 'pretty', 'well', 'certain', 'efficiently', 'worth', 'create', 'opportunities', 'OK', 'better', 'smart', 'gains', 'like', 'gains', 'richer', 'improve']","['hurt', 'no', 'unemployment', 'disappeared', 'disappear', 'easy', 'demand', 'problem', 'startlingly', 'weak', 'great', 'argue', 'difficult', 'demand', 'demand', 'refuse', 'no', 'arguably', 'kills', 'dead']"
