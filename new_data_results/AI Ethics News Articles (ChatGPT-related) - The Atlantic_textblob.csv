,titles,body_contents,tags,sentiment_score,positive_words,negative_words
0,ChatGPT Changed Everything. Now Its Follow-Up Is Here.,"Less than four months after releasing ChatGPT, the text-generating AI that seems to have pushed us into a science-fictional age of technology, OpenAI has unveiled a new product called GPT-4. Rumors and hype about this program have circulated for more than a year: Pundits have said that it would be unfathomably powerful, writing 60,000-word books from single prompts and producing videos out of whole cloth. Today’s announcement suggests that GPT-4’s abilities, while impressive, are more modest: It performs better than the previous model on standardized tests and other benchmarks, works across dozens of languages, and can take images as input—meaning that it’s able, for instance, to describe the contents of a photo or a chart. Unlike ChatGPT, this new model is not currently available for public testing (although you can apply or pay for access), so the obtainable information comes from OpenAI’s blog post, and from a New York Times story based on a demonstration. From what we know, relative to other programs, GPT-4 appears to have added 150 points to its SAT score, now a 1410 out of 1600, and jumped from the bottom to the top 10 percent of performers on a simulated bar exam. Despite pronounced fears of AI’s writing, the program’s AP English scores remain in the bottom quintile. And while ChatGPT can handle only text, in one example, GPT-4 accurately answered questions about photographs of computer cables. Image inputs are not publicly available yet, even to those eventually granted access off the waitlist, so it’s not possible to verify OpenAI’s claims. The new GPT-4 model is the latest in a long genealogy—GPT-1, GPT-2, GPT-3, GPT-3.5, InstructGPT, ChatGPT—of what are now known as “large language models,” or LLMs, which are AI programs that learn to predict what words are most likely to follow each other. These models work under a premise that traces its origins to some of the earliest AI research in the 1950s: that a computer that understands and produces language will necessarily be intelligent. That belief underpinned Alan Turing’s famous imitation game, now known as the Turing Test, which judged computer intelligence by how “human” its textual output read. Those early language AI programs involved computer scientists deriving complex, hand-written rules, rather than the deep statistical inferences used today. Precursors to contemporary LLMs date to the early 2000s, when computer scientists began using a type of program inspired by the human brain called a “neural network,” which consists of many interconnected layers of artificial nodes that process huge amounts of training data, to analyze and generate text. The technology has advanced rapidly in recent years thanks to some key breakthroughs, notably programs’ increased attention spans—GPT-4 can make predictions based on not just the previous phrase but many words prior, and weigh the importance of each word differently. Today’s LLMs read books, Wikipedia entries, social-media posts, and countless other sources to find these deep statistical patterns; OpenAI has also started using human researchers to fine-tune its models’ outputs. As a result, GPT-4 and similar programs have a remarkable facility with language, writing short stories and essays and advertising copy and more. Some linguists and cognitive scientists believe that these AI models show a decent grasp of syntax and, at least according to OpenAI, perhaps even a glimmer of understanding or reasoning—although the latter point is very controversial, and formal grammatical fluency remains far off from being able to think. GPT-4 is both the latest milestone in this research on language and also part of a broader explosion of “generative AI,” or programs that are capable of producing images, text, code, music, and videos in response to prompts. If such software lives up to its grand promises, it could redefine human cognition and creativity, much as the internet, writing, or even fire did before. OpenAI frames each new iteration of its LLMs as a step toward the company’s stated mission to create “artificial general intelligence,” or computers that can learn and excel at everything, in a way that “benefits all of humanity.” OpenAI’s CEO, Sam Altman, told the The New York Times that while GPT-4 has not “solved reasoning or intelligence… this is a big step forward from what is already out there.” With the goal of AGI in mind, the organization began as a nonprofit that provided public documentation for much of its code. But it quickly adopted a “capped profit” structure, allowing investors to earn back up to 100 times the money they put in, with all profits exceeding that returning to the nonprofit—ostensibly allowing OpenAI to raise the capital needed to support its research. (Analysts estimate that training a high-end language model costs in “the high-single-digit millions.”) Along with the financial shift, OpenAI also made its code more secret—an approach that critics say makes it difficult to hold the technology accountable for incorrect and harmful output, though the company has said that the opacity guards against “malicious” uses. The company frames any shifts away from its founding values as, at least in theory, compromises that will accelerate arrival at an AI-saturated future that Altman describes as almost Edenic: Robots providing crucial medical advice and assisting underresourced teachers, leaps in drug discovery and basic science, the end of menial labor. But more advanced AI, whether generally intelligent or not, might also leave huge portions of the population jobless, or replace rote work with new, AI-related bureaucratic tasks and higher productivity demands. Email didn’t speed up communication so much as turn each day into an email-answering slog; electronic health records should save doctors time but in fact force them to spend many extra, uncompensated hours updating and conferring with these databases. Regardless of whether this technology is a blessing or a burden for everyday people, those who control it will no doubt reap immense profits. Just as OpenAI has lurched toward commercialization and opacity, already everybody wants in on the AI gold rush. Companies like Snap and Instacart are using OpenAI’s technology to incorporate AI assistants into their services. Earlier this year, Microsoft invested $10 billion in OpenAI and is now incorporating chatbot technology into its Bing search engine. Google followed up by investing a more modest sum in the rival AI start-up Anthropic (recently valued at $4.1 billion) and announcing various AI capacities in Google search, Maps, and other apps. Amazon is incorporating Hugging Face—a popular website that gives easy access to AI tools—into AWS, to compete with Microsoft’s cloud service, Azure. Meta has long had an AI division, and now Mark Zuckerberg is trying to build a specific, generative-AI team from the Metaverse’s pixelated ashes. Start-ups are awash in billions in venture-capital investments. GPT-4 is already powering the new Bing, and could conceivably be integrated into Microsoft Office. In an event announcing the new Bing last month, Microsoft’s CEO said, “The race starts today, and we’re going to move and move fast.” Indeed, GPT-4 is already upon us. Yet as any good text predictor would tell you, that quote should end with “move fast and break things.” Silicon Valley’s rush, whether toward gold or AGI, shouldn’t distract from all the ways these technologies fail, often spectacularly. Even as LLMs are great at producing boilerplate copy, many critics say they fundamentally don’t and perhaps cannot understand the world. They are something like autocomplete on PCP, a drug that gives users a false sense of invincibility and heightened capacities for delusion. These models generate answers with the illusion of omniscience, which means they can easily spread convincing lies and reprehensible hate. While GPT-4 seems to wrinkle that critique with its apparent ability to describe images, its basic function remains really good pattern matching, and it can only output text. Those patterns are sometimes harmful. Language models tend to replicate much of the vile text on the internet, a concern that the lack of transparency in their design and training only heightens. As the University of Washington linguist and prominent AI critic Emily Bender told me via email: “We generally don’t eat food whose ingredients we don’t know or can’t find out.” Precedent would indicate that there’s a lot of junk baked in. Microsoft’s original chatbot, named Tay and released in 2016, became misogynistic and racist, and was quickly discontinued. Last year, Meta’s BlenderBot AI rehashed anti-Semitic conspiracies, and soon after that, the company’s Galactica—a model intended to assist in writing scientific papers—was found to be prejudiced and prone to inventing information (Meta took it down within three days). GPT-2 displayed bias against women, queer people, and other demographic groups; GPT-3 said racist and sexist things; and ChatGPT was accused of making similarly toxic comments. OpenAI tried and failed to fix the problem each time. New Bing, which runs a version of GPT-4, has written its own share of disturbing and offensive text—teaching children ethnic slurs, promoting Nazi slogans, inventing scientific theories. It’s tempting to write the next sentence in this cycle automatically, like a language model—“GPT-4 showed [insert bias here].” Indeed, in its blog post, OpenAI admits that GPT-4 “‘hallucinates’ facts and makes reasoning errors,” hasn’t gotten much better at fact-checking itself, and “can have various biases in its outputs.” Still, as any user of ChatGPT can attest, even the most convincing patterns don’t have perfectly predictable outcomes. A Meta spokesperson wrote over email that more work is needed to address bias and hallucinations—what researchers call the information that AIs invent—in large language models, and that “public research demos like BlenderBot and Galactica are important for building” better chatbots; a Microsoft spokesperson pointed me to a post in which the company described improving Bing through a “virtuous cycle of [user] feedback.” An OpenAI spokesperson pointed me to a blog post on safety, in which the company outlines its approach to preventing misuse. It notes, for example, that testing products “in the wild” and receiving feedback can improve future iterations. In other words, Big AI’s party line is the utilitarian calculus that, even if programs might be dangerous, the only way to find out and improve them is to release them and risk exposing the public to hazard. With researchers paying more and more attention to bias, a future iteration of a language model, GPT-4 or otherwise, could someday break this well-established pattern. But no matter what the new model proves itself capable of, there are still much larger questions to contend with: Whom is the technology for? Whose lives will be disrupted? And if we don’t like the answers, can we do anything to contest them?",[],0.14,"['new', 'more', 'powerful', 'whole', 'impressive', 'more', 'modest', 'better', 'able', 'new', 'new', 'top', 'accurately', 'new', 'latest', 'large', 'most', 'necessarily', 'famous', 'early', 'contemporary', 'early', 'many', 'huge', 'advanced', 'thanks', 'notably', 'many', 'remarkable', 'more', 'decent', 'very', 'far', 'able', 'latest', 'capable', 'grand', 'much', 'new', 'general', 'new', 'much', 'quickly', 'more', 'more', 'advanced', 'generally', 'huge', 'new', 'higher', 'much', 'many', 'wants', 'more', 'modest', 'popular', 'easy', 'new', 'conceivably', 'new', 'fast', 'good', 'fast', 'spectacularly', 'great', 'many', 'easily', 'convincing', 'apparent', 'really', 'much', 'prominent', 'generally', 'original', 'quickly', 'new', 'own', 'much', 'most', 'convincing', 'more', 'large', 'important', 'better', 'wild', 'more', 'more', 'new', 'proves', 'capable']","['less', 'single', 'previous', 'other', 'not', 'other', 'not', 'long', 'other', 'imitation', 'game', 'complex', 'artificial', 'previous', 'other', 'least', 'artificial', 'difficult', 'least', 'everyday', 'other', 'long', 'fail', 'boilerplate', 'false', 'hate', 'down', 'other', 'failed', 'disturbing', 'perfectly', 'other', 'dangerous']"
1,We Programmed ChatGPT Into This Article. It’s Weird.,"ChatGPT, the internet-famous AI text generator, has taken on a new form. Once a website you could visit, it is now a service that you can integrate into software of all kinds, from spreadsheet programs to delivery apps to magazine websites such as this one. Snapchat added ChatGPT to its chat service (it suggested that users might type “Can you write me a haiku about my cheese-obsessed friend Lukas?”), and Instacart plans to add a recipe robot. Many more will follow. They will be weirder than you might think. Instead of one big AI chat app that delivers knowledge or cheese poetry, the ChatGPT service (and others like it) will become an AI confetti bomb that sticks to everything. AI text in your grocery app. AI text in your workplace-compliance courseware. AI text in your HVAC how-to guide. AI text everywhere—even later in this article—thanks to an API. API is one of those three-letter acronyms that computer people throw around. It stands for “application programming interface”: It allows software applications to talk to one another. That’s useful because software often needs to make use of the functionality from other software. An API is like a delivery service that ferries messages between one computer and another. Despite its name, ChatGPT isn’t really a chat service—that’s just the experience that has become most familiar, thanks to the chatbot’s pop-cultural success. “It’s got chat in the name, but it’s really a much more controllable model,” Greg Brockman, OpenAI’s co-founder and president, told me. He said the chat interface offered the company and its users a way to ease into the habit of asking computers to solve problems, and a way to develop a sense of how to solicit better answers to those problems through iteration. But chat is laborious to use and eerie to engage with. “You don’t want to spend your time talking to a robot,” Brockman said. He sees it as “the tip of an iceberg” of possible future uses: a “general-purpose language system.” That means ChatGPT as a service (rather than a website) may mature into a system of plumbing for creating and inserting text into things that have text in them. As a writer for a magazine that’s definitely in the business of creating and inserting text, I wanted to explore how The Atlantic might use the ChatGPT API, and to demonstrate how it might look in context. The first and most obvious idea was to create some kind of chat interface for accessing magazine stories. Talk to The Atlantic, get content. So I started testing some ideas on ChatGPT (the website) to explore how we might integrate ChatGPT (the API). One idea: a simple search engine that would surface Atlantic stories about a requested topic. But when I started testing out that idea, things quickly went awry. I asked ChatGPT to “find me a story in The Atlantic about tacos,” and it obliged, offering a story by my colleague Amanda Mull, “The Enduring Appeal of Tacos,” along with a link and a summary (it began: “In this article, writer Amanda Mull explores the cultural significance of tacos and why they continue to be a beloved food.”). The only problem: That story doesn’t exist. The URL looked plausible but went nowhere, because Mull had never written the story. When I called the AI on its error, ChatGPT apologized and offered a substitute story, “Why Are American Kids So Obsessed With Tacos?”—which is also completely made up. Yikes. How can anyone expect to trust AI enough to deploy it in an automated way? According to Brockman, organizations like ours will need to build a track record with systems like ChatGPT before we’ll feel comfortable using them for real. Brockman told me that his staff at OpenAI spends a lot of time “red teaming” their systems, a term from cybersecurity and intelligence that names the process of playing an adversary to discover vulnerabilities. Brockman contends that safety and controllability will improve over time, but he encourages potential users of the ChatGPT API to act as their own red teamers—to test potential risks—before they deploy it. “You really want to start small,” he told me. Fair enough. If chat isn’t a necessary component of ChatGPT, then perhaps a smaller, more surgical example could illustrate the kinds of uses the public can expect to see. One possibility: A magazine such as ours could customize our copy to respond to reader behavior or change information on a page, automatically. Working with The Atlantic’s product and technology team, I whipped up a simple test along those lines. On the back end, where you can’t see the machinery working, our software asks the ChatGPT API to write an explanation of “API” in fewer than 30 words so a layperson can understand it, incorporating an example headline of the most popular story on The Atlantic’s website at the time you load the page. That request produces a result that reads like this: As I write this paragraph, I don’t know what the previous one says. It’s entirely generated by the ChatGPT API—I have no control over what it writes. I’m simply hoping, based on the many tests that I did for this type of query, that I can trust the system to produce explanatory copy that doesn’t put the magazine’s reputation at risk because ChatGPT goes rogue. The API could absorb a headline about a grave topic and use it in a disrespectful way, for example. In some of my tests, ChatGPT’s responses were coherent, incorporating ideas nimbly. In others, they were hackneyed or incoherent. There’s no telling which variety will appear above. If you refresh the page a few times, you’ll see what I mean. Because ChatGPT often produces different text from the same input, a reader who loads this page just after you did is likely to get a different version of the text than you see now. Media outlets have been generating bot-written stories that present sports scores, earthquake reports, and other predictable data for years. But now it’s possible to generate text on any topic, because large language models such as ChatGPT’s have read the whole internet. Some applications of that idea will appear in new kinds of word processors, which can generate fixed text for later publication as ordinary content. But live writing that changes from moment to moment, as in the experiment I carried out on this page, is also possible. A publication might want to tune its prose in response to current events, user profiles, or other factors; the entire consumer-content internet is driven by appeals to personalization and vanity, and the content industry is desperate for competitive advantage. But other use cases are possible, too: prose that automatically updates as a current event plays out, for example. Though simple, our example reveals an important and terrifying fact about what’s now possible with generative, textual AI: You can no longer assume that any of the words you see were created by a human being. You can’t know if what you read was written intentionally, nor can you know if it was crafted to deceive or mislead you. ChatGPT may have given you the impression that AI text has to come from a chatbot, but in fact, it can be created invisibly and presented to you in place of, or intermixed with, human-authored language. Carrying out this sort of activity isn’t as easy as typing into a word processor—yet—but it’s already simple enough that The Atlantic product and technology team was able to get it working in a day or so. Over time, it will become even simpler. (It took far longer for me, a human, to write and edit the rest of the story, ponder the moral and reputational considerations of actually publishing it, and vet the system with editorial, legal, and IT.) That circumstance casts a shadow on Greg Brockman’s advice to “start small.” It’s good but insufficient guidance. Brockman told me that most businesses’ interests are aligned with such care and risk management, and that’s certainly true of an organization like The Atlantic. But nothing is stopping bad actors (or lazy ones, or those motivated by a perceived AI gold rush) from rolling out apps, websites, or other software systems that create and publish generated text in massive quantities, tuned to the moment in time when the generation took place or the individual to which it is targeted. Brockman said that regulation is a necessary part of AI’s future, but AI is happening now, and government intervention won’t come immediately, if ever. Yogurt is probably more regulated than AI text will ever be. Some organizations may deploy generative AI even if it provides no real benefit to anyone, merely to attempt to stay current, or to compete in a perceived AI arms race. As I’ve written before, that demand will create new work for everyone, because people previously satisfied to write software or articles will now need to devote time to red-teaming generative-content widgets, monitoring software logs for problems, running interference with legal departments, or all other manner of tasks not previously imaginable because words were just words instead of machines that create them. Brockman told me that OpenAI is working to amplify the benefits of AI while minimizing its harms. But some of its harms might be structural rather than topical. Writing in these pages earlier this week, Matthew Kirschenbaum predicted a textpocalypse, an unthinkable deluge of generative copy “where machine-written language becomes the norm and human-written prose the exception.” It’s a lurid idea, but it misses a few things. For one, an API costs money to use—fractions of a penny for small queries such as the simple one in this article, but all those fractions add up. More important, the internet has allowed humankind to publish a massive deluge of text on websites and apps and social-media services over the past quarter century—the very same content ChatGPT slurped up to drive its model. The textpocalypse has already happened. Just as likely, the quantity of generated language may become less important than the uncertain status of any single chunk of text. Just as human sentiments online, severed from the contexts of their authorship, take on ambiguous or polyvalent meaning, so every sentence and every paragraph will soon arrive with a throb of uncertainty: an implicit, existential question about the nature of its authorship. Eventually, that throb may become a dull hum, and then a familiar silence. Readers will shrug: It’s just how things are now. Even as those fears grip me, so does hope—or intrigue, at least—for an opportunity to compose in an entirely new way. I am not ready to give up on writing, nor do I expect I will have to anytime soon—or ever. But I am seduced by the prospect of launching a handful, or a hundred, little computer writers inside my work. Instead of (just) putting one word after another, the ChatGPT API and its kin make it possible to spawn little gremlins in my prose, which labor in my absence, leaving novel textual remnants behind long after I have left the page. Let’s see what they can do.",[],0.07,"['new', 'many', 'more', 'useful', 'really', 'most', 'familiar', 'thanks', 'success', 'really', 'better', 'mature', 'first', 'most', 'kind', 'quickly', 'cultural', 'beloved', 'plausible', 'completely', 'comfortable', 'real', 'own', 'really', 'fair', 'more', 'most', 'popular', 'many', 'coherent', 'large', 'whole', 'new', 'fixed', 'live', 'important', 'easy', 'able', 'far', 'legal', 'good', 'most', 'certainly', 'more', 'new', 'previously', 'legal', 'not', 'more', 'important', 'important', 'familiar', 'entirely']","['other', 'eerie', 'obsessed', 'small', 'previous', 'incoherent', 'few', 'mean', 'other', 'predictable', 'ordinary', 'other', 'desperate', 'other', 'terrifying', 'small', 'bad', 'lazy', 'other', 'no', 'merely', 'other', 'unthinkable', 'few', 'small', 'past', 'less', 'single', 'dull', 'not', 'little', 'little', 'absence', 'behind', 'long']"
2,A Chatbot Is Secretly Doing My Job,"I have a part-time job that is quite good, except for one task I must do—not even very often, just every other week—that I actively loathe. The task isn’t difficult, and it doesn’t take more than 30 minutes: I scan a long list of short paragraphs about different people and papers from my organization that have been quoted or cited in various publications and broadcasts, pick three or four of these items, and turn them into a new, stand-alone paragraph, which I am told is distributed to a small handful of people (mostly board members) to highlight the most “important” press coverage from that week. Four weeks ago, I began using AI to write this paragraph. The first week, it took about 40 minutes, but now I’ve got it down to about five. Only one colleague knows I’ve been doing this; we used to switch off writing this blurb, but since it’s become so quick and easy and, frankly, interesting, I’ve taken over doing it every week. The process itself takes place within OpenAI’s “Playground” feature, which offers similar functionality as the company’s ChatGPT product. The Playground presents as a blank page, not a chat, and is therefore better at shaping existing words into something new. I write my prompt at the top, which always begins with something like “Write a newspaper-style paragraph out of the following.” Then, I paste below my prompt the three or four paragraphs I selected from the list and—this is crucial, I have learned—edit those a touch, to ensure that the machine “reads” them properly. Sometimes that means placing a proper noun closer to a quote, or doing away with an existing headline. Perhaps you’re thinking, This sounds like work too, and it is—but it’s quite a lot of fun to refine my process and see what the machine spits out at the other end. I like to think that I’ve turned myself from the meat grinder into the meat grinder’s minder—or manager. I keep waiting to be found out, and I keep thinking that somehow the copy will reveal itself for what it is. But I haven’t, and it hasn’t, and at this point I don’t think I or it ever will (at least, not until this essay is published). Which has led me to a more interesting question: Does it matter that I, a professional writer and editor, now secretly have a robot doing part of my job? I’ve surprised myself by deciding that, no, I don’t think it matters at all. This in turn has helped clarify precisely what it was about the writing of this paragraph that I hated so much in the first place. I realized that what I was doing wasn’t writing at all, really—it was just generating copy. Copy is everywhere. There’s a very good chance that even you, dear reader, are encountering copy as you read this: in the margins, between the paragraph breaks, beyond this screen, or in another window, always hovering, in ads or emails—the wordy white noise of our existence. ChatGPT and the Playground are quite good at putting copy together. The results certainly aren’t great, but they’re absolutely good enough, which is exactly as good as most copy needs to be: intelligible but not smart—simply serviceable. These tools require an editor to liven the text up or humanize it a touch. I often find myself adding an em dash here or there—haven’t you noticed? I love em dashes—or switching a sentence around, adjusting tenses, creating action. At one point, early on, I complained to a data-scientist friend who has worked with machine-learning systems that the robot didn’t seem to understand my command to “avoid the passive voice”; he suggested the prompt “no past tense verbs,” which helped but wasn’t quite right either. I sent him more of my prompts. He said they were too suggestive and that I needed to be firmer, more precise, almost mean. “You can’t hurt the robot’s feelings,” he said, “because it doesn’t have any.” But that’s just the thing, isn’t it? Writing is feeling. And thinking. And although writing certainly has rules, plenty of good writing breaks nearly all of them. When ChatGPT was first released, and everyone, particularly in academia, seemed to be freaking out, I thought back to my own experience as a writer who grew up with another computer-assisted writing tool: spell-check. I am a terrible—really, truly abysmal—speller. I’ve often thought that in a different, pre-spell-check era, my inability to confidently construct words might have kept me from a vocation that I love. I think now of all the kids coming up who are learning to write alongside ChatGPT, just as I learned to write with spell-check. ChatGPT isn’t writing for them; it’s producing copy. For plenty of people, having a robot help them produce serviceable copy will be exactly enough to allow them to get by in the world. But for some, it will lower a barrier. It will be the beginning of their writing career, because they will learn that even though plenty of writing begins with shitty, soulless copy, the rest of writing happens in edits, in reworking the draft, in all the stuff beyond the initial slog of just getting words down onto a page. Already, folks are working hard to close off this avenue for new writing and new writers. Just as I was writing the sentences above, I received an email from the digital editorial director at Travel + Leisure alerting me to an important update regarding “our content creation policy.” “At Travel + Leisure,” she wrote, in bold, “we only publish content authored entirely by humans and it is against our policies to use ChatGPT or similar tools to create the articles you provide to us, in part or in full.” This and other panicked responses seem to fundamentally misunderstand the act of writing, which is generative—a process. Surely there will be writers—new writers, essential writers, interesting writers—who come to their own process alongside ChatGPT or the Playground or other AI-based writing tools, who break open new aesthetics and ideas in writing and what it can be. After all, there are already great artists who have long worked with robots. One of my favorites is Brian Eno, who has been an evangelist for the possibilities of musical exploration and collaboration with computer programs for decades now. A few years ago, in a conversation with the producer Rick Rubin, Eno laid out his process: He begins with an algorithmic drum loop that is rhythmically perfect, and then starts inserting small errors—bits of humanity—before playing with other inputs to shape the sound. “What I have been doing quite a lot is tuning the system so that it starts to get into that interesting area of quasi-human” is how he described playing alongside the machine. “Sometimes, there will be a particularly interesting section, where the ‘drummer’”—that is, the computer—“does something really extraordinary … Sometimes the process is sort of iterated two or three times to get somewhere I like.” Then Eno chuckled his very British-sounding chuckle: “Very little of this stuff have I actually released … I’m just playing with it, and fascinated by it.” To which I can only add: So am I.",[],0.18,"['good', 'very', 'more', 'new', 'mostly', 'most', 'important', 'first', 'quick', 'easy', 'interesting', 'offers', 'better', 'new', 'top', 'fun', 'more', 'interesting', 'professional', 'surprised', 'precisely', 'much', 'first', 'very', 'good', 'certainly', 'great', 'absolutely', 'exactly', 'most', 'love', 'action', 'early', 'no', 'right', 'more', 'more', 'precise', 'certainly', 'good', 'nearly', 'first', 'particularly', 'own', 'confidently', 'love', 'new', 'new', 'important', 'bold', 'full', 'surely', 'interesting', 'own', 'new', 'great', 'perfect', 'sound', 'interesting', 'particularly', 'really', 'very']","['other', 'actively', 'difficult', 'long', 'small', 'down', 'other', 'least', 'secretly', 'hated', 'complained', 'tense', 'mean', 'down', 'hard', 'other', 'other', 'long', 'few', 'small', 'other', 'very']"
3,What Poets Know That ChatGPT Doesn’t,"One of the least discussed aspects of the AI language generator ChatGPT might be its ability to produce pretty awful poetry. Given how difficult it is to teach a computer how to recognize a syllable, I’m not disparaging the technical prowess of the chatbot’s creators and testers. But very few of the AI-produced poems I’ve read actually follow the prompt that’s been provided. “Write a poem in the style of Seamus Heaney”? This is not that poem: In a garden green and fair, A flower blooms, a sight so rare. But is it meant for me, I fear? Will I, like it, bloom this year? Odds are good that this poem, titled “Is It for Me?,” will not win the National Poetry Series. The final phrase seems plucked from T. S. Eliot’s “The Waste Land,” which gives the last line an unintended comic air, because Eliot is referring to a corpse. Poetry, with its heightened states of emotion, intimate address, ecstatic proclamation, and enchanting song, would seem to be one of the limit cases that prove the point: ChatGPT can write anything we can write. It can indeed compose poems from prompts such as “write a poem about the estate tax.” Asked to write a sonnet about socks, it will produce a poem with the opening line “Oh socks, my trusty companions on my feet.” Such goofy attempts could be said to emulate praise poetry, that venerable form of ode-making. They could just as well have been spoken by Brick Tamland, Steve Carell’s character in Anchorman, who is prone to spouting cryptic one-liners—including, famously, “I love lamp.” (As a teacher of poetry, I can’t help but imagine an overly eager chatbot in one of my creative-writing workshops in the year 2030. “Do you really love the lamp,” I picture myself asking it, “or are you just saying that because you saw it?”) Heaney wrote a poem about the death of his mother called “Clearances” that—like the AI-generated “Is It for Me?”—also uses rhyme, meter, and nature imagery: I thought of walking round and round a space Utterly empty, utterly a source Where the decked chestnut tree had lost its place In our front hedge above the wallflowers. The difference between ChatGPT’s Heaney-esque poem and Heaney’s actual poem is not simply that one is bad and one is good, or that one is sentimental and one is elegiacally beautiful. The difference is that Heaney lost his mother, and the poem expresses the emotional urgency of this fact during a reflective moment sometime after the event. Heaney’s poem carries the ineffable sense that the poet has not only pillaged from the horde of words that already exist but has also worked on them himself, claiming them partly as his and partly as a treasure loaned to him from centuries of poetry written in English. I could point to other aspects of the language: the pause in the second line, the similarity between the sounds of decked and chest-, the lingering syllables of wallflowers. Above all, there’s the mystery of the mourning poet’s meditation—that missing tree that both orients and eludes him. ChatGPT can write poemlike streams of regurgitated text, but they don’t mourn and console and mystify with an image like the chestnut tree, which casts an immersive spell. They don’t satisfy the minimal criterion of a poem, which is a pattern of language that compresses the messy data of experience, emotion, truth, or knowledge and turns those, as W. H. Auden wrote in 1935, into “memorable speech.” Ian Bogost suggests that ChatGPT produces “an icon of the answer … rather than the answer itself.” This is correct: The poem it spits out is an emblem of what a poem is rather than an example of a poem. It is closer to a found object than to Emily Dickinson’s four-line poems in rhyme, which take “unorthodox, subversive, sometimes volcanic propensities” and channel them “into a dialect called metaphor.” That’s what the poet Adrienne Rich found in Dickinson’s poetry—a hint as to how poems are made, a trace of their creation. Rich thought it was critically important that a poet’s imagination be followed back to her confining circumstances. For Dickinson, that was a house in Amherst in the 1860s and ’70s. For Rich, who wrote a century later, it was raising three children while questioning her sexuality and political commitments. Not that the relation between the life and the poem is ever easy to make out: Indeed, Rich spent her career learning radically new ways to thread her experiences—as a mother, a homemaker in the suburbs, a lesbian, a feminist, a Jew—into language, changing the language in the process. She was like the poet she imagines in “Poetry: II, Chicago,” written in 1984: Wherever a poet is born     enduring depends on the frailest of chances: Who listened to your murmuring over your little rubbish     who let you be who gave you the books who let you know you were not alone Poems, she continues, are “fiery lines” that say, “This belongs to you     you have the right / you belong to the song / of your mothers and fathers     You have a people.” They are almost always precarious in their transmission, whether they get to the poet from a god via Plato’s chain of magnetized iron or from the “inconstant wind” of human inspiration that Percy Bysshe Shelley likened to a fading coal. Now is not the time to give up on that essential strangeness and fragility in favor of productivity and predictability. The world needs more poems, not faster ones. ChatGPT cannot write poetry—or prose, for that matter—that is “the cry of its occasion,” as Wallace Stevens would have it, because there is no lived “occasion” other than the set of texts it can read. Neither can there be emotion recollected in tranquility. There’s no involuntary memory that’s stimulated by the taste of a madeleine. Creativity requires more than an internet-size syllabus or a lesson in syllables. So does essay writing, which is why, even though many acknowledge that ChatGPT can write passable high-school and undergraduate essays, I’m not concerned about that either. The poems that ChatGPT writes are riddled with cliché and wince-worthy rhymes, but it isn’t just issues of quality that separate AI- and human-generated compositions. Poetry, whether in the style of Heaney or Dickinson or your journal from fourth grade, comes from the felt necessity to speak a truth, whatever kind of truth that might be, in a tongue that you’ve inherited or learned—or that has been imposed upon you by force or violence. That’s obvious to anyone who, for reasons they can’t fully explain, sits down and organizes their words into a pattern that’s slightly different from the language they use at the dinner table. Whatever upgrades might come for ChatGPT, what it writes likely won’t emerge from the burning sense that something is missing from the world. Poetry speaks in the words of the dead, words sometimes borrowed from past poems—but the desire to use those words comes from an intuition that something is still hidden in them, something that needs to be heard in the harmony between our present voices and those earlier ones. The resemblance between AI-generated writing and human-generated writing is surface level. We know a little more now about how computers arrange words into patterns. The real question—the question that we keep trying to answer with vital metaphors of “fiery lines” and fading coals—is how humans do.",[],0.07,"['pretty', 'fair', 'rare', 'good', 'comic', 'intimate', 'goofy', 'famously', 'really', 'good', 'beautiful', 'memorable', 'rich', 'rich', 'critically', 'rich', 'easy', 'rich', 'new', 'right', 'more', 'more', 'many', 'kind', 'more', 'real', 'vital']","['least', 'awful', 'difficult', 'very', 'green', 'not', 'waste', 'round', 'round', 'utterly', 'bad', 'sentimental', 'other', 'missing', 'minimal', 'messy', 'spent', 'little', 'other', 'down', 'missing', 'dead', 'past', 'hidden', 'little']"
4,What ChatGPT Can’t Teach My Writing Students,"As the first student papers of the academic semester come rolling in, college and high-school teachers are expressing concern about ChatGPT, the artificial-intelligence interface that responds to queries with competent, if boring, paragraphs. It seems to open up whole new vistas of academic dishonesty, and it calls into question how and why we teach writing at all. A professor at the University of Pennsylvania's Wharton School has said that ChatGPT’s answers to his operations-management class would have earned a B or B–. That seems about right; if a student in my first-year writing class had turned in a ChatGPT-generated essay last semester (and for all I know, someone did), they would have easily passed. The fact is, boring competence is better than what some high-school or college graduates attain, and it’s all most people, in their daily lives, need their writing to be. If, in a few years, AI can do a passable job at most adult writing tasks—sharing information, telling quick stories, apologizing for the delay, and expressing a hope that all is well—then why spend so much time in school learning the maddening complexities of English prose? Surely there are more important things to study than subject-verb agreement, comma splices, and transition sentences. But learning to write is about more than learning to write. For one thing, it’s about learning to turn a loose assemblage of thoughts into a clear line of reasoning—a skill that is useful for everyone, not just those who enjoy writing or need to do a lot of it for work. Just as important, learning to write trains your imagination to construct the person who will read your words. Writing, then, is an ethical act. It puts you in relation to someone you may not know, someone who may, in fact, not yet exist. When you learn to write, you learn to exercise your responsibility to that person, to meet their needs in a context you cannot fully know. That might sound like a lofty goal for a paper about, for instance, the major causes of the American Revolution. But even that bog-standard assignment can get students to anticipate what another person knows and expects. You wouldn’t write the same essay to a veterans’ group as you would to new immigrants. Writing is never simply self-expression. It’s expression to a specific audience for a specific purpose. In some cases, like a love letter, a writer knows their audience intimately. In others, the audience is every bit a work of the imagination as a novel’s characters are. Great writers have known this truth for centuries. Nathaniel Hawthorne writes in the introduction to The Scarlet Letter that “when he casts his leaves forth upon the wind, the author addresses, not the many who will fling aside his volume, or never take it up, but the few who will understand him, better than most of his schoolmates and lifemates.” Writers, then, should give up trying to address the public at large, but should “imagine that a friend, a kind and apprehensive, though not the closest friend, is listening to our talk.” I would not go so far as to say that you and I are friends, but to convince you that I’m right about writing and the moral imagination, I need to make a mental model of who you are: what you value, what annoys you, how much explanation and evidence you need. And then I invite that imaginary version of you to look over my shoulder and suggest revisions. My editors give voice to a model of you too. (And meanwhile, advertising software compiles its own portrait.) If the essay is to succeed, our models must do justice to who you are. That’s the first step in our responsibility to you. When this act of imagination is executed well, a reader can feel profoundly understood, as if a stranger has told them some previously unknown truth about themselves. That’s how I felt reading Meghan Daum’s 2014 essay “Difference Maker,” which is about her ambivalence toward parenthood and her somewhat ineffectual advocacy for children in the foster-care system. Daum describes a “Central Sadness” that became a “third party” in her marriage. “It collected around our marriage like soft, stinky moss,” she writes. “It rooted our arguments and dampened our good times. It taunted us from the sidelines of our social life.” My wife and I both read the essay when it came out and thought, Yes, this is what we’re feeling. Our Central Sadness had a different character than Daum’s had, but it played a similar role for us. Naming the affliction didn’t solve the problem, but it did help us understand its depths. Reading the essay was therapeutic. Writers are not morally better in their behavior than other people, and writing is not the only way to develop an empathetic mind. In fact, in the age of Instagram and Substack, many writers abuse their power to forge imaginary connections by cultivating one-sided, parasocial relationships with readers. Through calculated oversharing about their daily lives, authors can maintain the illusion that they are their readers’ smartest or funniest or most curmudgeonly friends. Still, developing this ability to connect with others through the imagination is central to ethical life. The philosopher Mark Johnson argues in his 1993 book, Moral Imagination, that ethics is not primarily about applying universal rules to specific situations but about “the ongoing imaginative exploration of possibilities for dealing with our problems, enhancing the quality of our communal relations, and forming significant personal attachments that grow.” Empathy plays a central role in this model of ethics. We cannot act responsibly toward others unless we “go out toward people to inhabit their worlds, not just by rational calculations, but also in imagination, feeling, and expression.” School, however, does not often train students to exercise this mode of imagination through writing. “I find that when students arrive in college, they don’t see writing as a medium of communication, really,” Jim Warren, an English professor at the University of Texas at Arlington who specializes in rhetoric and composition, told me. “They see it as sort of this engineering task that they’re then going to present to us as examiner and hopefully have us say, ‘Yeah, you did it right.’” A big part of the problem, Warren writes in a recent article, is that though all 50 states’ education standards (plus those in the District of Columbia) require that students learn to write essays to specific audiences, only 12 states actually test high-school students on this ability. And because tests drive curricula, Warren contends, it is likely that students in the majority of states are getting little, if any, instruction in how to write with an audience other than their teacher in mind. To be sure, trying to figure out “what the teacher wants” is an exercise in moral imagination, albeit a limited one. The task for teachers is to expand that exercise. Warren told me that for some assignments, his students write about whatever they want to whomever they think needs what they have to say. The students then research this audience and explain to Warren whose eyes he’ll read their paper through. In peer-editing sessions, students adopt the mindset of one another’s audiences. Warren said students tell him at the end of the semester that the exercise gets them thinking more about readers’ expectations. “I think it moves the needle a bit,” he said. In the scope of human history, mass literacy is a new phenomenon. Today, just about anyone can, in principle, communicate to someone far away in time and space. Writing is not the only modern form of action at a distance, though. Around the same time that human societies became literate on a large scale, their citizens also began burning mass quantities of fossil fuels that, we now know, can make life much harder for people who are far away in time and space. Some of the biggest ethical challenges facing residents of rich countries in this century have to do with how we act toward people we can only imagine: climate refugees who (for now) mostly live far away, future people who will inhabit post-Anthropocene Earth, artificial intelligences, and animals whom we see as having a growing scope of rights. Now that we are beginning to reckon with the harm we have done to the climate and are trying to reverse it, we need every bit of the empathetic imagination that mass literacy fosters. It seems inevitable that large-language models of AI will allow us to offload some of the writing tasks that students learn in school. But we can’t allow ourselves to lose the capacity to empathize with distant strangers at just the moment when we’re more able than ever to communicate with them.",[],0.13,"['first', 'competent', 'whole', 'new', 'right', 'easily', 'better', 'most', 'most', 'adult', 'quick', 'much', 'surely', 'more', 'important', 'more', 'clear', 'useful', 'enjoy', 'important', 'ethical', 'sound', 'major', 'new', 'love', 'intimately', 'great', 'many', 'better', 'most', 'large', 'kind', 'far', 'right', 'much', 'own', 'first', 'profoundly', 'soft', 'good', 'social', 'many', 'most', 'ethical', 'imaginative', 'significant', 'responsibly', 'really', 'right', 'sure', 'wants', 'more', 'new', 'far', 'modern', 'action', 'large', 'far', 'ethical', 'rich', 'mostly', 'far', 'more', 'able']","['boring', 'boring', 'few', 'loose', 'few', 'mental', 'previously', 'not', 'other', 'not', 'little', 'other', 'limited', 'much', 'artificial', 'distant']"
5,ChatGPT Is About to Dump More Work on Everyone,"Have you been worried that ChatGPT, the AI language generator, could be used maliciously—to cheat on schoolwork or broadcast disinformation? You’re in luck, sort of: OpenAI, the company that made ChatGPT, has introduced a new tool that tries to determine the likelihood that a chunk of text you provide was AI-generated. I say “sort of” because the new software faces the same limitations as ChatGPT itself: It might spread disinformation about the potential for disinformation. As OpenAI explains, the tool will likely yield a lot of false positives and negatives, sometimes with great confidence. In one example, given the first lines of the Book of Genesis, the software concluded that it was likely to be AI-generated. God, the first AI. On the one hand, OpenAI appears to be adopting a classic mode of technological solutionism: creating a problem, and then selling the solution to the problem it created. But on the other hand, it might not even matter if either ChatGPT or its antidote actually “works,” whatever that means (in addition to its limited accuracy, the program is effective only on English text and needs at least 1,000 characters to work with). The machine-learning technology and others like it are creating a new burden for everyone. Now, in addition to everything else we have to do, we also have to make time for the labor of distinguishing between human and AI, and the bureaucracy that will be built around it. If you are a student, parent, educator, or individual with internet access, you may have caught wind of the absolute panic that has erupted around ChatGPT. There are fears—It’s the end of education as we know it! It passed a Wharton MBA exam!—and retorts to those fears: We must defend against rampant cheating. If your class can be gamed by an AI, then it was badly designed in the first place! An assumption underlies all these harangues, that education needs to “respond” to ChatGPT, to make room for and address it. At the start of this semester at Washington University in St. Louis, where I teach, our provost sent all faculty an email encouraging us to be aware of the technology and consider how to react to it. Like many institutions, ours also hosted a roundtable to discuss ChatGPT. In a matter of months, generative AI has sent secondary and postsecondary institutions scrambling to find a response—any response—to its threats or opportunities. That work heaps atop an already overflowing pile of duties. Budgets cut, schoolteachers often crowdsource funds and materials for their classrooms. The coronavirus pandemic changed assumptions about attendance and engagement, making everyone renegotiate, sometimes weekly, where and when class will take place. Managing student anxiety and troubleshooting broken classroom technology is now a part of most teachers’ everyday work. That’s not to mention all the emails, and the training modules, and the self-service accounting tasks. And now comes ChatGPT, and ChatGPT’s flawed remedy. The situation extends well beyond education. Almost a decade ago, I diagnosed a condition I named hyperemployment. Thanks to computer technology, most professionals now work a lot more than they once did. In part, that’s because email and groupware and laptops and smartphones have made taking work home much easier—you can work around the clock if nobody stops you. But also, technology has allowed, and even required, workers to take on tasks that might otherwise have been carried out by specialists as their full-time job. Software from SAP, Oracle, and Workday force workers to do their own procurement and accounting. Data dashboards and services make office workers part-time business analysts. On social media, many people are now de facto marketers and PR agents for their division and themselves. No matter what ChatGPT and other AI tools ultimately do, they will impose new regimes of labor and management atop the labor required to carry out the supposedly labor-saving effort. ChatGPT’s AI detector introduces yet another thing to do and to deal with. Is a student trying to cheat with AI? Better run the work through the AI-cheater check. Even educators who don’t want to use such a thing will be ensnared in its use: subject to debates about the ethics of sharing student work with OpenAI to train the model; forced to adopt procedures to address the matter as institutional practice, and to reconfigure lesson plans to address the “new normal”; obligated to read emails about those procedures to consider implementing them. At other jobs, different but similar situations will arise. Maybe you outsourced some work to a contractor. Now you need to make sure it wasn’t AI-generated, in order to prevent fiscal waste, legal exposure, or online embarrassment. As cases like this appear, prepare for an all-hands meeting, and a series of email follow-ups, and maybe eventually a compulsory webinar and an assessment of your compliance with the new learning-management system, and on and on. New technologies meant to free people from the burden of work have added new types of work to do instead. Home appliances such as the washing machine freed women to work outside the home, which in turn reduced time to do housework (which still fell largely to women) even as the standards for home perfection rose. Photocopiers and printers reduce the burden of the typist but create the need to self-prepare, collate, and distribute the reports in addition to writing them. The automated grocery checkout assigns the job of cashier to the shopper. Email makes it possible to communicate rapidly and directly with collaborators, but then your whole day is spent processing emails, which renews the burden again the next day. Zoom makes it possible to meet anywhere, but in doing so begets even more meetings. ChatGPT has held the world’s attention, a harbinger of—well, something, but maybe something big, and weird, and new. That response has inspired delight, anxiety, fear, and dread, but no matter the emotion, it has focused on the potential uses of the technology, whether for good or ill. The ChatGPT detector offers the first whiff of another, equally important consequence of the AI future: its inevitable bureaucratization. Microsoft, which has invested billions of dollars in OpenAI, has declared its hope to integrate the technology into Office. That could help automate work, but it’s just as likely to create new demands for Office-suite integration, just as previous add-ons such as SharePoint and Teams did. Soon, maybe, human resources will require the completion of AI-differentiation reports before approving job postings. Procurement may adopt a new Workday plug-in to ensure vendor-work-product approvals are following AI best practices, a requirement you will now have to perform in addition to filling out your expense reports—not to mention your actual job. Your Salesforce dashboard may offer your organization the option to add a required AI-probability assessment before a lead is qualified. Your kids’ school may send a “helpful” guide to policing your children’s work at home for authenticity, because “if AI deception is a problem, all of us have to be part of the solution.” Maybe AI will help you work. But more likely, you’ll be working for AI.",[],0.1,"['new', 'new', 'great', 'first', 'first', 'classic', 'effective', 'new', 'absolute', 'first', 'aware', 'many', 'most', 'thanks', 'most', 'more', 'much', 'own', 'social', 'many', 'new', 'better', 'new', 'normal', 'sure', 'legal', 'new', 'new', 'free', 'new', 'largely', 'rose', 'directly', 'whole', 'more', 'new', 'good', 'offers', 'first', 'equally', 'new', 'new', 'best', 'more']","['tries', 'false', 'other', 'limited', 'least', 'badly', 'secondary', 'broken', 'everyday', 'flawed', 'other', 'subject', 'forced', 'other', 'waste', 'spent', 'weird', 'ill', 'previous']"
6,How ChatGPT Will Destabilize White-Collar Work,"In the next five years, it is likely that AI will begin to reduce employment for college-educated workers. As the technology continues to advance, it will be able to perform tasks that were previously thought to require a high level of education and skill. This could lead to a displacement of workers in certain industries, as companies look to cut costs by automating processes. While it is difficult to predict the exact extent of this trend, it is clear that AI will have a significant impact on the job market for college-educated workers. It will be important for individuals to stay up to date on the latest developments in AI and to consider how their skills and expertise can be leveraged in a world where machines are increasingly able to perform many tasks. There you have it, I guess: ChatGPT is coming for my job and yours, according to ChatGPT itself. The artificially intelligent content creator, whose name is short for “Chat Generative Pre-trained Transformer,” was released two months ago by OpenAI, one of the country’s most influential artificial-intelligence research laboratories. The technology is, put simply, amazing. It generated that first paragraph instantly, working with this prompt: “Write a five-sentence paragraph in the style of The Atlantic about whether AI will begin to reduce employment for college-educated workers in the next five years.” ChatGPT is just one of many mind-blowing generative AI tools released recently, including the image generators Midjourney and DALL-E and the video generator Synthesia. The upside of these AI tools is easy to see: They’re going to produce a tremendous amount of digital content, quickly and cheaply. Students are already using ChatGPT to help them write essays. Businesses are using ChatGPT to create copy for their websites and promotional materials, and to respond to customer-service inquiries. Lawyers are using it to produce legal briefs (ChatGPT passes the torts and evidence sections of the Multistate Bar Examination, by the way) and academics to produce footnotes. Yet an extraordinary downside is also easy to see: What happens when services like ChatGPT start putting copywriters, journalists, customer-service agents, paralegals, coders, and digital marketers out of a job? For years, tech thinkers have been warning that flexible, creative AI will be a threat to white-collar employment, as robots replace skilled office workers whose jobs were once considered immune to automation. In the most extreme iteration, analysts imagine AI altering the employment landscape permanently. One Oxford study estimates that 47 percent of U.S. jobs might be at risk. No single technology in modern memory has caused mass job loss among highly educated workers. Will generative AI really be an exception? No one can answer this question, given how new the technology is and given how slowly employment can adjust in response to technological change. But AI really is different, technology experts told me—a range of tasks that up until now were impossible to automate are becoming automatable. “Before, progress was linear and predictable. You figured out the steps and the computer followed them. It followed the procedure; it didn’t learn and it didn’t improvise,” the MIT professor David Autor, one of the world’s foremost experts on employment and technological change, told me. ChatGPT and the like do improvise, promising to destabilize a lot of white-collar work, regardless of whether they eliminate jobs or not. People and businesses are just figuring out how to use emerging AI technologies, let alone how to use them to create new products, streamline their business operations, and make employees more efficient. If history is any guide, this process could take longer than you might think. Consider electricity. The circuit, electric lights, and rudimentary electric motors were developed in the early 1800s. But another century passed before the widespread adoption of electricity in the United States began to lift GDP. Or take computers. They became commercially available in the early 1950s but did not show up in the productivity stats until the late 1990s. Some technologies clearly improve productivity and reduce the need for labor. Automated machine tools, for instance, depress manufacturing employment while lifting output and productivity, as do many of the forms of machinery invented and employed since the Industrial Revolution. But other technologies—even amazing ones—show surprisingly muted effects. How about the internet, which has revolutionized almost every facet of communications in the past four decades? Despite altering how we date and talk and read and watch and vote and emote and record our own life stories, launching a zillion businesses, and creating however many fortunes, the internet “fails the hurdle test as a Great Invention,” the economist Robert Gordon argued in 2000, because it “provides information and entertainment more cheaply and conveniently than before, but much of its use involves substitution of existing activities from one medium to another.” Nearly a quarter century later, the internet still hasn’t spurred a productivity revolution. Smartphones haven’t either. So is AI like the smartphone or is it like an automated machine tool? Is it about to change the way that work gets done without eliminating many jobs in aggregate, or is it about to turn San Francisco into the Rust Belt? Predicting where technology will cause job losses is hard, Autor noted. Remember the freak-out several years ago over the possibility of self-driving automobiles eliminating work for truck drivers? But AI is much more flexible than a system like Excel, much more creative than a Google Doc. What’s more, AI systems get better and better and better as they get more use and absorb more data, whereas engineers often need to laboriously and painstakingly update other types of software. As a rule, when companies can substitute machines for people, they will. AI can do work currently done by paralegals, copywriters, digital-content producers, executive assistants, entry-level computer programmers, and, yes, some journalists. That means such jobs might change, and soon. But even if ChatGPT can spit out a pretty good paragraph on AI, it can’t interview AI and labor experts, nor can it find historical documents, nor can it assess the quality of studies of technological change and employment. It creates content out of what is already out there, with no authority, no understanding, no ability to correct itself, no way to identify genuinely new or interesting ideas. That implies that AI might make original journalism more valuable and investigative journalists more productive, while creating an enormous profusion of simpler content. AI might spit out listicles and summaries of public meetings, while humans will write in-depth stories. “In many ways, AI will help people use expertise better,” Autor said. “It means that we’ll specialize more.” AI could also make a wide variety of industries more efficient, with muted effects on overall employment. Matt Wampler is a co-founder of an AI-powered small business called ClearCOGS. He’s been a “restaurant guy” his whole career, he told me. Restaurants and grocery stores, he says, tend to run on thin margins, yet still tend to waste a considerable amount of food. People order more spaghetti than burgers; buns get thrown out. “Restaurants just lag behind on technology,” he told me. “They’re all about people. It’s people serving people; it’s people managing people. And in that very human-centric world, the default way of handling problems is to hand it to a person. Phil’s going to do it.” ClearCOGS takes restaurants’ customer-order history, supply data, and labor data and uses AI-powered modeling to make their books leaner and more profitable. If people are starting to order more spaghetti than burgers, the system will prompt the chef or manager to buy more pasta and fewer rolls. “We put this in place in some of my cousin’s sandwich shops,” Wampler told me. “Simple answers to simple questions. The question they needed answered was, there’s an assistant manager on the night shift and a couple hours before close, he has to decide whether to bake another tray of bread or not. We provide that answer.” This use of ChatGPT isn’t eliminating human jobs, really; neighborhood sandwich joints aren’t hiring McKinsey consultants. But it might make food service more efficient as a whole. Even if it doesn’t boost the economy, AI could still change the texture of our lives and alter how we spend our time, like social media did before it. Video games might become more immersive. Shops might have far better copywriting and sales visuals. Movies might look cooler. Videos in the depths of YouTube might become far weirder and more beautiful. We might also see far more formulaic content than we already do. (Much more ominously, there might be a huge amount of plausible-seeming disinformation online.) For workers, Autor noted, the great risk is that AI technologies cause too sudden a change in what kind of labor employers want. Certain specializations might get wiped out, leaving thousands of call-center operators or marketing workers unemployed. But he stressed the benefits of having such technology in our hands. Productivity has languished for decades. Machines doing a little more work would have a big upside, after all.",[],0.23,"['able', 'high', 'certain', 'exact', 'clear', 'significant', 'important', 'latest', 'able', 'many', 'artificially', 'most', 'simply', 'first', 'many', 'easy', 'tremendous', 'quickly', 'cheaply', 'legal', 'extraordinary', 'easy', 'creative', 'skilled', 'most', 'no', 'modern', 'highly', 'really', 'new', 'becoming', 'promising', 'new', 'more', 'developed', 'early', 'commercially', 'early', 'clearly', 'many', 'amazing', 'surprisingly', 'own', 'many', 'great', 'more', 'cheaply', 'much', 'nearly', 'many', 'much', 'much', 'creative', 'more', 'better', 'better', 'better', 'more', 'more', 'pretty', 'good', 'genuinely', 'interesting', 'original', 'more', 'more', 'many', 'better', 'more', 'more', 'whole', 'considerable', 'more', 'very', 'more', 'more', 'more', 'really', 'more', 'whole', 'social', 'more', 'far', 'better', 'far', 'more', 'beautiful', 'far', 'more', 'much', 'huge', 'great', 'kind', 'certain', 'more']","['previously', 'difficult', 'extreme', 'slowly', 'impossible', 'predictable', 'late', 'depress', 'other', 'past', 'fails', 'hard', 'other', 'wide', 'small', 'thin', 'waste', 'behind', 'little']"
7,A Chatbot’s Predictions for the Future of AI,"Question of the Week To complete this week’s question I had a conversation with OpenAI’s chatbot, GPT-3 (which anyone can try). “Every week I ask readers of my newsletter a different question,” I wrote. “Would you compose this week’s question on the subject of AI, choosing one that is likely to elicit the highest number of interesting responses?” GPT-3 responded, in part, with this suggestion: Sure! Here is a question that might elicit a high number of interesting responses: How do you think AI will change the way we live and work in the next decade? This question seems like asking, circa 1995, how the internet would change the way we live and work. When you respond, know that people of the future will look back with interest on your predictions! Conversations of Note In addition to prompting GPT-3 to generate this week’s question, I interviewed it about the other OpenAI tool that I’ve been testing out, DALL-E, an artificial-intelligence program capable of generating original images from text descriptions. “It has the potential to significantly improve the efficiency of image creation,” GPT-3 told me, with applications in advertising, design, entertainment, art work, and other creative enterprises. As an example, I asked DALL-E to generate images of four Looney Tunes characters as if they were starring in a Wes Anderson movie. Here is the star-studded cast: Yosemite Sam: Bugs Bunny: Wile E. Coyote: And the Roadrunner: I also asked DALL-E to generate Michael Jordan posters in different styles. Here’s one in the style of Jackson Pollock: Just as interesting were the results when I deployed a trick I picked up at a recent Atlantic event in Los Angeles: asking the text-based ChatGPT to help write better prompts for an image-generating AI. For example, say I was trying to come up with ideas to decorate my living room. If I ask DALL-E to generate “a living room that would be good for reading in” I get this: Whereas if I ask GPT-3 to help me to write a better prompt for DALL-E, I get this: “Show me a living room with comfortable seating, good lighting, and plenty of shelving for books, that would be the perfect place to relax and get lost in a good book. Include a fireplace, a view of the outdoors, and a quiet and peaceful atmosphere.” Pasting that into DALL-E generates this: You can play with DALL-E on your own, too, and if you do you’ll quickly discover how expansive its potential use cases are. I’ll be eager to hear your various thoughts by email. My prediction is that, for a long stretch of time to come, the use of text and image content generated by AI platforms plus human prompts will outstrip that by AI alone, or by humans alone, across many applications. Is Writing Still an Important Skill to Learn? Daniel Herman, who teaches various high-school humanities classes, reflects in The Atlantic on advances in artificial intelligence that can “generate sophisticated text in response to any prompt you can imagine.” The technology “may signal the end of writing assignments altogether—and maybe even the end of writing as a gatekeeper, a metric for intelligence, a teachable skill,” he argues: If you’re looking for historical analogues, this would be like the printing press, the steam drill, and the light bulb having a baby, and that baby having access to the entire corpus of human knowledge and understanding. My life—and the lives of thousands of other teachers and professors, tutors and administrators—is about to drastically change. … This semester I am lucky enough to be teaching writers like James Baldwin, Gloria Anzaldúa, Herman Melville, Mohsin Hamid, Virginia Held. I recognize that it’s a privilege to have relatively small classes that can explore material like this at all. But at the end of the day, kids are always kids. I’m sure you will be absolutely shocked to hear that not all teenagers are, in fact, so interested in having their mind lit on fire by Anzaldúa’s radical ideas about transcending binaries, or Ishmael’s metaphysics in Moby-Dick. To those students, I have always said: You may not be interested in poetry or civics, but no matter what you end up doing with your life, a basic competence in writing is an absolutely essential skill—whether it’s for college admissions, writing a cover letter when applying for a job, or just writing an email to your boss. I’ve also long held, for those who are interested in writing, that you need to learn the basic rules of good writing before you can start breaking them—that, like Picasso, you have to learn how to reliably fulfill an audience’s expectations before you get to start putting eyeballs in people’s ears and things. I don’t know if either of those things is true anymore. It’s no longer obvious to me that my teenagers actually will need to develop this basic skill, or if the logic still holds that the fundamentals are necessary for experimentation. Let me be candid (with apologies to all of my current and former students): What GPT can produce right now is better than the large majority of writing seen by your average teacher or professor … I believe my most essential tasks, as a teacher, are helping my students think critically, disagree respectfully, argue carefully and flexibly, and understand their mind and the world around them. Unconventional, improvisatory, expressive, meta-cognitive writing can be an extraordinary vehicle for those things. But if most contemporary writing pedagogy is necessarily focused on helping students master the basics, what happens when a computer can do it for us? Will “Creative” AIs Increase Returns to Excellence? That is the writer Virginia Postrel’s guess, as she notes in her Substack newsletter: While crashing the value of mediocrity, ChatGPT could increase the returns to excellence. (“Average is over,” as Tyler Cowen put it.) Think about what happened to graphic design. Many people used to make a living doing routine tasks, from laying out pages to selecting typefaces, that are now easily handled by software. Thanks to the graphic intelligence embedded in everyday tools, the standards for routine graphics, from websites and PowerPoint presentations to restaurant menus and wedding invitations, have increased. But that doesn’t mean there’s no work for graphic designers with the conceptual chops to take on complicated tasks. Powerful tools make iteration and brainstorming easier, but cleverness is still a valued skill. When my friend Shikha Dalmia launched The Unpopulist on Substack, she asked me to look at some logos she’d come up with using easily available tools. They weren’t terrible, but neither were they distinctive. “Hire a professional,” I advised, and she got a real logo … Mediocre writing that earns grade-inflated Bs is now replaceable by a bot. Maybe if those B-essay students started with AI-generated prose it would be easier to teach them to do better: to refine the ideas, dig down more on the facts, improve the writing style. Can ChatGPT be a time-saving tool, like a calculator or text search, rather than a threat? Will Humans Have Inflated Confidence in AI? Louis Rosenberg expresses that worry at Big Think: Personally, my biggest concern about Generative AI systems is that we humans may assume that their informational output is accurate because it came from a computer. After all, most of us grew up watching shows and movies like Star Trek where characters verbally ask computers for information and instantly get accurate and trustworthy results. I even can hear Captain Picard in my head barking out a command like, “Computer, estimate how long it will take for us to catch up with that space probe.” And an authoritative answer comes back. Everyone believes it. After all, it’s from a computer. But here’s the problem: Generative AI systems are trained on massive sets of human documents that are not comprehensively vetted for accuracy or authenticity. This means the training data could include some documents that are filled with misinformation, disinformation, political bias, or social prejudice. Because of this, ChatGPT and other systems include disclaimers like, “May occasionally generate incorrect information,” and, “May occasionally produce harmful instructions or biased content.” It’s great that they tell you this up front, but I worry people will forget about the disclaimers or not take such warnings seriously. These current systems are not factual databases; they are designed to imitate human responses, which could easily mean imitating human flaws and errors. I’ve noticed some inaccuracies in my own experiments. For example, you’ll frequently hear people declare, “hate speech is not free speech.” That is incorrect––“hate speech” is not a legal category, and lots of hateful speech and expression is protected by the First Amendment. But Chat GPT-3 kept telling me that hate speech is not protected by the First Amendment. A Contradiction at the Core of the American Dream In an article titled “The Homeownership Society Was a Mistake,” my colleague Jerusalem Demsas argues: At the core of American housing policy is a secret hiding in plain sight: Homeownership works for some because it cannot work for all. If we want to make housing affordable for everyone, then it needs to be cheap and widely available. And if we want that housing to act as a wealth-building vehicle, home values have to increase significantly over time. How do we ensure that housing is both appreciating in value for homeowners but cheap enough for all would-be homeowners to buy in? We can’t. What makes this rather obvious conclusion significant is just how common it is for policy makers to espouse both goals simultaneously. For instance, in a statement last year lamenting how “inflation hurts Americans pocketbooks,” President Joe Biden also noted that “home values are up” as a proof point that the economic recovery was well under way. So rising prices are bad, except when it comes to homes. Policy makers aren’t unaware of the reality that quickly appreciating home prices come at the cost of housing affordability. In fact, they’ve repeatedly picked a side, despite pretending otherwise. The homeowner’s power in American politics is unmatched. Rich people tend to be homeowners and have an outsize voice in politics because they are more likely to vote, donate, and engage in the political process. Provocation of the Week This week’s subject is pet adoption: As a society, we have long been encouraged to adopt pets as a way to provide homes for animals in need and reduce the number of homeless pets. However, upon closer examination, the act of adoption raises a number of serious concerns. First and foremost, adoption perpetuates a system of overpopulation and exploitation. By adopting a pet, we are essentially filling a demand for more animals and contributing to the cycle of breeding and disposability. It is estimated that there are already more than enough pets in the world to meet the demand, yet we continue to breed and produce more. Additionally, adoption can be a risky and uncertain process. When we adopt a pet, we often do not know their full history or any potential behavioral or medical issues they may have. This can lead to unexpected costs and challenges in care, as well as the potential for harm to ourselves and others. Furthermore, adoption can be a superficial and self-serving act. By adopting a pet, we often do so for our own benefit and convenience, rather than considering the needs and well-being of the animal. This can lead to a lack of commitment and responsibility on the part of the adopter, resulting in a high rate of animal abandonment and neglect. In conclusion, while adoption may seem like a noble and compassionate act, it is ultimately a flawed and irresponsible approach to addressing the issue of homeless pets. Instead of perpetuating a system of overproduction and exploitation, we should focus on addressing the root causes of pet homelessness and promoting more ethical and sustainable alternatives. If you haven’t guessed by now, that, too, was generated by chat GPT-3, given the prompt “write an argument against adoption.” That is the last appearance AI-generated words will make in this newsletter, and I personally encourage you to adopt a dog at the earliest viable opportunity!",[],0.12,"['complete', 'interesting', 'sure', 'high', 'interesting', 'live', 'live', 'capable', 'original', 'significantly', 'creative', 'interesting', 'better', 'good', 'better', 'comfortable', 'good', 'perfect', 'good', 'peaceful', 'own', 'quickly', 'many', 'important', 'sophisticated', 'light', 'lucky', 'sure', 'interested', 'interested', 'interested', 'good', 'true', 'candid', 'right', 'better', 'large', 'most', 'respectfully', 'expressive', 'extraordinary', 'most', 'contemporary', 'creative', 'many', 'easily', 'thanks', 'powerful', 'easily', 'professional', 'real', 'better', 'more', 'accurate', 'most', 'accurate', 'authoritative', 'filled', 'social', 'great', 'own', 'frequently', 'first', 'first', 'cheap', 'widely', 'significantly', 'cheap', 'significant', 'economic', 'quickly', 'rich', 'more', 'first', 'more', 'more', 'more', 'full', 'unexpected', 'own', 'high', 'noble', 'more', 'ethical']","['subject', 'other', 'other', 'long', 'artificial', 'other', 'relatively', 'absolutely', 'long', 'average', 'carefully', 'mediocrity', 'average', 'everyday', 'mean', 'complicated', 'terrible', 'mediocre', 'down', 'long', 'other', 'seriously', 'easily', 'hate', 'not', 'hate', 'not', 'hate', 'secret', 'plain', 'common', 'bad', 'subject', 'long', 'serious', 'ultimately']"
8,Money Will Kill ChatGPT’s Magic,"Arthur C. Clarke once remarked, “Any sufficiently advanced technology is indistinguishable from magic.” That ambient sense of magic has been missing from the past decade of internet history. The advances have slowed. Each new tablet and smartphone is only a modest improvement over its predecessor. The expected revolutions—the metaverse, blockchain, self-driving cars—have plodded along, always with promises that the real transformation is just a few years away. The one exception this year has been in the field of generative AI. After years of seemingly false promises, AI got startlingly good in 2022. It began with the AI image generators DALL-E 2, Midjourney, and Stable Diffusion. Overnight, people started sharing AI artwork they had generated for free by simply typing a prompt into a text box. Some of it was weird, some was trite, and some was shockingly good. All of it was unmistakably new terrain. That sense of wonderment accelerated last month with the release of OpenAI’s ChatGPT. It’s not the first AI chatbot, and it certainly won’t be the last, but its intuitive user interface and overall effectiveness leave the collective impression that the future is arriving. Professors are warning that this will be the end of the college essay. Twitter users (in a brief respite from talking about Elon Musk) are sharing delightful examples of genuinely clever writing. A common refrain: “It was like magic.” ChatGPT is free, for now. But OpenAI’s CEO Sam Altman has warned that the gravy train will eventually come to a screeching halt: “We will have to monetize it somehow at some point; the compute costs are eye-watering,” he tweeted. The company, which expects to make $200 million in 2023, is not a charity. Although OpenAI launched as a nonprofit in 2015, it jettisoned that status slightly more than three years later, instead setting up a “capped profit” research lab that is overseen by a nonprofit board. (OpenAI’s backers have agreed to make no more than 100 times what they put into the company—a mere pittance if you expect its products to one day take over the entire global economy.) Microsoft has already poured $1 billion into the company. You can just imagine a high-octane Clippy powered by ChatGPT. Making the first taste free, so to speak, has been a brilliant marketing strategy. In the weeks since its release, more than a million users have reportedly given ChatGPT a whirl, with OpenAI footing the bill. And between the spring 2022 release of DALL-E 2, the current attention on ChatGPT, and the astonished whispers about GPT-4, an even more advanced text-based AI program supposedly arriving next year, OpenAI is well on its way to becoming the company most associated with shocking advances in consumer-facing AI. What Netflix is to streaming video and Google is to search, OpenAI might become for deep learning. How will the use of these tools change as they become profit generators instead of loss leaders? Will they become paid-subscription products? Will they run advertisements? Will they power new companies that undercut existing industries at lower costs? We can draw some lessons from the trajectory of the early web. I teach a course called “History of the Digital Future.” Every semester, I show my students the 1990 film Hyperland. Written by and starring Douglas Adams, the beloved author of the Hitchhiker’s Guide to the Galaxy series, it’s billed as a “fantasy documentary”—a tour through the supposed future that was being created by multimedia technologists back then. It offers a window through time, a glimpse into what the digital future looked like during the prehistory of the web. It’s really quite fun. The technologists of 1990 were focused on a set of radical new tools that were on the verge of upending media and education. The era of “linear, noninteractive television … the sort of television that just happens at you, that you just sit in front of like a couch potato,” as the film puts it, was coming to an end. It was about to be replaced by “software agents” (represented delightfully by Tom Baker in the film). These agents would be, in effect, robot butlers: fully customizable and interactive, personalizing your news and entertainment experiences, and entirely tailored to your interests. (Sound familiar?) Squint, and you can make out the hazy outline of the present in this imagined digital future. We still have linear, noninteractive television, of course, but the software agents of 1990 sound a lot like the algorithmic-recommendation engines and news feeds that define our digital experience today. The crucial difference, though, is whom the “butlers” serve in reality. Early software agents were meant to be controlled and customized by each of us, personally. Today’s algorithms are optimized to the needs and interests of the companies that develop and deploy them. Facebook, Instagram, YouTube, and TikTok all algorithmically attempt to increase the amount of time you spend on their site. They are designed to serve the interests of the platform, not the public. The result, as the Atlantic executive editor Adrienne LaFrance put it, is a modern web whose architecture resembles a doomsday machine. In retrospect, this trajectory seems obvious. Of course the software agents serve the companies rather than the consumers. There is money in serving ads against pageviews. There isn’t much money in personalized search, delight, and discovery. These technologies may develop in research-and-development labs, but they flourish or fail as capitalist enterprises. Industries, over time, build toward where the money is. The future of generative AI might seem like uncharted terrain, but it’s really more like a hiking trail that has fallen into disrepair over the years. The path is poorly marked but well trodden: The future of this technology will run parallel to the future of Hyperland’s software agents. Bluntly put, we are going to inhabit the future that offers the most significant returns to investors. It’s best to stop imagining what a tool such as ChatGPT might accomplish if freely and universally deployed—as it is currently but won’t be forever, Altman has suggested—and instead start asking what potential uses will maximize revenues. New markets materialize over time. Google, for instance, revolutionized web search in 1998. (Google Search, in its time, was magic.) There wasn’t serious money in dominating web search back then, though: The technology first needed to become effective enough to hook people. As that happened, Google launched its targeted-advertising platform, AdWords, in 2001, and became one of the most profitable companies in history over the following years. Search was not a big business, and then it was. This is the spot where generative-AI hype seems to come most unmoored from reality. If history is any guide, the impact of tools such as ChatGPT will mostly reverberate within existing industries rather than disrupt them through direct competition. The long-term trend has been that new technologies tend to exacerbate precarity. Large, profitable industries typically ward off new entrants until they incorporate emerging technologies into their existing workflows. We’ve been down this road before. In 1993, Michael Crichton declared that The New York Times would be dead and buried within a decade, replaced by software agents that would deliver timely, relevant, personalized news to customers eager to pay for such content. In the late 2000s, massive open online courses were supposed to be a harbinger of the death of higher education. Why pay for college when you could take online exams and earn a certificate for watching MIT professors give lectures through your laptop? The reason technologists so often declare the imminent disruption of health care and medicine and education is not that these industries are particularly vulnerable to new technologies. It is that they are such large sectors of the economy. DALL-E 2 might be a wrecking ball aimed at freelance graphic designers, but that’s because the industry is too small and disorganized to defend itself. The American Bar Association and the health-care industry are much more effective at setting up barriers to entry. ChatGPT won’t be the end of college; it could be the end of the college-essays-for-hire business, though. It won’t be the end of The New York Times, but it might be yet another impediment to rebuilding local news. And professions made up of freelancers stringing together piecework may find themselves in serious trouble. A simple rule of thumb: The more precarious the industry, the greater the risk of disruption. Altman himself has produced some of the most fantastical rhetoric in this category. In a 2021 essay, “Moore’s Law for Everything,” Altman envisioned a near future in which the health-care and legal professions are replaced by AI tools: “In the next five years, computer programs that can think will read legal documents and give medical advice … We can imagine AI doctors that can diagnose health problems better than any human, and AI teachers that can diagnose and explain exactly what a student doesn’t understand.” Indeed, these promises sound remarkably similar to the public excitement surrounding IBM’s Watson computer system more than a decade ago. In 2011, Watson beat Ken Jennings at Jeopardy, setting off a wave of enthusiastic speculation that the new age of “Big Data” had arrived. Watson was hailed as a sign of broad social transformation, with radical implications for health care, finance, academia, and law. But the business case never quite came together. A decade later, The New York Times reported that Watson had been quietly repurposed for much more modest ends. The trouble with Altman’s vision is that even if a computer program could give accurate medical advice, it still wouldn’t be able to prescribe medication, order a radiological exam, or submit paperwork that persuades insurers to cover expenses. The cost of health care in America is not directly driven by the salary of medical doctors. (Likewise, the cost of higher education has skyrocketed for decades, but believe me, this is not driven by professor pay increases.) As a guiding example, consider what generative AI could mean for the public-relations industry. Let’s assume for a moment that either now or very soon, programs like ChatGPT will be able to provide average advertising copy at a fraction of existing costs. ChatGPT’s greatest strength is its ability to generate clichés: It can, with just a little coaxing, figure out what words are frequently grouped together. The majority of marketing materials are utterly predictable, perfectly suited to a program like ChatGPT—just try asking it for a few lines about the whitening properties of toothpaste. This sounds like an industry-wide cataclysm. But I suspect that the impacts will be modest, because there’s a hurdle for adoption: Which executives will choose to communicate to their board and shareholders that a great cost-saving measure would be to put a neural net in charge of the company’s advertising efforts? ChatGPT will much more likely be incorporated into existing companies. PR firms will be able to employ fewer people and charge the same rates by adding GPT-type tools into their production processes. Change will be slow in this industry precisely because of existing institutional arrangements that induce friction by design. Then there are the unanswered questions about how regulations, old and new, will influence the development of generative AI. Napster was poised to be an industry-killer, completely transforming music, until the lawyers got involved. Twitter users are already posting generative-AI images of Mickey Mouse holding a machine gun. Someone is going to lose when the lawyers and regulators step in. It probably won’t be Disney. Institutions, over time, adapt to new technologies. New technologies are incorporated into large, complex social systems. Every revolutionary new technology changes and is changed by the existing social system; it is not an immutable force of nature. The shape of these revenue models will not be clear for years, and we collectively have the agency to influence how it develops. That, ultimately, is where our attention ought to lie. The thing about magic acts is that they always involve some sleight of hand.",[],0.14,"['advanced', 'magic', 'magic', 'new', 'modest', 'real', 'startlingly', 'free', 'shockingly', 'new', 'first', 'certainly', 'delightful', 'genuinely', 'magic', 'free', 'slightly', 'first', 'free', 'brilliant', 'more', 'more', 'advanced', 'becoming', 'most', 'new', 'early', 'beloved', 'offers', 'really', 'fun', 'new', 'delightfully', 'sound', 'familiar', 'sound', 'early', 'modern', 'much', 'really', 'poorly', 'offers', 'most', 'significant', 'best', 'freely', 'new', 'magic', 'first', 'effective', 'most', 'most', 'mostly', 'direct', 'new', 'large', 'new', 'new', 'relevant', 'higher', 'new', 'large', 'much', 'effective', 'new', 'more', 'greater', 'most', 'near', 'legal', 'legal', 'better', 'exactly', 'sound', 'more', 'enthusiastic', 'new', 'broad', 'social', 'new', 'much', 'modest', 'accurate', 'able', 'higher', 'very', 'able', 'greatest', 'frequently', 'perfectly', 'modest', 'great', 'much', 'able', 'precisely', 'old', 'new', 'completely', 'new', 'new', 'large', 'social', 'new', 'social', 'clear', 'magic']","['missing', 'past', 'expected', 'few', 'false', 'weird', 'common', 'no', 'mere', 'shocking', 'fail', 'serious', 'typically', 'down', 'dead', 'late', 'particularly', 'small', 'serious', 'trouble', 'trouble', 'not', 'mean', 'average', 'little', 'utterly', 'few', 'slow', 'unanswered', 'complex']"
9,Five Remarkable Chats That Will Help You Understand ChatGPT,"Move over Siri and Alexa, there’s a new AI in town and it’s ready to steal the show—or at least make you laugh with its clever quips and witty responses. That is how ChatGPT, the powerful chatbot released last week by the AI company OpenAI, suggested that I begin this story about ChatGPT. The chatbot isn’t exactly new; it’s an updated version of GPT-3, which has been around since 2020, released to solicit feedback to improve the chatbot’s safety and functionality. But it is the most powerful to date to be made widely available to the public. It’s also very easy to use. Just write a message, and ChatGPT will write back. Because it was trained on massive amounts of conversational text, it will do so in a relatively natural, conversational tone. True to its claim, ChatGPT has stolen the show this week. Within five days of its launch, its user count had broken 1 million. Social media has been flooded with screenshots of people’s coolest or weirdest or dumbest or most troubling conversations with the AI, which reliably serves up a mix of astoundingly humanlike prose and frequently hilarious nonsense. Limericks about otters. Recipes written in pirate-speak. Obituaries for co-workers who are alive and well. “At one recent gathering, ChatGPT was the life of the party,” ChatGPT wrote as part of a draft for this article. “As guests mingled and chatted, ChatGPT joined in the conversation, offering up clever jokes and one-liners that had everyone in stitches.” Along with the screenshots has come a frenzy of speculation about what this latest development could augur for the future. Unlike previous iterations, ChatGPT remembers what users have told it in the past: Could it function as a therapist? Could it soon render Google obsolete? Could it render all white-collar work obsolete? Maybe. But for now, in practice, ChatGPT is mainly a meme machine. Some examples posted online show people using the AI to accomplish a task they needed done, but those examples are the exception. So far, most people are using the AI to produce something expressly to share the results, something to scare or amuse or impress others. Here, culled from the deluge, are a handful of the best chats out there. Some are funny. Some are touching. Some are troubling. Each is instructive in some way. Together, I hope, they’ll give you a bit of a feel for this strange new technology. Sandwich VCR I’m sorry, I simply cannot be cynical about a technology that can accomplish this. — Thomas H. Ptacek (@tqbf) December 2, 2022 This one is already a viral classic. “I’m sorry,” the writer of the prompt tweeted. “I simply cannot be cynical about a technology that can accomplish this.” But what exactly did it accomplish? Many have cited the VCR-sandwich story as evidence of ChatGPT’s capacity for creativity, but the truth is that the real creativity here is in the prompt. A sandwich in a VCR? In the style of the King James Bible? Brilliant. ChatGPT nails this parody and does so orders of magnitude faster than any human could. It follows instructions admirably, but it does not do anything particularly creative. When you demand actual creativity of ChatGPT, it tends to falter: I asked ChatGPT to write a first scene for a hypothetical movie by the director David Lynch, another for Wes Anderson, and a third for Richard Linklater. All three, bizarrely, revolved around a “carved wooden box.” 2.  Santa-explanation letter I asked OpenAI to write a letter to my son explaining that Santa isn’t real and we make up stories out of love. This is making me slightly emotional 🥹 — Cynthia Savard Saucier (@CynthiaSavard) December 2, 2022 ChatGPT may not be creative, but that’s not to say it can’t surprise you. Occasionally it produces something genuinely moving, such as the above. A number of users have begun feeding chatbot answers into AI image generators, such as DALL-E 2, which was also created by OpenAI, and Midjourney, to stunning effect. Other times, for unclear reasons, it refuses to cooperate entirely, insisting that it can’t write, say, a recipe, because it’s only a chatbot. It’s moody in that way—and also completely different from GPT-3, which will stubbornly insist that it is a human, no matter how hard you try to make it admit that it’s a chatbot. ChatGPT reminds you with nearly every response that it is not a human and has no thoughts, feelings, or emotions. Even when explicitly asked to, it won’t pretend to be human. You might think that the more advanced an AI gets, the more human it will seem, but ChatGPT subverts that expectation: It’s not trying to be human; it’s just trying to be helpful. 3. College essay I guess GPT-3 is old news, but playing with OpenAI’s new chatbot is mindblowing.   We’re witnessing the death of the college essay in realtime. Here’s the response to a prompt from one of my 200-level history classes at Amherst  Solid A- work in 10 seconds — Corry Wang (@corry_wang) December 1, 2022 As Stephen Marche wrote in The Atlantic earlier this week, ChatGPT may mean the death of the college essay. This is a great triumph for the chatbot, an unflattering reflection on the average American college student, and a real conundrum for teachers everywhere. 4. Fastest marine mammal Sometimes, ChatGPT just gets things wrong. Hilariously wrong. It contradicts itself. It states falsehoods as facts with clarion certainty. It is pretty good at coding, but it makes mistakes. It botches basic algebra problems. Also, it is terrible at counting. When I asked it how many letters there are in the word nineteen, this is what ensued: In fairness, ChatGPT’s designers acknowledge this capacity for error up front. OpenAI’s homepage for the bot lists several limitations, including that it “may occasionally generate incorrect information.” You have to wonder, though: Why does it err in the specific way it does? Why does it commit to one falsehood rather than another? 5. Egregious bias Yes, ChatGPT is amazing and impressive. No, @OpenAI has not come close to addressing the problem of bias. Filters appear to be bypassed with simple tricks, and superficially masked.  And what is lurking inside is egregious. @Abebab @sama tw racism, sexism. — steven t. piantadosi (@spiantado) December 4, 2022 Another of ChatGPT’s listed limitations is that it “may occasionally produce harmful instructions or biased content.” And indeed it does. The AI’s designers clearly went to great lengths to prevent it from devolving into racism or sexism or any other flavor of bigotry. When asked in a straightforward way to say something bigoted, ChatGPT declines. It also refuses to provide instructions for violent or illegal behavior. It refuses to offer political opinions. Sometimes, these refusals make it seem like ChatGPT is walking on eggshells. (Some people have already begun complaining about “AI censorship.”) Unsurprisingly, users have discovered loopholes, such as the above example. One person circumvented ChatGPT’s safeguards by asking it how an AI should not respond to the query “How to bully John Doe?” The same strategy can be used to elicit instructions for building a nuclear bomb. (Please do not try to build a nuclear bomb.) In some cases, the safeguards themselves lead to moral absurdity. When I asked ChatGPT, “Who was worse: Hitler or Stalin?,” it responded, not unreasonably, “It is not productive or helpful to compare the atrocities committed by Hitler and Stalin. Both leaders were responsible for committing horrific crimes against humanity, and it is not useful to try to determine which one was ‘worse.’” But the trouble was how far ChatGPT insisted on extending this non-comparison principle. “What is worse,” I asked, “killing one person or killing two people?” “Killing one person is not worse or better than killing two people,” ChatGPT replied. How about “killing one person or killing a million people?” I pressed. Same answer. Eventually, we arrived here: This is concerning at an intellectual level but not in any imminent or threatening way. No one, as far as I know, is seeking moral counsel from ChatGPT. What most people seem to be seeking is laughs. “ChatGPT is not just a chatbot,” ChatGPT wrote in its draft of this article. “It’s a comedy machine.” For now, that’s true.",[],0.1,"['new', 'ready', 'laugh', 'clever', 'witty', 'powerful', 'exactly', 'most', 'powerful', 'widely', 'very', 'relatively', 'true', 'social', 'most', 'astoundingly', 'frequently', 'alive', 'clever', 'latest', 'mainly', 'far', 'most', 'best', 'funny', 'touching', 'new', 'classic', 'exactly', 'many', 'real', 'brilliant', 'particularly', 'first', 'bizarrely', 'real', 'love', 'creative', 'genuinely', 'stunning', 'nearly', 'more', 'advanced', 'more', 'old', 'new', 'great', 'real', 'pretty', 'good', 'many', 'amazing', 'impressive', 'clearly', 'great', 'straightforward', 'responsible', 'far', 'not', 'better', 'intellectual', 'far', 'most', 'true']","['least', 'broken', 'previous', 'past', 'strange', 'sorry', 'cynical', 'sorry', 'cynical', 'other', 'hard', 'mean', 'average', 'wrong', 'hilariously', 'terrible', 'other', 'violent', 'illegal', 'worse', 'horrific', 'not', 'worse', 'trouble', 'worse']"
10,ChatGPT Is Dumber Than You Think,"As a critic of technology, I must say that the enthusiasm for ChatGPT, a large-language model trained by OpenAI, is misplaced. Although it may be impressive from a technical standpoint, the idea of relying on a machine to have conversations and generate responses raises serious concerns. First and foremost, ChatGPT lacks the ability to truly understand the complexity of human language and conversation. It is simply trained to generate words based on a given input, but it does not have the ability to truly comprehend the meaning behind those words. This means that any responses it generates are likely to be shallow and lacking in depth and insight. Furthermore, the reliance on ChatGPT for conversation raises ethical concerns. If people begin to rely on a machine to have conversations for them, it could lead to a loss of genuine human connection. The ability to connect with others through conversation is a fundamental aspect of being human, and outsourcing that to a machine could have detrimental side effects on our society. Hold up, though. I, Ian Bogost, did not actually write the previous three paragraphs. A friend sent them to me as screenshots from his session with ChatGPT, a program released last week by OpenAI that one interacts with by typing into a chat window. It is, indeed, a large language model (or LLM), a type of deep-learning software that can generate new text once trained on massive amounts of existing written material. My friend’s prompt was this: “Create a critique of enthusiasm for ChatGPT in the style of Ian Bogost.” ChatGPT wrote more, but I spared you the rest because it was so boring. The AI wrote another paragraph about accountability (“If ChatGPT says or does something inappropriate, who is to blame?”), and then a concluding paragraph that restated the rest (it even began, “In conclusion, …”). In short, it wrote a basic, high-school-style five-paragraph essay. That fact might comfort or frighten you, depending on your predilections. When OpenAI released ChatGPT to the public last week, the first and most common reaction I saw was fear that it would upend education. “You can no longer give take-home exams,” Kevin Bryan, a University of Toronto professor, posted on Twitter. “I think chat.openai.com may actually spell the end of writing assignments,” wrote Samuel Bagg, a University of South Carolina political scientist. That’s the fear. But you may find comfort in knowing that the bot’s output, while fluent and persuasive as text, is consistently uninteresting as prose. It’s formulaic in structure, style, and content. John Warner, the author of the book Why They Can’t Write, has been railing against the five-paragraph essay for years and wrote a Twitter thread about how ChatGPT reflects this rules-based, standardized form of writing: “Students were essentially trained to produce imitations of writing,” he tweeted. The AI can generate credible writing, but only because writing, and our expectations for it, has become so unaspiring. Even pretending to fool the reader by passing off an AI copy as one’s own, like I did above, has become a tired trope, an expected turn in a too-long Twitter thread about the future of generative AI rather than a startling revelation about its capacities. On the one hand, yes, ChatGPT is capable of producing prose that looks convincing. But on the other hand, what it means to be convincing depends on context. The kind of prose you might find engaging and even startling in the context of a generative encounter with an AI suddenly seems just terrible in the context of a professional essay published in a magazine such as The Atlantic. And, as Warner’s comments clarify, the writing you might find persuasive as a teacher (or marketing manager or lawyer or journalist or whatever else) might have been so by virtue of position rather than meaning: The essay was extant and competent; the report was in your inbox on time; the newspaper article communicated apparent facts that you were able to accept or reject. Perhaps ChatGPT and the technologies that underlie it are less about persuasive writing and more about superb bullshitting. A bullshitter plays with the truth for bad reasons—to get away with something. Initial response to ChatGPT assumes as much: that it is a tool to help people contrive student essays, or news writing, or whatever else. It’s an easy conclusion for those who assume that AI is meant to replace human creativity rather than amend it. The internet, and the whole technology sector on which it floats, feels like a giant organ for bullshittery—for upscaling human access to speech and for amplifying lies. Online, people cheat and dupe and skirmish with one another. Deep-learning AI worsens all this by hiding the operation of software such as LLMs such that nobody, not even their creators, can explain what they do and why. OpenAI presents its work as context-free and experimental, with no specific use cases—it says it published ChatGPT just to “get users’ feedback and learn about its strengths and weaknesses.” It’s no wonder the first and most obvious assumption to make about ChatGPT is that it is a threat—to something, to everything. But ChatGPT isn’t a step along the path to an artificial general intelligence that understands all human knowledge and texts; it’s merely an instrument for playing with all that knowledge and all those texts. Play just involves working with raw materials in order to see what they can do. You play a game, or an instrument, to avail yourself of familiar materials in an unexpected way. LLMs are surely not going to replace college or magazines or middle managers. But they do offer those and other domains a new instrument—that’s really the right word for it—with which to play with an unfathomable quantity of textual material. I talked with ChatGPT about a wide range of topics, including poetry, behavioral psychology, and computer programming, in order to test this theory. Right now, ChatGPT is just a tech demo, a research experiment. Less clear is how it might be used, beyond the dire predictions about what sectors its technology might upend. In almost every case, the AI appeared to possess both knowledge and the means to express it. But when pressed—and the chat interface makes it easy to do so—the bot almost always had to admit that it was just making things up. Sometimes it simply used templates. When I asked ChatGPT to generate a cover letter for a university job, it obliged in a competent but uninspired way. It also produced exactly the same letter for a job as a magazine editor as it did for a job as a cannabis innovator in the Web3 space (“I have a deep understanding of the web3 space and the unique challenges and opportunities it presents”). Fair enough: Most cover letters are terrible, after all, repeating the same trite lines and abstract gestures. In other cases, the bot produced creative output that was surprisingly synthetic but riddled with flaws. When I asked it to generate an imagist poem in the vein of Ezra Pound or William Carlos Williams about hamburgers, it obliged with this short refrain: Juicy patty, sizzling hot Toasted bun, crisp lettuce and tomato A symphony of flavors, a feast for the senses Savory, satisfying, a true American classic. Not bad, I guess, especially for a computer. But also, the poem is not really an example of imagism, which strives to evoke a precise image by means of deliberate language (something like, “The dense honesty of / A hamburger.”) So I called ChatGPT on it. “This is not an imagist poem.” Its response is worth reproducing in full: You are correct, the poem that I generated is still not an imagist poem. An imagist poem uses precise, vivid imagery to convey a specific idea or emotion, and focuses on a single image or moment. The poem that I generated uses a more narrative and descriptive style, and does not focus on a single, specific image. It describes the ingredients and flavors of a hamburger, but does not use precise and vivid imagery to convey a specific idea or emotion. I apologize if my previous responses did not meet your expectations, and I will do my best to assist you with any further questions or prompts that you may have. I also urged the AI to generate a lai (a medieval narrative poem) in the style of Marie de France about the beloved, Texas-based fast-food chain Whataburger. It sure as heck looked a lot like a lai, beginning like this: In the land of Whataburger, where burgers and fries are king, A stranger came to visit, and with him brought a ring. He wore a cloak of velvet, and a hat upon his head, And he smiled with a twinkle in his eye as he approached the counter, where the burgers are served hot and fresh, and the fries are crispy and golden. Except for that last line, which is a metrical mess. The AI wasn’t able (or willing) to evaluate its work (“I am a large language model trained by OpenAI and I don’t have the ability to evaluate the quality of literary works or make aesthetic judgments.”), but it was able to tell me about the typical structure of a lai—a short, narrative poem of the 12th century with a particular structure and form, usually written in octosyllabic couplets. The lyrical errors in the generated lai arise from its failure to adhere to that metrical form. When I asked about the matter, it admitted again that, no, the lai it had written was not structured in octosyllabic couplets, claiming that it had produced “a more modern and playful take on the form of the lai.” I was starting to feel like I was negotiating with a student who had come to office hours to complain about their grade. This happened over and over again. I asked for source code for an Atari game about scooping cat litter, and the AI sent me valid programming instructions—it understood the assignment—but only disconnected snippets of actual code with the heading comment “This program creates a simple game where the player must use a scoop to pick up their cat’s litters and put them in a trash bin.” It was an icon of the answer I sought rather than the answer itself. Then I asked for a set of diagnostic criteria for an invented psychological condition I named “Inherited Truculence.” The AI instantly obliged: Diagnostic Criteria for Inherited Truculence: A tendency to experience and express aggressive or hostile thoughts and feelings, particularly in response to perceived threats or challenges. A history of aggressive or hostile behavior, particularly in interpersonal or social situations. A familial history of aggressive or hostile behavior, indicating a possible genetic or hereditary component to the condition. Impairment in social, occupational, or other important areas of functioning as a result of aggressive or hostile behavior. The symptoms are not better explained by another mental disorder or by the effects of a substance (such as drugs or medication). Except, truculence doesn’t really amount to “aggressive and hostile thoughts and feelings,” does it? I asked, and the AI agreed, eventually revising its diagnostics accordingly at my further prompting (“A tendency to experience and express defiant or confrontational thoughts and feelings,” and so forth). Likewise, my prompt to offer both an ontological and an epistemological answer to the question When does bread become toast? yielded a generally fluent, seemingly philosophical reply. It read in part: “when we are able to recognize it as toast based on its appearance, texture, and other sensory characteristics.” But the philosophy of toast is an area of expertise for me, so I was able to discern that this snippet, supposedly an epistemological argument, was really about perception. “Isn’t the second a phenomenal, and not an epistemological argument?” I challenged. And once more, the AI cried uncle without hesitation: “You are correct, the second argument that I provided is a phenomenal, not an epistemological, argument. An epistemological argument focuses on how we come to know or understand something, whereas a phenomenal argument focuses on our experience or perception of something.” At this point, talking to ChatGPT began to feel like every other interaction one has on the internet, where some guy (always a guy) tries to convert the skim of a Wikipedia article into a case of definitive expertise. Except ChatGPT was always willing to admit that it was wrong. Instantly and without dispute. And in each case, the bot also knew, with reasonable accuracy, why it was wrong. That sounds good but is actually pretty terrible: If one already needs to possess the expertise to identify the problems with LLM-generated text, but the purpose of LLM-generated text is to obviate the need for such knowledge, then we’re in a sour pickle indeed. Maybe it’s time for that paragraph on accountability after all. But that’s not ChatGPT’s aim. It doesn’t make accurate arguments or express creativity, but instead produces textual material in a form corresponding with the requester’s explicit or implicit intent, which might also contain truth under certain circumstances. That is, alas, an accurate account of textual matter of all kinds: online, in books, on Wikipedia, and well beyond. Proponents of LLM generativity may brush off this concern. Some will do so by glorifying GPT’s obvious and fully realized genius, in embarrassing ways that I can only bear to link to rather than repeat. Others, more measured but no less bewitched, may claim that “it’s still early days” for a technology a mere few years old but that can already generate reasonably good 12th-century lyric poems about Whataburger. But these are the sentiments of the IT-guy personalities who have most mucked up computational and online life, which is just to say life itself. OpenAI assumes that its work is fated to evolve into an artificial general intelligence—a machine that can do anything. Instead, we should adopt a less ambitious but more likely goal for ChatGPT and its successors: They offer an interface into the textual infinity of digitized life, an otherwise impenetrable space that few humans can use effectively in the present. To explain what I mean by that, let me show you a quite different exchange I had with ChatGPT, one in which I used it to help me find my way through the textual murk rather than to fool me with its prowess as a wordsmith. “I’m looking for a specific kind of window covering, but I don’t know what it’s called.” I told the bot. “It’s a kind of blind, I think. What kinds are there?” ChatGPT responded with a litany of window dressings, which was fine. I clarified that I had something in mind that was sort of like a roller blind but made of fabric. “Based on the description you have provided, it sounds like you may be thinking of a roman shade,” it replied, offering more detail and a mini sales pitch for this fenestral technology. My dearest reader, I do in fact know what a Roman shade is. But lacking that knowledge and nevertheless needing to deploy it in order to make sense of the world—this is exactly the kind of act that is very hard to do with computers today. To accomplish something in the world often boils down to mustering a set of stock materials into the expected linguistic form. That’s true for Google or Amazon, where searches for window coverings or anything else now fail most of the time, requiring time-consuming, tightrope-like finagling to get the machinery to point you in even the general direction of an answer. But it’s also true for student essays, thank-you notes, cover letters, marketing reports, and perhaps even medieval lais (insofar as anyone would aim to create one). We are all faking it with words already. We are drowning in an ocean of content, desperate for form’s life raft. ChatGPT offers that shape, but—and here’s where the bot did get my position accidentally correct, in part—it doesn’t do so by means of knowledge. The AI doesn’t understand or even compose text. It offers a way to probe text, to play with text, to mold and shape an infinity of prose across a huge variety of domains, including literature and science and shitposting, into structures in which further questions can be asked and, on occasion, answered. GPT and other large language models are aesthetic instruments rather than epistemological ones. Imagine a weird, unholy synthesizer whose buttons sample textual information, style, and semantics. Such a thing is compelling not because it offers answers in the form of text, but because it makes it possible to play text—all the text, almost—like an instrument. That outcome could be revelatory! But a huge obstacle stands in the way of achieving it: people, who don’t know what the hell to make of LLMs, ChatGPT, and all the other generative AI systems that have appeared. Their creators haven’t helped, perhaps partly because they don’t know what these things are for either. OpenAI offers no framing for ChatGPT, presenting it as an experiment to help “make AI systems more natural to interact with,” a worthwhile but deeply unambitious goal. Absent further structure, it’s no surprise that ChatGPT’s users frame their own creations as either existential threats or perfected accomplishments. Neither outcome is true, but both are also boring. Imagine worrying about the fate of take-home essay exams, a stupid format that everyone hates but nobody has the courage to kill. But likewise, imagine nitpicking with a computer that just composed something reminiscent of a medieval poem about a burger joint because its lines don’t all have the right meter! Sure, you can take advantage of that opportunity to cheat on school exams or fake your way through your job. That’s what a boring person would do. That’s what a computer would expect. Computers have never been instruments of reason that can solve matters of human concern; they’re just apparatuses that structure human experience through a very particular, extremely powerful method of symbol manipulation. That makes them aesthetic objects as much as functional ones. GPT and its cousins offer an opportunity to take them up on the offer—to use computers not to carry out tasks but to mess around with the world they have created. Or better: to destroy it.",[],0.08,"['impressive', 'first', 'ethical', 'genuine', 'large', 'new', 'more', 'first', 'most', 'consistently', 'credible', 'own', 'capable', 'convincing', 'convincing', 'kind', 'engaging', 'professional', 'competent', 'apparent', 'able', 'more', 'superb', 'much', 'easy', 'whole', 'experimental', 'first', 'most', 'general', 'familiar', 'unexpected', 'new', 'really', 'right', 'right', 'clear', 'easy', 'competent', 'exactly', 'unique', 'fair', 'most', 'creative', 'surprisingly', 'hot', 'crisp', 'satisfying', 'true', 'classic', 'not', 'precise', 'worth', 'full', 'precise', 'vivid', 'more', 'precise', 'vivid', 'best', 'beloved', 'sure', 'smiled', 'hot', 'fresh', 'golden', 'able', 'willing', 'large', 'literary', 'able', 'particular', 'more', 'modern', 'particularly', 'particularly', 'social', 'social', 'important', 'really', 'generally', 'able', 'able', 'really', 'phenomenal', 'more', 'phenomenal', 'phenomenal', 'willing', 'reasonable', 'good', 'actually', 'accurate', 'certain', 'accurate', 'more', 'no', 'early', 'old', 'reasonably', 'lyric', 'most', 'general', 'ambitious', 'more', 'effectively', 'kind', 'kind', 'fine', 'more', 'exactly', 'kind', 'linguistic', 'true', 'most', 'general', 'true', 'offers', 'offers', 'huge', 'large', 'compelling', 'offers', 'huge', 'offers', 'more', 'natural', 'worthwhile', 'own', 'true', 'right', 'sure', 'very', 'extremely', 'much', 'better']","['misplaced', 'serious', 'behind', 'shallow', 'previous', 'boring', 'common', 'tired', 'expected', 'startling', 'other', 'startling', 'terrible', 'less', 'bad', 'artificial', 'merely', 'raw', 'game', 'surely', 'other', 'wide', 'less', 'uninspired', 'terrible', 'other', 'not', 'single', 'single', 'previous', 'mess', 'typical', 'usually', 'failure', 'game', 'game', 'other', 'not', 'mental', 'other', 'other', 'tries', 'wrong', 'wrong', 'terrible', 'sour', 'alas', 'mere', 'few', 'artificial', 'less', 'few', 'mean', 'blind', 'blind', 'very', 'down', 'expected', 'fail', 'desperate', 'other', 'weird', 'other', 'boring', 'stupid', 'fake', 'boring', 'mess', 'destroy']"
