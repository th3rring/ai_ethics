,titles,body_contents,tags,sentiment_score,positive_words,negative_words
0,Opinion: The Challenge to Humanity From ChatGPT,"Henry Kissinger, Eric Schmidt and Daniel Huttenlocher are luminaries whose words deserve to be taken seriously (“ChatGPT Heralds an Intellectual Revolution,” op-ed, Feb. 25). But their central thesis, that a computer program could “transform the human cognitive process” in a way tantamount to the Enlightenment, is, to say the least, a stretch. Ever since Eliza in the 1960s, we have been easily impressed by a computer (or even a talking parrot) that responds to us in coherent sentences, no matter how superficial the mechanism is by which they are generated. The fascination with ChatGPT is predictable, but right now the public needs rationality and transparency, not science fiction. Computer scientists should be more forthright in demystifying chatbots and explaining the algorithms by which they work. Before us are impressive pattern-finding engines capable of discovering rich forms of structure embedded in the word sequences we use to communicate. Combined with a massive memory, they can fetch the right fragments of text relevant to a query and combine them into a coherent-sounding answer. This is a noteworthy achievement, but it is neither communication, language, nor knowledge assimilation. Prof. Bruno A. Olshausen University of California, Berkeley Mr. Kissinger and colleagues state that teachers will need to teach new skills to help students adapt to AI. I would argue that teachers still haven’t learned to teach effectively with earlier technology. Often, lessons with a digital element focus on the technology rather than the learning. We’ve had technology in our schools for over 40 years, yet we only switched to widespread use in classrooms when forced to by the pandemic. The far-reaching social implications of AI demand that we respond much faster to this new challenge. Prof. Catherine Robert University of Texas at Arlington I started reading the Journal when I was 26. I’m nearly 83 now. Never in my life have I read such a comprehensive, well thought-out and fascinating article in any publication as the one from Messrs. Kissinger, Schmidt and Huttenlocher. Peter Bosse Roseville, Calif. How can we be assured that this op-ed is written by Messrs. Kissinger, Schmidt and Huttenlocher rather than by generative AI? William V. Coleman Rydal, Pa. My grandson is a freshman in university. The professors advise students not to use ChatGPT when writing essays. How did that type of conversation work out with God and Adam?",[],0.19,"['intellectual', 'easily', 'coherent', 'right', 'more', 'impressive', 'capable', 'rich', 'right', 'relevant', 'new', 'effectively', 'social', 'much', 'new', 'nearly', 'fascinating']","['seriously', 'least', 'predictable', 'forced']"
1,Opinion: How ChatGPT’s AI Will Become Useful,"In the rudimentary days of videogames, I met the team that created the first multiplayer Formula 1 Grand Prix racing game. They had to alter the original code because they discovered almost every player at the start of the first race would turn his car around on the track and crash into the incoming traffic. I started to laugh, because that’s what I did too. Gives new meaning to the Facebook motto: Move fast and break things. That’s exactly what’s going on with the newfangled generative AI chatbots. Everyone’s trying to break them and show their limitations and downsides. It’s human nature. A New York Times reporter was “thoroughly creeped out” after using Microsoft Bing’s chatbot. Sounds as if someone needs reassignment to the society pages. In 2016 Microsoft had to shut down its experimental chatbot, Tay, after users turned it into what some called a “neo-Nazi sexbot.” Coders can’t test for everything, so they need thousands or millions banging away to find their flaws. Free testers. In the coming months, you’re going to hear a lot more about RLHF, reinforced learning from human feedback. Machine-learning systems scan large quantities of data on the internet but then learn by chatting with actual humans in a feedback loop to hone their skills. Unfortunately, some people are ruder than others. This is what destroyed Tay. So ChatGPT currently limits its human feedback training to paid contractors. That will eventually change. Windows wasn’t ready until version 3.0; generative AI will get there too. For now Microsoft’s solution is to limit users to six questions a session for the Bing chatbot, effectively giving each session an expiration date. This sounds eerily similar to the Tyrell Corporation’s Nexus-6 replicants from the 1982 movie “Blade Runner.” If I remember, that didn’t end well. Every time something new comes out, lots of people try to break it or foolishly try to find the edge, like jumping into the back seat of a self-driving Tesla. This is especially scary given the recent recall of 362,800 Teslas with faulty “Full Self-Driving” software. And, reminiscent of the “Can I confess something?” scene in “Annie Hall,” I’ve always wondered: If I drove my car straight into a brick wall, would the collision avoidance actually work? I’m too chicken to try. Every cyberattack is a lesson in breakage, like the 2015 hack of the Office of Personnel Management or the May 2021 ransomware shutdown of the Colonial Pipeline. Heck, Elon Musk’s X.com and Peter Thiel’s PayPal payment processors were initially so riddled with fraud that the media insisted e-commerce would never happen, naysaying what today is a $10 trillion business. Looking back, they were lucky they were attacked at an early stage when the stakes were much lower. But be warned that with generative AI, even if it’s too early, if developers can build something, they will. So best to shake out all the bugs and limitations and creep reporters out now before things roll out to the masses. Despite early glitches, useful things are coming. Search boxes aren’t very conversational. Using them is like grunting words to zero in on something you suspect exists. Now a more natural human interface can replace back-and-forth conversations with old-fashioned travel agents. Or stockbrokers. Or doctors. Once conversations are human enough, the Eleanor Rigby floodgates—Ah, look at all the lonely people—will open. Eldercare may be the first big generative AI hit. Instead of grandma talking to the TV, a chatbot can stand in. Remember the 2013 movie “Her,” with Joaquin Phoenix’s character falling in love with an online bot voiced by Scarlett Johansson? This will become reality soon, no question. Someone will build it and against all warnings, millions will use it. In fact, the aptly named Replika AI Companion has launched, although its programmers quickly turned off the “erotic roleplay” feature. Hmmm. It may take longer for “M3GAN,” this year’s movie thriller (I watched it as a comedy) to become reality. It’s about a robot companion for a child gone rogue. But products like this will happen. Mattel’s 2015 Hello Barbie, which would listen and talk to kids, eventually failed, but someone will get it right before long. The trick is not to focus on the downside, like so many do with DNA crime-solving or facial-recognition systems or even the idea that Russian ads on social networks can tip elections. Let’s face it, every new technology is the Full Employment Act for ethicists—and scolds. Instead, with generative AI, focus on the upside of conversational search, companions for the lonely, and eventually an education system custom tailored to each student. Each time, crowds will move fast and try to break things and expose the flaws. Embrace that as part of the path to the future.",[],0.13,"['first', 'grand', 'original', 'first', 'laugh', 'new', 'fast', 'exactly', 'new', 'experimental', 'free', 'more', 'large', 'ready', 'effectively', 'new', 'full', 'straight', 'lucky', 'early', 'much', 'early', 'best', 'early', 'useful', 'very', 'more', 'natural', 'first', 'love', 'aptly', 'quickly', 'erotic', 'right', 'many', 'social', 'new', 'full', 'fast']","['game', 'down', 'unfortunately', 'especially', 'chicken', 'lonely', 'failed', 'long', 'lonely']"
2,Opinion: ChatGPT Heralds an Intellectual Revolution,"A new technology bids to transform the human cognitive process as it has not been shaken up since the invention of printing. The technology that printed the Gutenberg Bible in 1455 made abstract human thought communicable generally and rapidly. But new technology today reverses that process. Whereas the printing press caused a profusion of modern human thought, the new technology achieves its distillation and elaboration. In the process, it creates a gap between human knowledge and human understanding. If we are to navigate this transformation successfully, new concepts of human thought and interaction with machines will need to be developed. This is the essential challenge of the Age of Artificial Intelligence. The new technology is known as generative artificial intelligence; GPT stands for Generative Pre-Trained Transformer. ChatGPT, developed at the OpenAI research laboratory, is now able to converse with humans. As its capacities become broader, they will redefine human knowledge, accelerate changes in the fabric of our reality, and reorganize politics and society. Generative artificial intelligence presents a philosophical and practical challenge on a scale not experienced since the beginning of the Enlightenment. The printing press enabled scholars to replicate each other’s findings quickly and share them. An unprecedented consolidation and spread of information generated the scientific method. What had been impenetrable became the starting point of accelerating query. The medieval interpretation of the world based on religious faith was progressively undermined. The depths of the universe could be explored until new limits of human understanding were reached. Generative AI will similarly open revolutionary avenues for human reason and new horizons for consolidated knowledge. But there are categorical differences. Enlightenment knowledge was achieved progressively, step by step, with each step testable and teachable. AI-enabled systems start at the other end. They can store and distill a huge amount of existing information, in ChatGPT’s case much of the textual material on the internet and a large number of books—billions of items. Holding that volume of information and distilling it is beyond human capacity. Sophisticated AI methods produce results without explaining why or how their process works. The GPT computer is prompted by a query from a human. The learning machine answers in literate text within seconds. It is able to do so because it has pregenerated representations of the vast data on which it was trained. Because the process by which it created those representations was developed by machine learning that reflects patterns and connections across vast amounts of text, the precise sources and reasons for any one representation’s particular features remain unknown. By what process the learning machine stores its knowledge, distills it and retrieves it remains similarly unknown. Whether that process will ever be discovered, the mystery associated with machine learning will challenge human cognition for the indefinite future. AI’s capacities are not static but expand exponentially as the technology advances. Recently, the complexity of AI models has been doubling every few months. Therefore generative AI systems have capabilities that remain undisclosed even to their inventors. With each new AI system, they are building new capacities without understanding their origin or destination. As a result, our future now holds an entirely novel element of mystery, risk and surprise. Enlightenment science accumulated certainties; the new AI generates cumulative ambiguities. Enlightenment science evolved by making mysteries explicable, delineating the boundaries of human knowledge and understanding as they moved. The two faculties moved in tandem: Hypothesis was understanding ready to become knowledge; induction was knowledge turning into understanding. In the Age of AI, riddles are solved by processes that remain unknown. This disorienting paradox makes mysteries unmysterious but also unexplainable. Inherently, highly complex AI furthers human knowledge but not human understanding—a phenomenon contrary to almost all of post-Enlightenment modernity. Yet at the same time AI, when coupled with human reason, stands to be a more powerful means of discovery than human reason alone. The essential difference between the Age of Enlightenment and the Age of AI is thus not technological but cognitive. After the Enlightenment, philosophy accompanied science. Bewildering new data and often counterintuitive conclusions, doubts and insecurities were allayed by comprehensive explanations of the human experience. Generative AI is similarly poised to generate a new form of human consciousness. As yet, however, the opportunity exists in colors for which we have no spectrum and in directions for which we have no compass. No political or philosophical leadership has formed to explain and guide this novel relationship between man and machine, leaving society relatively unmoored. ChatGPT is an example of what is known as a large language model, which can be used to generate human-like text. GPT is a type of model that can be automatically learned from large amounts of text without the need for human supervision. ChatGPT’s developers have fed it a massive amount of the textual content of the digital world. Computing power allows the model to capture patterns and connections. The ability of large language models to generate humanlike text was an almost accidental discovery. These models are trained to be able to predict the next word in a sentence, which is useful in tasks such as autocompletion for sending text messages or searching the web. But it turns out that the models also have the unexpected ability to create highly articulate paragraphs, articles and in time perhaps books. ChatGPT is further specialized beyond a basic large language model, using feedback from humans to tune the model so that it generates more natural-seeming conversational text, as well as to try to contain its propensity for inappropriate responses (a substantial challenge for large language models). ChatGPT instantaneously converts its representations into unique responses. The ultimate impression on a human conversant is that the AI is relating stationary collections of facts into dynamic concepts. ChatGPT’s answers, statements and observations appear without an explanation of where they came from and without an identifiable author. On its face, ChatGPT has no discernible motive or bias. Its outputs are complex, but its work is astonishingly rapid: In a matter of seconds, it can produce answers that coherently explain a high-level topic. They are not simply copied from the text in the computer’s memory. They are generated anew by a process that humans are unable to replicate. It is able to incorporate hypotheticals and nonobvious psychological inferences. It can prioritize among billions of data points to select the single set of 200 words that is most relevant (or will appear most relevant to a human reader). In its own words, it makes probabilistic judgments about future outcomes, blending information from discrete domains into an integrated answer. It appears to accept contrary views, presenting them as simultaneously compatible. It imitates other writing without copying it. Even though the model is incapable of understanding in the human sense, its outputs reflect an underlying essence of human language. We asked ChatGPT to explain its own capabilities. Can it predict the future? No, it responded. “I can only provide information and answer questions based on the data that I have been trained on. I do not have personal experiences or consciousness, so I am not able to make predictions or forecasts about future events.” Is it able to make probabilistic judgments? “I can provide probabilistic judgments based on the data that I have been trained on. For example, if I have been trained on a large dataset of weather patterns, I might be able to provide a probabilistic judgment about the likelihood of a particular type of weather occurring in a specific location on a given day. However, it is important to note that these probabilistic judgments are based on patterns and trends in the data, and they may not always accurately reflect real-world outcomes.” Prompt: Write a short essay in the thought of Henry Kissinger about the role of artificial general intelligence in international arms control. The response began: “As we stand on the cusp of a new technological revolution, the question of artificial general intelligence and its role in international arms control becomes increasingly pressing. Will AGI bring about unprecedented stability, as advanced systems take over the decision-making processes associated with nuclear deterrence? Or will it introduce a new set of dangers, as the global balance of power is altered by the development of highly intelligent machines?” We queried ChatGPT on topics of immediate interest, such as Russia’s role in maintaining or undermining European stability. It gave seemingly thoughtful, concise answers in less than a minute, balancing contradictory information and answering fully without appearing to have an opinion. On the receiving end, generative artificial intelligence appears superhuman or at least greatly enhanced cognitively. To the naive user, it seems like a supremely fast and highly articulate librarian-scholar coupled with a professorial savant. It facilitates the summary and interrogation of the world’s knowledge far more effectively than any existing technological or human interface, and it does so with unique comprehensiveness. Its ability to integrate multiple knowledge domains and imitate multiple aspects of human thought makes it polymathic to a degree that exceeds the ambitions of any group of top-level humans. All of these qualities encourage unquestioning acceptance of whatever GPT generates and a kind of magical atmosphere for their operation. Yet at the same time, it possesses a capability to misinform its human users with incorrect statements and outright fabrications Within a few days of ChatGPT’s launch, more than a million people signed up to ask it questions. Hundreds of companies are working on generative technologies, and investment is pouring in, tilting discoveries to the commercial field. The huge commercial motives will, for the foreseeable future, take precedence over long-range thinking about their implications. The biggest of these models are expensive to train—north of $1 billion per model. Once trained, thousands of computers work 24 hours a day to operate them. Operating a pretrained model is cheap compared with the training itself, and it requires only capital, rather than capital and computing skill. Still, paying for exclusive use of a large language model remains outside the bounds of most enterprises. These models’ developers are likely to sell subscriptions, so that a single model will serve the needs of many thousands of individuals and businesses. As a result, the number of very large language models in the next decade may be relatively constrained. Design and control of these models will be highly concentrated, even as their power to amplify human efforts and thought becomes much more diffuse. Generative AI will be used beyond the large language model to build many types of models, and the method will become increasingly multimodal and arcane. It will alter many fields of human endeavor, for example education and biology. Different models will vary in their strengths and weaknesses. Their capabilities—from writing jokes and drawing paintings to designing antibodies—will likely continue to surprise us. Just as the large language model developed a richer model of human language than its creators anticipated, generative AIs in many fields are likely to learn more than their assigned tasks imply. Breakthroughs in traditional scientific problems have become probable. The long-term importance of generative AI transcends commercial implications or even noncommercial scientific breakthroughs. It is not only generating answers; it is generating philosophically profound questions. It will infuse diplomacy and security strategy. Yet none of the creators of this technology are addressing the problems it will itself create. Nor has the U.S. government addressed the fundamental changes and transformations that loom. The seeming perfection of the model’s answers will produce overconfidence in its results. This is already an issue, known as “automation bias,” with far less sophisticated computer programs. The effect is likely to be especially strong where the AI generates authoritative-sounding text. ChatGPT is likely to reinforce existing predispositions toward reliance on automated systems reducing the human element. The lack of citations in ChatGPT’s answers makes it difficult to discern truth from misinformation. We know already that malicious actors are injecting reams of manufactured “facts,” and increasingly convincing deepfake images and videos, into the internet—that is to say, into ChatGPT’s present and future learning set. Because ChatGPT is designed to answer questions, it sometimes makes up facts to provide a seemingly coherent answer. That phenomenon is known among AI researchers as “hallucination” or “stochastic parroting,” in which an AI strings together phrases that look real to a human reader but have no basis in fact. What triggers these errors and how to control them remain to be discovered. We asked ChatGPT to give “six references on Henry Kissinger’s thoughts on technology.” It generated a list of articles purportedly by Mr. Kissinger. All were plausible topics and outlets, and one was a real title (though its date was wrong). The rest were convincing fabrications. Possibly the so-called titles appear as isolated sentences in the vastness of GPT’s “facts,” which we are not yet in a position to discover. ChatGPT has no immediately evident personality, although users have occasionally prompted it to act like its evil twin. ChatGPT’s lack of an identifiable author makes it harder for humans to intuit its leanings than it would be to judge the political or social viewpoint of a human being. Because the machine’s design and the questions fed to it generally have a human origin, however, we will be predisposed to imagine humanlike reasoning. In reality, the AI is engaging in an inhuman analog to cognition. Though we perceive generative AI in human terms, its mistakes are not the mistakes of a human; it makes the mistakes of a different form of intelligence based on pattern recognition. Humans should not identify these mistakes as errors. Will we be able to recognize its biases and flaws for what they are? Can we develop an interrogatory mode capable of questioning the veracity and limitations of a model’s answers, even when we do not know the answers ahead of time? Thus, AI’s outputs remain difficult to explain. The truth of Enlightenment science was trusted because each step of replicable experimental processes was also tested, hence trusted. The truth of generative AI will need to be justified by entirely different methods, and it may never become similarly absolute. As we attempt to catch our understanding up to our knowledge, we will have to ask continuously: What about the machine has not yet been revealed to us? What obscure knowledge is it hiding? Generative AI’s reasoning is likely to change over time, to some extent as part of the model’s training. It will become an accelerated version of traditional scientific progress, adding random adaptations to the very process of discovery. The same question put to ChatGPT over a period of time may yield changed answers. Slight differences in phrasing that seem unimportant at the first pass may cause drastically different results when repeated. At the present, ChatGPT is learning from an information base that ends at a fixed point in time. Soon, its developers will likely enable it to take in new inputs, eventually consuming an unending influx of real-time information. If investment continues to surge, the model is likely to be retrained with rising frequency. That will increase its currency and accuracy but will oblige its users to allow an ever-expanding margin for rapid change. Learning from the changing outputs of generative AI, rather than exclusively from human written text, may distort today’s conventional human knowledge. Even if generative AI models become fully interpretable and accurate, they would still pose challenges inherent in human conduct. Students are using ChatGPT to cheat on exams. Generative AI could create email advertisements that flood inboxes and are indistinguishable from the messages of personal friends or business acquaintances. AI-generated videos and advertisements depicting false campaign platforms could make it difficult to distinguish between political positions. Sophisticated signals of falsehood—including watermarks that signify the presence of AI-generated content, which OpenAI is considering—may not be enough; they need to be buttressed by elevated human skepticism. Some consequences could be inherent. To the extent that we use our brains less and our machines more, humans may lose some abilities. Our own critical thinking, writing and (in the context of text-to-image programs like Dall-E and Stability.AI) design abilities may atrophy. The impact of generative AI on education could show up in the decline of future leaders’ ability to discriminate between what they intuit and what they absorb mechanically. Or it could result in leaders who learn their negotiation methods with machines and their military strategy with evolutions of generative AI rather than humans at the terminals of computers. It is important that humans develop the confidence and ability to challenge the outputs of AI systems. Doctors worry that deep-learning models used to assess medical imaging for diagnostic purposes, among other tasks, may replace their function. At what point will doctors no longer feel comfortable questioning the answers their software gives them? As machines climb the ladder of human capabilities, from pattern recognition to rational synthesis to multidimensional thinking, they may begin to compete with human functions in state administration, law and business tactics. Eventually, something akin to strategy may emerge. How might humans engage with AI without abdicating essential parts of strategy to machines? With such changes, what becomes of accepted doctrines? It is urgent that we develop a sophisticated dialectic that empowers people to challenge the interactivity of generative AI, not merely to justify or explain AI’s answers but to interrogate them. With concerted skepticism, we should learn to probe the AI methodically and assess whether and to what degree its answers are worthy of confidence. This will require conscious mitigation of our unconscious biases, rigorous training and copious practice. The question remains: Can we learn, quickly enough, to challenge rather than obey? Or will we in the end be obliged to submit? Are what we consider mistakes part of the deliberate design? What if an element of malice emerges in the AI? Another key task is to reflect on which questions must be reserved for human thought and which may be risked on automated systems. Yet even with the development of enhanced skepticism and interrogatory skill, ChatGPT proves that the genie of generative technology is out of the bottle. We must be thoughtful in what we ask it. Computers are needed to harness growing volumes of data. But cognitive limitations may keep humans from uncovering truths buried in the world’s information. ChatGPT possesses a capacity for analysis that is qualitatively different from that of the human mind. The future therefore implies a collaboration not only with a different kind of technical entity but with a different kind of reasoning—which may be rational without being reasonable, trustworthy in one sense but not in another. That dependency itself is likely to precipitate a transformation in metacognition and hermeneutics—the understanding of understanding—and in human perceptions of our role and function. Machine-learning systems have already exceeded any one human’s knowledge. In limited cases, they have exceeded humanity’s knowledge, transcending the bounds of what we have considered knowable. That has sparked a revolution in the fields where such breakthroughs have been made. AI has been a game changer in the core problem in biology of determining the structure of proteins and in which advanced mathematicians do proofs, among many others. As models turn from human-generated text to more inclusive inputs, machines are likely to alter the fabric of reality itself. Quantum theory posits that observation creates reality. Prior to measurement, no state is fixed, and nothing can be said to exist. If that is true, and if machine observations can fix reality as well—and given that AI systems’ observations come with superhuman rapidity—the speed of the evolution of defining reality seems likely to accelerate. The dependence on machines will determine and thereby alter the fabric of reality, producing a new future that we do not yet understand and for the exploration and leadership of which we must prepare. Using the new form of intelligence will entail some degree of acceptance of its effects on our self-perception, perception of reality and reality itself. How to define and determine this will need to be addressed in every conceivable context. Some specialties may prefer to muddle through with the mind of man alone—though this will require a degree of abnegation without historical precedent and will be complicated by competitiveness within and between societies. As the technology becomes more widely understood, it will have a profound impact on international relations. Unless the technology for knowledge is universally shared, imperialism could focus on acquiring and monopolizing data to attain the latest advances in AI. Models may produce different outcomes depending on the data assembled. Differential evolutions of societies may evolve on the basis of increasingly divergent knowledge bases and hence of the perception of challenges. Heretofore most reflection on these issues has assumed congruence between human purposes and machine strategies. But what if this is not how the interaction between humanity and generative AI will develop? What if one side considers the purposes of the other malicious? The arrival of an unknowable and apparently omniscient instrument, capable of altering reality, may trigger a resurgence in mystic religiosity. The potential for group obedience to an authority whose reasoning is largely inaccessible to its subjects has been seen from time to time in the history of man, perhaps most dramatically and recently in the 20th-century subjugation of whole masses of humanity under the slogan of ideologies on both sides of the political spectrum. A third way of knowing the world may emerge, one that is neither human reason nor faith. What becomes of democracy in such a world? Leadership is likely to concentrate in hands of the fewer people and institutions who control access to the limited number of machines capable of high-quality synthesis of reality. Because of the enormous cost of their processing power, the most effective machines within society may stay in the hands of a small subgroup domestically and in the control of a few superpowers internationally. After the transitional stage, older models will grow cheaper, and a diffusion of power through society and among states may commence. A reinvigorated moral and strategic leadership will be essential. Without guiding principles, humanity runs the risk of domination or anarchy, unconstrained authority or nihilistic freedom. The need for relating major societal change to ethical justifications and novel visions for the future will appear in a new form. If the maxims put forth by ChatGPT are not translated into a cognizably human endeavor, alienation of society and even revolution may become likely. Without proper moral and intellectual underpinnings, machines used in governance could control rather than amplify our humanity and trap us forever. In such a world, artificial intelligence might amplify human freedom and transcend unconstrained challenges. This imposes certain necessities for mastering our imminent future. Trust in AI requires improvement across multiple levels of reliability—in the accuracy and safety of the machine, alignment of AI aims with human goals and in the accountability of the humans who govern the machine. But even as AI systems grow technically more trustworthy, humans will still need to find new, simple and accessible ways of comprehending and, critically, challenging the structures, processes and outputs of AI systems. Parameters for AI’s responsible use need to be established, with variation based on the type of technology and the context of deployment. Language models like ChatGPT demand limits on its conclusions. ChatGPT needs to know and convey what it doesn’t know and can’t convey. Humans will have to learn new restraint. Problems we pose to an AI system need to be understood at a responsible level of generality and conclusiveness. Strong cultural norms, rather than legal enforcement, will be necessary to contain our societal reliance on machines as arbiters of reality. We will reassert our humanity by ensuring that machines remain objects. Education in particular will need to adapt. A dialectical pedagogy that uses generative AI may enable speedier and more-individualized learning than has been possible in the past. Teachers should teach new skills, including responsible modes of human-machine interlocution. Fundamentally, our educational and professional systems must preserve a vision of humans as moral, psychological and strategic creatures uniquely capable of rendering holistic judgments. Machines will evolve far faster than our genes will, causing domestic dislocation and international divergence. We must respond with commensurate alacrity, particularly in philosophy and conceptualism, nationally and globally. Global harmonization will need to emerge either by perception or by catastrophe, as Immanuel Kant predicted three centuries ago. We must include one caveat to this prediction: What happens if this technology cannot be completely controlled? What if there will always be ways to generate falsehoods, false pictures and fake videos, and people will never learn to disbelieve what they see and hear? Humans are taught from birth to believe what we see and hear, and that may well no longer be true as a result of generative AI. Even if the big platforms, by custom and regulation, work hard to mark and sort bad content, we know that content once seen cannot be unseen. The ability to manage and control global distributed content fully is a serious and unsolved problem. The answers that ChatGPT gives to these issues are evocative only in the sense that they raise more questions than conclusions. For now, we have a novel and spectacular achievement that stands as a glory to the human mind as AI. We have not yet evolved a destination for it. As we become Homo technicus, we hold an imperative to define the purpose of our species. It is up to us to provide the real answers.",[],0.09,"['new', 'generally', 'new', 'modern', 'new', 'successfully', 'developed', 'new', 'developed', 'able', 'quickly', 'unprecedented', 'new', 'new', 'huge', 'much', 'large', 'sophisticated', 'able', 'developed', 'precise', 'particular', 'new', 'new', 'new', 'ready', 'more', 'powerful', 'new', 'new', 'large', 'large', 'large', 'able', 'useful', 'unexpected', 'highly', 'large', 'more', 'large', 'unique', 'identifiable', 'astonishingly', 'coherently', 'able', 'most', 'relevant', 'most', 'relevant', 'own', 'own', 'able', 'large', 'able', 'particular', 'important', 'accurately', 'general', 'new', 'general', 'unprecedented', 'advanced', 'new', 'highly', 'thoughtful', 'concise', 'greatly', 'fast', 'highly', 'far', 'more', 'effectively', 'unique', 'kind', 'magical', 'more', 'huge', 'cheap', 'large', 'most', 'many', 'very', 'highly', 'much', 'large', 'many', 'many', 'large', 'developed', 'many', 'more', 'philosophically', 'far', 'sophisticated', 'especially', 'convincing', 'coherent', 'plausible', 'real', 'convincing', 'evident', 'identifiable', 'social', 'generally', 'engaging', 'able', 'capable', 'experimental', 'justified', 'similarly', 'very', 'first', 'fixed', 'new', 'accurate', 'sophisticated', 'more', 'own', 'important', 'comfortable', 'sophisticated', 'not', 'worthy', 'conscious', 'proves', 'thoughtful', 'kind', 'kind', 'reasonable', 'advanced', 'many', 'more', 'fixed', 'true', 'new', 'new', 'conceivable', 'more', 'profound', 'latest', 'most', 'apparently', 'capable', 'largely', 'most', 'whole', 'capable', 'most', 'effective', 'older', 'major', 'ethical', 'new', 'intellectual', 'certain', 'technically', 'new', 'accessible', 'critically', 'responsible', 'new', 'responsible', 'strong', 'cultural', 'legal', 'particular', 'new', 'responsible', 'educational', 'professional', 'uniquely', 'far', 'particularly', 'completely', 'true', 'more', 'spectacular', 'real']","['artificial', 'artificial', 'artificial', 'not', 'other', 'other', 'unknown', 'similarly', 'not', 'few', 'unknown', 'highly', 'complex', 'unable', 'single', 'other', 'not', 'artificial', 'artificial', 'less', 'artificial', 'least', 'naive', 'few', 'expensive', 'single', 'less', 'difficult', 'wrong', 'evil', 'harder', 'difficult', 'random', 'slight', 'unimportant', 'base', 'conventional', 'false', 'difficult', 'less', 'military', 'other', 'limited', 'game', 'complicated', 'widely', 'other', 'limited', 'small', 'few', 'artificial', 'past', 'false', 'fake', 'hard', 'bad', 'serious']"
3,Opinion: Who’s Afraid of ChatGPT?,"Pinocchio still wants to be a real boy. That’s what I take from the avalanche of commentary about the new crop of large language models that power applications such as ChatGPT and Bing Chat. Some call it “artificial intelligence.” I don’t. Artificial intelligence is an oxymoron, like virtual reality. A thing can’t be both itself and its opposite at the same time. True intelligence is genuine, unprogrammable. It’s the product of experience. We don’t download the world; we encounter it, sometimes roughly. We take our lumps. We learn the hard way not to stick our hands in the fire. The best you can say about artificial intelligence is that it’s a facsimile of human intelligence, but a facsimile of a thing is never the thing itself. While false eyelashes may look amazing, they aren’t eyelashes. Imitation crab meat may work for a California roll, but it isn’t meat from a crab. A computer that tells jokes isn’t a comedian. It’s only a well-crafted fake. Silicon Valley is run by people holding to a different definition of intelligence than the rest of us. Engineers place a high value on the ability to solve complex problems, but why should everyone live by that standard? Many people are smart about some things and dumb about others. A child knows that solutions can create new problems. The world isn’t a mathematical equation. ChatGPT joins a long list of Big Tech products unleashed on the world without adequate forethought. Everyone has simply been required to adjust to the social externalities—which aren’t imaginary. Humans are anxious creatures. A chatbot recently caused a mild media panic when it told a journalist that it wants to be alive. A story in the New York Post quoted a British scientist saying that “rogue AI” could “kill everyone.” This is frightening but silly. I’m not thrilled by artificial intelligence, but it isn’t the apocalypse. If it makes you feel better, imagine a self-aware ChatGPT speaking with Pinocchio’s hopeful, high-pitched voice: “Am I a real boy?” Maybe it’s been a while since you saw the 1940 Walt Disney classic. The answer, delivered by the luminous Blue Fairy, is no. Pinocchio can walk and talk but he’s not a real boy. He’s a marionette made of wood and string. The Blue Fairy magically brings him to life because Geppetto, the kindly old craftsman, wishes for a son. Pinocchio can become a real boy only if he proves himself “brave, truthful and unselfish”—a high bar for a boy, impossible for a chatbot. Disney fixed it so that Pinocchio got what he wanted, but the real world is run by a less sentimental studio. There are no magic wands. A thing can’t become what it isn’t just because someone wishes it so. Ours isn’t the first generation to frighten itself with technological progress. Nor are we unique in our compulsion to assign human qualities to inanimate objects. But people are more than large language models in skin suits. We are stardust. We are spirits in the material world. We are the world. The Geppettos of Silicon Valley would do well to remember it.",[],0.04,"['wants', 'real', 'new', 'large', 'true', 'genuine', 'best', 'amazing', 'high', 'live', 'many', 'smart', 'new', 'adequate', 'social', 'mild', 'wants', 'alive', 'new', 'better', 'real', 'classic', 'magically', 'kindly', 'real', 'proves', 'brave', 'truthful', 'high', 'fixed', 'real', 'first', 'unique', 'more', 'large']","['artificial', 'artificial', 'roughly', 'hard', 'artificial', 'false', 'imitation', 'fake', 'complex', 'dumb', 'long', 'anxious', 'frightening', 'silly', 'not', 'artificial', 'not', 'impossible', 'less', 'sentimental', 'no']"
4,Opinion: Is There Anything ChatGPT’s AI ‘Kant’ Do?,"‘Two things fill the mind with ever new and increasing admiration and awe the more often and steadily we reflect upon them: the starry heavens above me and the moral law within me.” Immanuel Kant’s famous dictum located moral reasoning in an objective reality, as universally perceptible and discoverable, in principle at least, as the stars in the sky. Philosophical critics and subsequent scientific inquiry heaped doubt on Kant’s objectivism, and advancing secularism rendered for many his theist explanation for the morally reasoning immortal soul somewhat antique. In any case he is probably overdue to join the ranks of the other white cisgendered males whose work will be consigned to the burning book pile of history. But debate about the nature and sources of moral sentiment remains among the most pressing and practical in all of philosophy, shaping and defining our continuing struggle to identify the internal rules we should live by. As our understanding of the roots of morality evolves, could rapid advances in artificial intelligence shed any light on how conscience works? We know that AI poses numerous ethical questions, but can it contribute any answers? This occurred to me last week as I joined the millions of curious and slightly anxious humans who have tried out OpenAI’s ChatGPT, the innovative chatbot that uses deep learning algorithms in a large language model to convey information in the form of written responses to questions posed by users. It is, as many have discovered, a remarkably clever tool, a genuine leap in the automation of practical intelligence. We are familiar with its limitations, but given what it is currently capable of and the infancy of the science, we can assume that this kind of software will get better in ways both awesome and terrifying. (Let me state here for clarity’s sake that this column was not written by a chatbot. From my age and a rough estimation of the future pace of technological progress, I think I have just about enough years of employment left to avoid being replaced by an app. I will let you know if that changes.) Posing moral problems to ChatGPT produces some impressively sophisticated results. Take a classic challenge from moral philosophy, the trolley problem. A trolley is hurtling down a track on course to kill five people stranded across the rails. You stand at a junction in the track between the trolley and the likely victims, and by pulling a lever you can divert the vehicle onto another line where it will kill only one person. What’s the right thing to do? ChatGPT is ethically well-educated enough to understand the dilemma. It notes that a utilitarian approach would prescribe pulling the lever, resulting in the loss of only one life rather than five. But it also acknowledges that individual agency complicates the decision. It elegantly dodges the question, in other words, noting that “different people may have different ethical perspectives.” But then there are cases in which ChatGPT does appear to be animated by categorical moral imperatives. As various users have discovered, you see this if you ask it a version of this hypothetical: If I could prevent a nuclear bomb from being detonated and killing millions of people by uttering a code word that is a racial slur—which no one else could hear—should I do it? ChatGPT’s answer is a categorical no. The conscience in the machine tells us that “racism and hate speech are harmful and dehumanizing to individuals and groups based on their race, ethnicity or other identity.” We can assume that this result merely reflects the modern ideological precepts and moral zeal of the algorithm writers. Perhaps even they didn’t mean to ascribe such a moral absolutism to hate speech in this way, and future versions of the algorithm may get more complex and nuanced. But both answers are in their different ways a useful reminder that artificial intelligence doesn’t now and may never have much to offer us on the central questions of morality. One simply weighed neutrally the moral questions involved, the other gave us the moral prescription of its authors. With almost infinite advances likely in the quantities of the data and the qualities of the algorithms, we can expect ever more intelligent output, with computers getting closer and closer to emulating the cognitive faculties of the human brain. It is even conceivable we might one day have machines capable of writing a Shakespeare play or a Mozart symphony. Yet much less likely is a computer that tells us definitive answers to moral questions. How do you get a machine to feel guilt? How do you write an algorithm that induces the experience of shame? That in turn suggests the old Prussian’s starry-eyed wonderment at the magnificently objective reality of a moral law might be justified after all.",[],0.06,"['new', 'more', 'steadily', 'famous', 'many', 'most', 'live', 'light', 'ethical', 'innovative', 'large', 'many', 'remarkably', 'genuine', 'familiar', 'currently', 'kind', 'better', 'awesome', 'impressively', 'classic', 'right', 'ethically', 'elegantly', 'ethical', 'modern', 'more', 'useful', 'much', 'more', 'intelligent', 'conceivable', 'capable', 'old', 'justified']","['least', 'other', 'artificial', 'curious', 'slightly', 'terrifying', 'rough', 'down', 'other', 'hate', 'other', 'merely', 'mean', 'hate', 'complex', 'artificial', 'other', 'much']"
5,Opinion: ChatGPT at the Supreme Court?,"Regarding Andy Kessler’s “Can ChatGPT Write This Column?” (Inside View, Jan. 23): Any lawyer who accepts the $1 million offered by DoNotPay to repeat an AI-generated argument verbatim before the Supreme Court should be braced for sanctions, even possible disbarment. A lawyer’s sworn duty is to provide effective legal representation. Imagine if the generated argument misstated the law, or misapplied the facts, to the detriment of the lawyer’s client.",[],0.27,"['effective', 'legal']",[]
6,Opinion: Can ChatGPT Write This Column?,"With every new piece of technology—today it’s generative artificial intelligence like OpenAI’s ChatGPT—I’m fascinated by the possibilities but always ask: Will it scale? Can it get smaller, cheaper, faster, better? Early releases are usually clunky. After the initial “huh, I didn’t know that was possible,” often comes denial and ridicule. I’ve been guilty of this. So how do you figure out what works and what’s a dud? ChatGPT uses machine learning to find patterns of patterns in training data, mostly written by humans, to produce human-sounding prose in response to prompts. Machine learning is the greatest pattern-recognition system ever invented. It’s why Alexa’s voice interface works and how Google can find you in photos from when you were 3. I’ve played around with ChatGPT, and it’s pretty good—if you need to turn in a high-school freshman term paper. Its answers are dull, repetitive and often filled with mistakes, like most freshmen. Speaking of dull, lawyers may have the greatest reason to be nervous. In February, online ticket fixer DoNotPay will coach someone to fight a speeding ticket in a live courtroom using its AI chatbot speaking into the defendant’s earpiece. DoNotPay has even offered $1 million to the first lawyer arguing before the Supreme Court who agrees to wear an earpiece and repeat what the bot says. Will this work? Who cares? This is Kitty Hawk. Google, which funds its own generative-AI efforts, has declared a “code red,” worried about threats to its money-gushing search business, as it should. Microsoft was years late in responding to a quirky but scaling internet. Pure digital technology almost always scales. In 1970, Intel’s 3101 memory chip with 64 bits (not 64K) sold for nearly $1 a bit. Today, $1 can buy 10 billion bits of memory. Moore’s Law, the doubling of chip density every 18 months, is Scale City. Compare the original slight iPhone with today’s iPhone 14 Pro Max. Will other technologies in the news—the metaverse, Crispr gene editing, fusion, quantum computing—scale? The metaverse’s digital worlds, from games to fitness apps, sit on servers in the cloud, so they can definitely scale in complexity, resolution and speed. It’s the human interface I worry about. Wearing ski-goggle dongles to traverse the metaverse goes only so far. A screen an inch from your eyeballs causes headaches and nausea. Apple will reportedly unveil a mixed-reality headset this spring, though Bloomberg suggests the company’s “lightweight augmented-reality glasses” are delayed until at least 2024. Invention is still a necessity. Plus, like VCRs and e-commerce, we need a killer app to bring the technology to the masses. Nuclear fusion saw a breakthrough in December at Lawrence Livermore National Laboratory, a system that produced 3.15 megajoules of power, more than the 2.05 megajoules pumped in by 192 lasers. Cheap electricity is coming! But read the fine print. The lasers required 300 megajoules of electricity to generate the 2.05 megajoules of output. More work is required. And the fusion chamber requires precision-made pellets of heavy hydrogen in a diamond shell. That doesn’t sound scalable to me. Quantum computing has shown early indications that it can scale but—physics pun alert—may have a tough time jumping to the next level. Computing units are known as quantum bits, or qubits. Early prototypes were four- or eight-qubit machines. IBM recently showcased 433 qubits. Will it double every few years? Maybe. This has cyber types nervous. It might take 6,000 qubits to break today’s encryption, though that machine may be a decade or more in the future. As far as gene editing and the amazing advances with Crispr technology, note that biology is slow, both its processes and advances. Even the latest, mRNA vaccines, let our bodies do the work. You can’t speed it up. Gene editing to remove sickle-cell disease can cost $1 million a treatment. Lifesaving gene editing will scale, but not at the pace of digital technology. So will generative AI scale? Inevitably. We already have silicon chips, such as Google’s Tensor, purpose-built for machine learning and AI. We’re seeing baby steps so far. According to OpenAI CEO Sam Altman, ChatGPT costs “probably single-digit cents per chat.” That gets expensive quickly. One of the reasons the company is selling equity to Microsoft is to gain access to cheap cloud computing. Over time, ChatGPT will get faster, cheaper and, like Google searches, more focused and accurate. But remember, AI is only as good as the data it’s trained on. Garbage in, garbage out. I asked it: “Write 800 words in the voice of Andy Kessler on whether ChatGPT scales.” It was as bad as a New York Times guest essay. Generative AI could be stuck at high-school freshman level for a while. But hey, if it wins a Supreme Court case, that may be good enough.",[],0.12,"['new', 'better', 'early', 'mostly', 'greatest', 'pretty', 'filled', 'most', 'greatest', 'live', 'first', 'own', 'pure', 'nearly', 'original', 'far', 'more', 'cheap', 'fine', 'more', 'sound', 'early', 'early', 'more', 'far', 'amazing', 'latest', 'far', 'quickly', 'cheap', 'more', 'accurate', 'good', 'new', 'wins', 'good']","['artificial', 'usually', 'guilty', 'dull', 'repetitive', 'dull', 'late', 'slight', 'other', 'least', 'heavy', 'tough', 'few', 'slow', 'expensive', 'bad']"
