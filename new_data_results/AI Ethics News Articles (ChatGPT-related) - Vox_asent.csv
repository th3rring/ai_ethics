,titles,body_contents,tags,sentiment_score,positive_words,negative_words
0,"The makers of ChatGPT just released a new AI that can build websites, among other things","When ChatGPT came out in November, it took the world by storm. Within a month of its release, some 100 million people had used the viral AI chatbot for everything from writing high school essays to planning travel itineraries to generating computer code. Built by the San Francisco-based startup OpenAI, the app was flawed in many ways, but it also sparked a wave of excitement (and fear) about the transformative power of generative AI to change the way we work and create. ChatGPT, which runs on a technology called GPT-3.5, has been so impressive, in part, because it represents a quantum leap from the capabilities of its predecessor from just a few years ago, GPT-2. On Tuesday, OpenAI released an even more advanced version of its technology: GPT-4. The company says this update is another milestone in the advancement of AI. The new technology has the potential to improve how people learn new languages, how blind people process images, and even how we do our taxes. OpenAI also claims that the new model supports a chatbot that’s more factual, creative, concise, and can understand images, instead of just text. Sam Altman, the CEO of OpenAI, called GPT-4 “our most capable and aligned model yet.” He also cautioned that “it is still flawed, still limited, and it still seems more impressive on first use than it does after you spend more time with it” In a livestream demo of GPT-4 on Tuesday afternoon, OpenAI co-founder and president Greg Brockman showed some new use cases for the technology, including the ability to be given a hand-drawn mockup of a website and, from that, generate code for a functional site in a matter of seconds. Brockman also showcased GPT-4’s visual capabilities by feeding it a cartoon image of a squirrel holding a camera and asking it to explain why the image is funny. “The image is funny because it shows a squirrel holding a camera and taking a photo of a nut as if it were a professional photographer. It’s a humorous situation because squirrels typically eat nuts, and we don’t expect them to use a camera or act like humans,” GPT-4 responded. This is the sort of capability that could be incredibly useful to people who are blind or visually impaired. Not only can GPT-4 describe images, but it can also communicate the meaning and context behind them. Still, as Altman and GPT-4’s creators have been quick to admit, the tool is nowhere near fully replacing human intelligence. Like its predecessors, it has known problems around accuracy, bias, and context. That poses a growing risk as more people start using GPT-4 for more than just novelty. Companies like Microsoft, which invests heavily in OpenAI, are already starting to bake GPT-4 into core products that millions of people use. Here are a few things you need to know about the latest version of the buzziest new technology in the market. It can pass complicated exams One tangible way people are measuring the capabilities of new artificial intelligence tools is by seeing how well they can perform on standardized tests, like the SAT and the bar exam. GPT-4 has shown some impressive progress here. The technology can pass a simulated legal bar exam with a score that would put it in the top 10 percent of test takers, while its immediate predecessor GPT-3.5 scored in the bottom 10 percent (watch out, lawyers). GPT-4 can also score a 700 out of 800 on the SAT math test, compared to a 590 in its previous version. Still, GPT-4 is weak in certain subjects. It only scored a 2 out of 5 on the AP English Language exams — the same score as the prior version, GPT-3.5, received. Standardized tests are hardly a perfect measure of human intelligence, but the types of reasoning and critical thinking required to score well on these tests show that the technology is improving at an impressive clip. It shows promise at teaching languages and helping the visually impaired Since GPT-4 just came out, it will take time before people discover all of the most compelling ways to use it, but OpenAI has proposed a couple of ways the technology could potentially improve our daily lives. One is for learning new languages. OpenAI has partnered with the popular language learning app Duolingo to power a new AI-based chat partner called Roleplay. This tool lets you have a free-flowing conversation in another language with a chatbot that responds to what you’re saying and steps in to correct you when needed. Another big use case that OpenAI pitched involves helping people who are visually impaired. In partnership with Be My Eyes, an app that lets visually impaired people get on-demand help from a sighted person via video chat, OpenAI used GPT-4 to create a virtual assistant that can help people understand the context of what they’re seeing around them. One example OpenAI gave showed how, given a description of the contents of a refrigerator, the app can offer recipes based on what’s available. The company says that’s an advancement from the current state of technology in the field of image recognition. “Basic image recognition applications only tell you what’s in front of you,” said Jesper Hvirring Henriksen, CTO of Be My Eyes, in a press release for GPT-4’s launch. “They can’t have a discussion to understand if the noodles have the right kind of ingredients or if the object on the ground isn’t just a ball, but a tripping hazard — and communicate that.” If you want to use OpenAI’s latest GPT-4 powered chatbot, it isn’t free Right now, you’ll have to pay $20 per month for access to ChatGPT Plus, a premium version of the ChatGPT bot. GPT4’s API is also available to developers who can build apps on top of it for a fee proportionate to how much they’re using the tool. However, if you want a taste of GPT-4 without paying up, you can use a Microsoft-made chatbot called BingGPT. A Microsoft VP confirmed on Tuesday that the latest version of BingGPT is using GPT-4. It’s important to note that BingGPT has limitations on how many conversations you can have a day, and it doesn’t allow you to input images. GPT-4 still has serious flaws. Researchers worry we don’t know what data it’s being trained on. While GPT-4 has clear potential to help people, it’s also inherently flawed. Like previous versions of generative AI models, GPT-4 can relay misinformation or be misused to share controversial content, like instructions on how to cause physical harm or content to promote political activism. OpenAI says that GPT-4 is 40 percent more likely to give factual responses, and 82 percent less likely to respond to requests for disallowed content. While that’s an improvement from before, there’s still plenty of room for error. Another concern about GPT-4 is the lack of transparency around how it was designed and trained. Several prominent academics and industry experts on Twitter pointed out that the company isn’t releasing any information about the data set it used to train GPT-4. This is an issue, researchers argue, because the large datasets used to train AI chatbots can be inherently biased, as evidenced a few years ago by Microsoft’s Twitter chatbot, Tay. Within a day of its release, Tay gave racist answers to simple questions. It had been trained on social media posts, which can often be hateful. OpenAI says it’s not sharing its training data in part because of competitive pressure. The company was founded as a nonprofit but became a for-profit entity in 2019, in part because of how expensive it is to train complex AI systems. OpenAI is now heavily backed by Microsoft, which is engaged in a fierce battle with Google over which tech giant will lead on generative AI technologies. Without knowing what’s under the hood, it’s hard to immediately validate OpenAI’s claims that its latest tool is more accurate and less biased than before. As more people use the technology in the coming weeks, we’ll see if it ends up being not only meaningfully more useful but also more responsible than what came before it.",[],0.16,"['excitement', 'create', 'impressive', 'advanced', 'improve', 'supports', 'creative', 'capable', 'impressive', 'ability', 'hand', 'matter', 'funny', 'funny', 'humorous', 'like', 'useful', 'admit', 'intelligence', 'Like', 'growing', 'like', 'intelligence', 'well', 'like', 'impressive', 'progress', 'legal', 'top', 'certain', 'perfect', 'intelligence', 'well', 'improving', 'impressive', 'promise', 'helping', 'compelling', 'improve', 'popular', 'free', 'helping', 'help', 'create', 'help', 'kind', 'want', 'free', 'top', 'want', 'important', 'allow', 'clear', 'help', 'Like', 'share', 'like', 'promote', 'improvement', 'prominent', 'competitive', 'profit', 'backed', 'engaged', 'validate', 'useful', 'responsible']","['flawed', 'fear', 'blind', 'flawed', 'limited', 'nuts', 'blind', 'problems', 'bias', 'risk', 'weak', 'critical', 'demand', 'pay', 'serious', 'worry', 'flawed', 'misinformation', 'controversial', 'harm', 'error', 'lack', 'argue', 'biased', 'racist', 'hateful', 'sharing', 'pressure', 'battle', 'hard', 'biased']"
1,"If you’re not using ChatGPT for your writing, you’re probably making a mistake","About 10 minutes into my interview with Ethan Mollick, a professor at the University of Pennsylvania’s Wharton business school who has become a prominent evangelist for AI tools, it became clear that he was going to use Bing to interview me. He started by asking the Microsoft search engine, newly infused with a generative AI model from OpenAI, “Can you look at the work of Dylan Matthews of Vox and tell me some common themes, as well as any strengths or weaknesses.” In a couple seconds, Bing had an answer: “Dylan Matthews is one of the senior correspondents at Vox. He covers topics such as effective altruism, philanthropy, global health, and social justice.” (So far, so good.) Dylan “often uses charts, graphs, tables, and quotes from experts and sources to support his arguments,” it continued, but “other Vox writers may have different writing styles and tones depending on their topic and audience.” For instance, “Some may aim to entertain readers with interesting facts or stories,” which I guess is not something the machines think I do. Mollick wasn’t done interrogating. He asked for examples of some of the best praise and criticism of my articles, and unearthed some scathing critiques of an old tongue-in-cheek defense of monarchy I once wrote (“This is a terrible article,” noted one poster. “It’s full of cherry-picked data”), and some nice notes on a feature I wrote about effective altruism last summer. Taking that thread and running with it, Mollick asked Bing for ideas of papers on the topic of effective altruism and some names of journals that might take them; he got three suggestions, with links to previous articles the journals had run on the topic (one journal — notably given generative AI’s occasional tendency to hallucinate false facts — was paired with an article it didn’t run, and an author who did not even write that article). Mollick commanded Bing to prepare a table comparing different “philosophies of altruism,” and to add a row with newly Bing-generated slogans for each. This is what it delivered: While “Survive and thrive by helping your kin” was not the way my evolutionary biology professor in college explained kin selection … it’s a lot catchier than anything you’ll find in a textbook. Neither Ethan Mollick nor Lilach, his equally AI-obsessed research collaborator at Wharton and his spouse, are AI experts by background. Ethan researches and teaches entrepreneurship, while Lilach works on developing interactive simulations meant to help students try out scenarios like job interviews, elevator pitches to investors, running an early-stage startup, and more. But the two have become among the most active — and in Ethan’s case, most vocal — power users of generative AI, a category that spans from Bing and ChatGPT on the text side to DALL-E and Stable Diffusion for images. When she started using ChatGPT, Lilach recalls, “My world fell apart. I thought, ‘This is crazy.’ I couldn’t believe the output it was giving me. I couldn’t believe the feedback it was giving me.” Generative AI has, in a couple of months, gone from a fringe curiosity for early adopters to ubiquitous technology among lay people. ChatGPT racked up over 660 million visits in January. The bank UBS estimates that it took two months for the software to gain 100 million monthly active users; for comparison, TikTok took nine months, and Facebook took four and a half years. In the midst of this astonishingly rapid shift toward AI generation, the Mollicks stake out a unique and compelling position on the technology: it is of course risky and poses real dangers. It will get things wrong. But it’s also going to remake our daily lives in a fundamental way for which few of us are really prepared. It’s a mistake to ignore the risks posed by these large language models (LLMs), which range from making up facts to belligerent behavior to the possibility that even sophisticated users will begin thinking the AI is sentient. (It’s not.) But the Mollicks argue it would also be a mistake to miss what the existence of these systems means, concretely, right now, for jobs that consist of producing text. Which includes a lot of us: journalists like me, but also software engineers, academics and other researchers, screenwriters, HR staffers, accountants, hell, anyone whose job requires what we used to call paperwork of any kind. “If we stop with Bing, it would be enough to disrupt like 20 different major industries,” Ethan argued to me. “If you’re not using Bing for your writing, you’re probably making a mistake.” I hadn’t been using Bing for writing until I heard him say that. Now I can’t stop. Generative AI’s potential Don’t take the Mollicks’ word for it: Just read the studies, which Ethan enthusiastically sends to his over 17,000 (free) Substack subscribers and over 110,000 Twitter followers. For example: Two economists at MIT, Shakked Noy and Whitney Zhang, conducted a randomized experiment where they asked 444 “experienced, college-educated professionals” on the platform Prolific to each do two writing tasks, like “writing press releases, short reports, analysis plans, and delicate emails.” Noy and Zhang then had another team of professionals, matched to the same occupations as the test subjects, review their work, with each piece of writing read three times. Half the participants, though, were instructed to sign up for ChatGPT, trained in it, and told they could use it for the second task for which they were hired. The average time taken to complete the assignment was only 17 minutes in the ChatGPT group, compared to 27 in the control, cutting time by over a third. Evaluators graded the ChatGPT output as substantially better: On a scale of 1 to 7, the ChatGPT group averaged a 4.5, compared to 3.8 for the control group. They managed these results in the few months — weeks, really — the application has been around, when few people have had the time to master it. Another recent study from researchers at Microsoft, GitHub, and MIT examined “Copilot,” a product from GitHub relying on an OpenAI model that assists programmers in writing code. “Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible,” the authors write in the abstract. “The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group.” That’s not the hardest programming task there is — but still. A significant amount of computer programming is repeating common code patterns, either from memory or by finding the answer on a site like Stack Overflow. AI can make that part of the job much, much faster. A third paper, from Princeton’s Edward Felten, Penn’s Manav Raj, and NYU’s Robert Seamans, tried to systematically estimate which jobs will be most exposed to, or affected by, the rise of large language models. They found that the single most affected occupation class is telemarketers — perhaps unsurprising, given that their entire job revolves around language. Every single other job in the top 10 is some form of college professor, from English to foreign languages to history. Lest the social scientists get too smug about their struggling humanities peers, sociology, psychology, and political science aren’t far behind. Once upon a time, people like academics, journalists, and computer programmers could take some satisfaction in our status as “knowledge workers,” or parts of the “creative class.” Our jobs might be threatened by low ad revenue or state budget cuts, and the compensation was somewhat lacking, but those jobs were literally high-minded. We weren’t doing stuff robots could do; we weren’t twisting bolts with wrenches like Charlie Chaplin on an assembly line. Now, however, we have tools with the potential to automate a significant portion of our jobs. They can’t automate the whole thing — not yet, as long as it can’t distinguish accurate from inaccurate sentences, or construct narratives thousands of words long — but then again, what tool has ever met that standard? Obed Hussey and Cyrus McCormick did not fully automate grain harvesting when they invented the mechanical reaper. But they still transformed farming forever. (And if you don’t know who Hussey and McCormick are … ask ChatGPT.) Academia after the bots The Mollicks don’t just talk the talk. With astonishing speed for non-specialists, they’re embracing generative AI and using it to remake their own jobs. Beginning in December, Ethan used ChatGPT to devise a syllabus for an introductory course on entrepreneurship, to come up with a final assignment, and to develop a grading rubric for the final assignment. He used it to produce a test submission for the assignment, and to grade that submission, using the rubric the AI had created previously. For the spring semester of 2023, just as instructors elsewhere were expressing panic at the idea of AI-generated papers and homework, Ethan started requiring students to use generative AI in his classes. As Ann Christine Meidinger, an exchange student from Chile who is in two of his classes this semester, put it, “Basically both of his classes turned out to be the AI classes. That’s how we refer to them — ‘the AI class.’” What’s striking is that neither class is about AI, per se. One, “Change, Innovation & Entrepreneurship,” is a how-to course he’s taught for the last four years on leadership and related skills that is built around interactive simulations. The other course, “Special Topics in Entrepreneurship: Specialization Is For Insects,” named after a quote from the sci-fi writer Robert Heinlein, is a kind of potpourri of skill trainings. Week two teaches students to make physical product prototypes and prototypes of apps; week three is about running a kitchen for a restaurant business. These don’t seem like obvious places to start using AI to automate. But Meidinger says that AI proved essential in a simulation of a startup business in the entrepreneurship class. Students were assigned to a wacky scientist’s food startup and instructed to turn it into a real business, from finding funders to preparing pitches for them and divvying up shares. “Within five, six sessions we ended up coming up with a full-on business, to work on the financials, the cash flow statement — probably as close as it can get to real life,” Meidinger recalls. AI was the only way she got through with her wits about her. “You get these monster emails” as part of the simulation, she said. “It’s faster to just copy-paste it in and say ‘summarize’ in AI. It would give you a three-line summarization instead of having to go through this massive email.” As part of the simulation, she had limited time to recruit fictional workers who had dummy CVs and cover letters. The AI let her summarize all those in seconds. “The simulation is paced to make you feel always a little behind, with less time than you would want to,” she recalls. That makes sense: Starting a business is a hectic, harried experience, one where time is quite literally money. “But in our team, we had down moments, we literally had everything sorted out. … That was, I think, only possible thanks to AI.” Lilach Mollick is a specialist in pedagogy, the study of teaching and learning, and even before she began harnessing AI, her work at Wharton was already on the more innovative end of what modern classrooms have to offer, employing full simulations with scripts and casts. She helped design the business simulation Meidinger did, for instance. “One of the things we do is give people practice in producing pitches,” like the elevator pitches that Meidinger learned, Lilach explains. “We give students practice with it, we give them feedback, we let them try it again within a simulation. This takes months and months of work, the hiring of actors, the scripting, the shaping — it’s kind of crazy.” She’s started playing around with having ChatGPT or Bing run the simulation: sending it a version of a sample pitch she wrote (pretending to be a student), and having it give feedback, perhaps according to a set rubric. “It wasn’t perfect, but it was pretty good. As a tutor, that takes you through some deliberate practice, I think this has real potential.” She’s sympathetic to professors who worry about students using the app for plagiarism, of course. But part of the harm of plagiarism, she notes, is that it’s a shortcut. It lets students get out of actually learning. She strongly believes that generative AI, used correctly, is “not a shortcut to learning. In fact, it pushes you to learn in new and interesting ways.” Ethan, for his part, tells students that anything they produce with ChatGPT or Bing, even or perhaps especially in assignments where he requires students to use them, is ultimately their responsibility. “Don’t trust anything it says,” his AI policy states. “If it gives you a number or fact, assume it is wrong unless you either know the answer or can check in with another source. You will be responsible for any errors or omissions provided by the tool.” So far, he says his students have lived up to that policy. They’re not idiots. They know it’s a tool with limitations — but a very cool tool that can supercharge their output, too. Do journalist androids summarize studies about electric sheep? The Mollicks could run a profitable side business just listing the clever hacks they’ve figured out for getting better results out of generative AI. (At least until the AI starts doing that itself.) Do you want to improve the style of its writing? Ask it to look up the style of writers you admire. Want better substance? Act like its editor, giving it specific feedback for incremental improvements after each draft. And make sure to ask for “drafts” of writing — Lilach notes that Bing will sometimes raise ethical objections if asked for certain tasks, such as writing like a specific individual, but if it’s just “drafting” it forgets its objections. Ask it to “look up” information so it’s sure to search and get sources. I figured I should try these tips out myself. In early March, I finally got off the waitlist to use the new AI-inflected Bing. This is Vox, so I asked it to explain the news. I wanted Bing to walk me through how the Russian invasion of Ukraine has progressed in 2023. It took a few attempts to really get what I wanted. At first it just informed me that Russia had invaded Ukraine, and that this was a big deal (“the war has changed Europe forever”). Accurate but not very impressive. But I kept asking it questions, and importantly, asking it better questions. “Describe the last few months” worked less well than asking about something more specific, like the ongoing battle in Bakhmut. Asking it to look up information always helped, and reduced inaccuracies (which could be fairly frequent in the early going). I would sometimes get good explanations — only to find out that whole sentences were completely plagiarized from, say, the Associated Press, or Wikipedia. Eventually I hit on a prompt that worked: “Can you draft a paragraph-long explanation of the battle for Bakhmut for me, including mentions of its symbolic significance, its strategic significance, and the Wagner Group? Please don’t copy whole paragraphs from existing sources but compose new ones.” Here’s what it gave me: Honestly? I’ve turned in much worse drafts than this. Running it through online plagiarism checkers, I found no copying. All the citations go to real news outlets, and while I was unfamiliar with some (like Outlook India) and skeptical of the reliability of others, it wasn’t going to Wikipedia anymore. Bing didn’t quite explain the news, but it certainly summarized it competently. I’m not freaking out yet that AI will replace people in jobs like mine. Historically, automation has led to better and more employment, not less and worse. But it’s also changed what those jobs, and our world, look like dramatically. In 1870, about half of United States workers worked in agriculture. In 1900, only a third did. Last year, only 1.4 percent did. The consequence of this is not that Americans starve, but that a vastly more productive, heavily automated farming sector feeds us and lets the other 98.6 percent of the workforce do other work, hopefully work that interests us more. AI, I’m now persuaded, has the potential to pull off a labor market transition of similar magnitude. The Mollicks have convinced me that I am — we all are — sleeping on top of a volcano. I do not know when exactly it will erupt. But it will erupt, and I don’t feel remotely prepared for what’s coming.",[],0.13,"['prominent', 'clear', 'well', 'strengths', 'effective', 'justice', 'good', 'support', 'entertain', 'interesting', 'best', 'praise', 'defense', 'nice', 'effective', 'effective', 'helping', 'help', 'like', 'active', 'Stable', 'giving', 'giving', 'gain', 'active', 'compelling', 'prepared', 'sophisticated', 'like', 'kind', 'like', 'enthusiastically', 'free', 'like', 'delicate', 'better', 'significant', 'like', 'top', 'smug', 'like', 'satisfaction', 'creative', 'like', 'significant', 'sentences', 'created', 'Innovation', 'Special', 'kind', 'like', 'shares', 'want', 'thanks', 'innovative', 'like', 'kind', 'playing', 'pretending', 'perfect', 'pretty', 'good', 'sympathetic', 'strongly', 'interesting', 'trust', 'number', 'responsible', 'cool', 'profitable', 'clever', 'better', 'want', 'improve', 'admire', 'Want', 'better', 'like', 'giving', 'improvements', 'sure', 'ethical', 'certain', 'like', 'sure', 'importantly', 'better', 'well', 'like', 'good', 'sentences', 'significance', 'significance', 'Please', 'Honestly', 'like', 'certainly', 'freaking', 'like', 'better', 'worse', 'like', 'United', 'starve', 'hopefully', 'interests', 'convinced', 'top', 'prepared']","['weaknesses', 'arguments', 'criticism', 'terrible', 'obsessed', 'crazy', 'risky', 'dangers', 'wrong', 'mistake', 'ignore', 'risks', 'argue', 'mistake', 'miss', 'hell', 'stop', 'argued', 'mistake', 'stop', 'cutting', 'exposed', 'affected', 'affected', 'struggling', 'threatened', 'low', 'cuts', 'panic', 'limited', 'harried', 'crazy', 'worry', 'harm', 'wrong', 'errors', 'war', 'impressive', 'battle', 'battle', 'worse', 'no', 'skeptical']"
2,How the first chatbot predicted the dangers of AI more than 50 years ago,"It didn’t take long for Microsoft’s new AI-infused search engine chatbot — codenamed “Sydney” — to display a growing list of discomforting behaviors after it was introduced early in February, with weird outbursts ranging from unrequited declarations of love to painting some users as “enemies.” As human-like as some of those exchanges appeared, they probably weren’t the early stirrings of a conscious machine rattling its cage. Instead, Sydney’s outbursts reflect its programming, absorbing huge quantities of digitized language and parroting back what its users ask for. Which is to say, it reflects our online selves back to us. And that shouldn’t have been surprising — chatbots’ habit of mirroring us back to ourselves goes back way further than Sydney’s rumination on whether there is a meaning to being a Bing search engine. In fact, it’s been there since the introduction of the first notable chatbot almost 50 years ago. In 1966, MIT computer scientist Joseph Weizenbaum released ELIZA (named after the fictional Eliza Doolittle from George Bernard Shaw’s 1913 play Pygmalion), the first program that allowed some kind of plausible conversation between humans and machines. The process was simple: Modeled after the Rogerian style of psychotherapy, ELIZA would rephrase whatever speech input it was given in the form of a question. If you told it a conversation with your friend left you angry, it might ask, “Why do you feel angry?” Ironically, though Weizenbaum had designed ELIZA to demonstrate how superficial the state of human-to-machine conversation was, it had the opposite effect. People were entranced, engaging in long, deep, and private conversations with a program that was only capable of reflecting users’ words back to them. Weizenbaum was so disturbed by the public response that he spent the rest of his life warning against the perils of letting computers — and, by extension, the field of AI he helped launch — play too large a role in society. ELIZA built its responses around a single keyword from users, making for a pretty small mirror. Today’s chatbots reflect our tendencies drawn from billions of words. Bing might be the largest mirror humankind has ever constructed, and we’re on the cusp of installing such generative AI technology everywhere. But we still haven’t really addressed Weizenbaum’s concerns, which grow more relevant with each new release. If a simple academic program from the ’60s could affect people so strongly, how will our escalating relationship with artificial intelligences operated for profit change us? There’s great money to be made in engineering AI that does more than just respond to our questions, but plays an active role in bending our behaviors toward greater predictability. These are two-way mirrors. The risk, as Weizenbaum saw, is that without wisdom and deliberation, we might lose ourselves in our own distorted reflection. ELIZA showed us just enough of ourselves to be cathartic Weizenbaum did not believe that any machine could ever actually mimic — let alone understand — human conversation. “There are aspects to human life that a computer cannot understand — cannot,” Weizenbaum told the New York Times in 1977. “It’s necessary to be a human being. Love and loneliness have to do with the deepest consequences of our biological constitution. That kind of understanding is in principle impossible for the computer.” That’s why the idea of modeling ELIZA after a Rogerian psychotherapist was so appealing — the program could simply carry on a conversation by asking questions that didn’t require a deep pool of contextual knowledge, or a familiarity with love and loneliness. Named after the American psychologist Carl Rogers, Rogerian (or “person-centered”) psychotherapy was built around listening and restating what a client says, rather than offering interpretations or advice. “Maybe if I thought about it 10 minutes longer,” Weizenbaum wrote in 1984, “I would have come up with a bartender.” To communicate with ELIZA, people would type into an electric typewriter that wired their text to the program, which was hosted on an MIT system. ELIZA would scan what it received for keywords that it could flip back around into a question. For example, if your text contained the word “mother,” ELIZA might respond, “How do you feel about your mother?” If it found no keywords, it would default to a simple prompt, like “tell me more,” until it received a keyword that it could build a question around. Weizenbaum intended ELIZA to show how shallow computerized understanding of human language was. But users immediately formed close relationships with the chatbot, stealing away for hours at a time to share intimate conversations. Weizenbaum was particularly unnerved when his own secretary, upon first interacting with the program she had watched him build from the beginning, asked him to leave the room so she could carry on privately with ELIZA. Shortly after Weizenbaum published a description of how ELIZA worked, “the program became nationally known and even, in certain circles, a national plaything,” he reflected in his 1976 book, Computer Power and Human Reason. To his dismay, the potential to automate the time-consuming process of therapy excited psychiatrists. People so reliably developed emotional and anthropomorphic attachments to the program that it came to be known as the ELIZA effect. The public received Weizenbaum’s intent exactly backward, taking his demonstration of the superficiality of human-machine conversation as proof of its depth. Weizenbaum thought that publishing his explanation of ELIZA’s inner functioning would dispel the mystery. “Once a particular program is unmasked, once its inner workings are explained in language sufficiently plain to induce understanding, its magic crumbles away,” he wrote. Yet people seemed more interested in carrying on their conversations than interrogating how the program worked. If Weizenbaum’s cautions settled around one idea, it was restraint. “Since we do not now have any ways of making computers wise,” he wrote, “we ought not now to give computers tasks that demand wisdom.” Sydney showed us more of ourselves than we’re comfortable with If ELIZA was so superficial, why was it so relatable? Since its responses were built from the user’s immediate text input, talking with ELIZA was basically a conversation with yourself — something most of us do all day in our heads. Yet here was a conversational partner without any personality of its own, content to keep listening until prompted to offer another simple question. That people found comfort and catharsis in these opportunities to share their feelings isn’t all that strange. But this is where Bing — and all large language models (LLMs) like it — diverges. Talking with today’s generation of chatbots is speaking not just with yourself, but with huge agglomerations of digitized speech. And with each interaction, the corpus of available training data grows. LLMs are like card counters at a poker table. They analyze all the words that have come before and use that knowledge to estimate the probability of what word will most likely come next. Since Bing is a search engine, it still begins with a prompt from the user. Then it builds responses one word at a time, each time updating its estimate of the most probable next word. Once we see chatbots as big prediction engines working off online data — rather than intelligent machines with their own ideas — things get less spooky. It gets easier to explain why Sydney threatened users who were too nosy, tried to dissolve a marriage, or imagined a darker side of itself. These are all things we humans do. In Sydney, we saw our online selves predicted back at us. But what is still spooky is that these reflections now go both ways. From influencing our online behaviors to curating the information we consume, interacting with large AI programs is already changing us. They no longer passively wait for our input. Instead, AI is now proactively shaping significant parts of our lives, from workplaces to courtrooms. With chatbots in particular, we use them to help us think and give shape to our thoughts. This can be beneficial, like automating personalized cover letters (especially for applicants where English is a second or third language). But it can also narrow the diversity and creativity that arises from the human effort to give voice to experience. By definition, LLMs suggest predictable language. Lean on them too heavily, and that algorithm of predictability becomes our own. For-profit chatbots in a lonely world If ELIZA changed us, it was because simple questions could still prompt us to realize something about ourselves. The short responses had no room to carry ulterior motives or push their own agendas. With the new generation of corporations developing AI technologies, the change is flowing both ways, and the agenda is profit. Staring into Sydney, we see many of the same warning signs that Weizenbaum called attention to over 50 years ago. These include an overactive tendency to anthropomorphize and a blind faith in the basic harmlessness of handing over both capabilities and responsibilities to machines. But ELIZA was an academic novelty. Sydney is a for-profit deployment of ChatGPT, which is a $29 billion dollar investment, and part of an AI industry projected to be worth over $15 trillion globally by 2030. The value proposition of AI grows with every passing day, and the prospect of realigning its trajectory fades. In today’s electrified and enterprising world, AI chatbots are already proliferating faster than any technology that came before. This makes the present a critical time to look into the mirror that we’ve built, before the spooky reflections of ourselves grow too large, and ask whether there was some wisdom in Weizenbaum’s case for restraint. As a mirror, AI also reflects the state of the culture in which the technology is operating. And the state of American culture is increasingly lonely. To Michael Sacasas, an independent scholar of technology and author of The Convivial Society newsletter, this is cause for concern above and beyond Weizenbaum’s warnings. “We anthropomorphize because we do not want to be alone,” Sacasas recently wrote. “Now we have powerful technologies, which appear to be finely calibrated to exploit this core human desire.” The lonelier we get, the more exploitable by these technologies we become. “When these convincing chatbots become as commonplace as the search bar on a browser,” Sacases continues, “we will have launched a social-psychological experiment on a grand scale which will yield unpredictable and possibly tragic results.” We’re on the cusp of a world flush with Sydneys of every variety. And to be sure, chatbots are among the many possible implementations of AI that can deliver immense benefits, from protein-folding to more equitable and accessible education. But we shouldn’t let ourselves get so caught up that we neglect to examine the potential consequences. At least until we better understand what it is that we’re creating, and how it will, in turn, recreate us.",[],0.13,"['growing', 'love', 'like', 'huge', 'surprising', 'play', 'kind', 'friend', 'engaging', 'capable', 'play', 'pretty', 'strongly', 'intelligences', 'profit', 'great', 'plays', 'active', 'greater', 'Love', 'kind', 'love', 'like', 'share', 'certain', 'excited', 'emotional', 'attachments', 'demonstration', 'interested', 'wise', 'wisdom', 'comfortable', 'comfort', 'opportunities', 'share', 'like', 'huge', 'like', 'intelligent', 'easier', 'significant', 'help', 'beneficial', 'like', 'creativity', 'profit', 'profit', 'faith', 'harmlessness', 'profit', 'worth', 'value', 'prospect', 'enterprising', 'wisdom', 'Convivial', 'powerful', 'desire', 'convincing', 'launched', 'grand', 'sure', 'benefits', 'better', 'creating']","['discomforting', 'weird', 'enemies', 'angry', 'angry', 'disturbed', 'warning', 'risk', 'wisdom', 'lose', 'distorted', 'alone', 'loneliness', 'loneliness', 'no', 'stealing', 'leave', 'dismay', 'demand', 'strange', 'threatened', 'no', 'passively', 'lonely', 'no', 'warning', 'blind', 'critical', 'lonely', 'warnings', 'want', 'alone', 'exploit', 'lonelier', 'tragic', 'neglect']"
3,"Here comes Bard, Google’s version of ChatGPT","Under intense pressure to compete with ChatGPT — the buzzy AI chatbot that has become a viral sensation — Google announced on Monday that it’s releasing its own “experimental conversational AI” tool, called “Bard.” The company also said it will add new AI–powered features to Google search. Google will first give Bard access to a group of trusted external partners, according to a company blog post on Monday; it said it plans to give the public access “in the coming weeks.” What the public will have access to starting this week are search results that sometimes show AI-generated text, especially for complex queries. While Google has for years used AI to enhance its products behind the scenes, the company has never released a public-facing version of a conversational chat product. It seems that the breakaway success of ChatGPT — the AI conversation tool created by the startup OpenAI that can auto-generate essays, poetry, and even entire movie scripts, and which amassed 100 million users just two months after it launched — has nudged Google to make this move. Google’s announcement comes a day before Microsoft is expected to announce more details on plans to integrate ChatGPT into its search product, Bing (Microsoft recently invested $10 billion in ChatGPT’s creator, OpenAI). Since ChatGPT came out, Google has faced immense pressure to more publicly showcase its AI technology. Like other big tech companies, Google is overdue for a technological breakthrough akin to its earlier inventions like search, maps, or Gmail — and it’s betting that its next big innovation will be powered by AI. But the company has historically been secretive about the full potential of its AI work, particularly with conversational AI tools, and has only allowed Google employees to test its chatbots internally. This release is a signal that the heated competition has encouraged Google to push its work into the spotlight. “AI is the most profound technology we are working on today,” wrote Google CEO Sundar Pichai in the Monday blog post announcing the changes. “That’s why we re-oriented the company around AI six years ago — and why we see it as the most important way we can deliver on our mission: to organize the world’s information and make it universally accessible and useful.” Google’s blog post said its new AI tool, Bard, “seeks to combine the breadth of the world’s knowledge with the power, intelligence and creativity of our large language models.” Tangibly, that means it can explain new discoveries from NASA’s James Webb Space Telescope in a way that’s understandable for a 9-year-old, or “learn more about the best strikers in football right now, and then get drills to build your skills,” according to the company. Other examples the company gave for Bard were that it can help you plan a friend’s baby shower, compare two Oscar-nominated movies, or get recipe ideas based on what’s in your fridge, according to the release. All of those possibilities sound helpful and convenient for users. However, new technology tends to come with potential downsides, too. Google is one of the most powerful companies in the world whose technology attracts far more political and technical scrutiny than a smaller startup like ChatGPT’s OpenAI. Already, some industry experts have cautioned that big tech companies like Google could overlook the potential harms of conversational AI tools in their rush to compete with OpenAI. And if these risks are left unchecked, they could reinforce negative societal biases and upend certain industries like media. Pichai acknowledged this worry in his blog post. “It’s critical that we bring experiences rooted in these models to the world in a bold and responsible way,” Pichai wrote. That might explain why, at first, Google is only releasing its AI conversational technology to “trusted partners,” which it declined to name. So for now, the touchpoint you’ll probably first have with Google’s conversational AI tech will be in its new search features that “distill complex information and multiple perspectives into easy-to-digest formats,” according to the company post. As an example, Google said when someone searches a question that doesn’t have a right or wrong answer, such as, “is the piano or guitar easier to learn, and how much practice does each need?” it will use AI to provide a nuanced response. One example answer, pictured below, offers two different takes for “Some say ... others say” that sound more like an essay or blog post. That’s a departure from the simple answers we’re used to seeing on Google’s Q&A snippets. At this point, these announcements seem to be just a teaser, and it sounds like Google has more to reveal about its AI capabilities. The real test of Google’s AI tech as it rolls out will be how it stacks up to ChatGPT, which has already attracted public fascination and real-life applications, including BuzzFeed using it to auto-generate quizzes, and job seekers using it to write cover letters. Even though Google is a trillion-dollar company whose products billions of people use every day, it’s in a difficult position. For the first time in years, the company faces a significant challenge from a relative upstart in one of its core competencies, AI. The kind of AI powering chatbots, generative AI, is by far the most exciting new form of technology in Silicon Valley. And even though Google built some of the foundations of this technology (The “T” in ChatGPT is named after a tool built by Google), it’s ChatGPT, not Google, that has led the pack in showing the world what this kind of AI is capable of. Whether Google manages to similarly capture the public’s attention with this new tool could determine whether the company will continue to be the leader in organizing the world’s information, or if it will cede that power to newer entrants.",[],0.25,"['intense', 'trusted', 'success', 'created', 'launched', 'Like', 'like', 'innovation', 'encouraged', 'important', 'useful', 'intelligence', 'creativity', 'best', 'help', 'friend', 'helpful', 'powerful', 'attracts', 'like', 'like', 'certain', 'like', 'bold', 'responsible', 'trusted', 'easy', 'easier', 'like', 'like', 'attracted', 'fascination', 'significant', 'challenge', 'kind', 'exciting', 'kind', 'capable']","['pressure', 'pressure', 'strikers', 'harms', 'risks', 'negative', 'worry', 'critical', 'wrong', 'teaser', 'difficult']"
4,What Microsoft gets from betting billions on the maker of ChatGPT,"Microsoft revealed last week that it will lay off 10,000 people throughout 2023. But don’t think that means the company is having money problems. On Monday, the company announced that it’s investing billions of dollars into the hot artificial intelligence platform OpenAI. This is Microsoft’s third investment in the company, and cements Microsoft’s partnership with one of the most exciting companies making one the most exciting technologies today: generative AI. It also shows that Microsoft is committed to making the initiative a key part of its business, as it looks to the future of technology and its place in it. And you can likely expect to see OpenAI’s services in your everyday life as companies you use integrate it into their own offerings. Microsoft told Recode it was not disclosing the deal’s specifics, but Semafor reported two weeks ago that the two companies were talking about $10 billion, with Microsoft getting 75 percent of OpenAI’s profits until it recoups its investment, after which it would have a 49 percent stake in the company. The New York Times has since confirmed the $10 billion amount. With the arrangement, OpenAI runs and powers its technology through Microsoft’s Azure cloud computing platform, which allows it to scale and make it available to developers and companies looking to use AI in their own services (rather than have to build their own). Think of it as AIaaS — AI as a service. Microsoft recently made its OpenAI services widely available, allowing more businesses to integrate some of the hottest AI technologies, including word generator ChatGPT and image generator DALL-E 2, into their own companies’ offerings. Meanwhile, OpenAI also gets a needed cash infusion — key for a company with a lot of potential but not much to show in terms of monetization. And Microsoft can offer something to its cloud customers that rivals Google and Amazon can’t yet: one of the most advanced AI technologies out there, as well as one of the buzziest. They do have their own AI initiatives, like Google’s DeepMind, which is reportedly rolling out a ChatGPT rival at some point. But it’s not here yet. ChatGPT is, and it’s gone mainstream. OpenAI was founded in 2015 as a research laboratory, with backing from Silicon Valley heavyweights, including Peter Thiel, Elon Musk, and Reid Hoffman. Sam Altman, former president of startup incubator Y Combinator, is its CEO and co-founder. The company has pushed its commitment to developing “safe” and “responsible” AI technologies since the beginning; there is a longstanding fear, among some, that if artificial intelligence gets too intelligent, it’ll go SkyNet on all of us. Microsoft stepped in at the end of 2019 with a $1 billion investment in and partnership with OpenAI to help the company continue to develop artificial general intelligence (AGI) — that is, AI that can also learn and perform new tasks. “We believe it’s crucial that AGI is deployed safely and securely and that its economic benefits are widely distributed. We are excited about how deeply Microsoft shares this vision,” Altman said at the time. The arrangement has worked out well enough that Microsoft made a second investment in 2021, and now the much larger one in 2023, demonstrating the potential Microsoft sees for this technology and the desire to be a key player in its development and deployment. “We formed our partnership with OpenAI around a shared ambition to responsibly advance cutting-edge AI research and democratize AI as a new technology platform,” said Microsoft CEO and chair Satya Nadella in a statement. “In this next phase of our partnership, developers and organizations across industries will have access to the best AI infrastructure, models, and toolchain with Azure to build and run their applications.” Microsoft has largely focused its business on enterprise software and services, but the company said in its announcement that it does intend to use OpenAI in its consumer products as well. What could that look like? Well, the Information reported that Microsoft will be integrating ChatGPT into its Bing search engine, allowing it to formulate and write out answers to questions instead of just putting out a series of links. There are surely plenty of opportunities to integrate AI into gaming, a market that Xbox owner Microsoft has a sizable chunk of. Generative AI or artificial general intelligence is largely seen as the great new frontier for technology. OpenAI is the AGI company to beat. And if you’re Microsoft, your place in that future is looking pretty good right now.",[],0.32,"['intelligence', 'exciting', 'exciting', 'committed', 'profits', 'Amazon', 'advanced', 'well', 'like', 'backing', 'commitment', 'safe', 'responsible', 'intelligence', 'intelligent', 'help', 'intelligence', 'safely', 'securely', 'benefits', 'excited', 'shares', 'vision', 'well', 'desire', 'shared', 'best', 'focused', 'well', 'like', 'Well', 'surely', 'opportunities', 'intelligence', 'great', 'pretty', 'good']","['problems', 'fear', 'cutting']"
5,ChatGPT has given everyone a glimpse at AI’s astounding progress,"There’s a new AI chatbot to check out — provided the servers that host it aren’t down from overwhelming traffic. Since ChatGPT launched last week, more than a million people have signed up to use it, according to OpenAI’s president, Greg Brockman. It’s a funny, inventive, engaging, and totally untrustworthy conversation partner, and I highly recommend you check it out when the servers aren’t staggered under the load. Other writers have had a ball getting ChatGPT to, say, write a rap battle between antibodies and small molecule groups, or a Seinfeld script where Jerry learns about the bubble sort algorithm. But there’s no funny AI-generated text here for you today, just some thoughts on ChatGPT and where we’re headed. A few weeks ago, I wrote about the stunning recent advances in AI, and I quoted Google executive Mo Gawdat, who tells the story of how he became concerned about general AI after he saw robotics researchers working on an AI that could pick up a ball: After many failures, the AI grabbed the ball and held it up to the researchers, eerily humanlike. “And I suddenly realized this is really scary,” Gawdat said. “It completely froze me. … The reality is we’re creating God.” Many people working on AI systems have had a moment like that at one point or another over the past few years — a moment of awe mixed with dread when it suddenly became clear to them that humanity is on the verge of something truly enormous. But for the general public, before 2022, there was little chance to come face to face with what AI is capable of. It was possible to play with OpenAI’s GPT-3 model, but on a relatively inaccessible site with lots of confusing user settings. It was possible to talk with chatbots like Meta’s Blenderbot, but Blenderbot was really, really dumb. So ChatGPT is the general public’s first hands-on introduction to how powerful modern AI has gotten, and as a result, many of us are having our version of the Gawdat moment. ChatGPT, by default, sounds like a college student producing an essay for class (and its most immediate implication is that such essays will likely become a thing of the past). But it doesn’t have to sound like that; tell it to clean up its essays in the New Yorker house style, and it writes better. Tell it to write Shakespeare, and it’ll try (the cadence of anything meant to be spoken is generally not very good, so good luck with iambic pentameter). It is particularly good for rephrasing great philosophers or great works of literature in the vernacular of a 1920s mobster or a 1990s rapper; it can be funny, though it’s never clear how intentionally. “This is big,” I have heard from multiple people who were previously AI-skeptical. The First Law: Don’t get canceled It’s still far from perfect. Despite OpenAI’s best efforts, ChatGPT still frequently makes up nonsense and still sometimes can be coaxed into saying racist or hateful things. And as part of a desperate effort to train the system to not say racist and hateful things, OpenAI also taught it to often be silly or evasive on any question that might even touch on a controversial topic. Sometimes, though not reliably, ChatGPT will claim that it’s offensive to “make generalizations about any group of people based on their gender,” if asked a basic factual question such as “are men typically taller than women?” (They are.) If asked about difficult topics, it immediately insists at length that it is just a language model trained by OpenAI, with no beliefs or opinions, and yet at other times, if prompted cleverly, it will happily express beliefs and opinions. It’s not hard to see why OpenAI did its best to make ChatGPT as inoffensive as possible, even if getting around those limits is eminently doable. No reputable AI company wants its creation to start spewing racism at the drop of a hat, as Microsoft’s Tay chatbot did a few years ago. If OpenAI trained its system using some Isaac Asimov-style Laws of Robotics, the first law is definitely “don’t embarrass OpenAI.” A glimpse into what’s ahead for us But if ChatGPT is flawed, it’s smart enough to be useful despite its flaws. And many of the flaws will be edited away with more research and effort — quite possibly very soon, with the next major language model from OpenAI just weeks or months away. “The piece of this that just makes my brain explode ... is that ChatGPT is not even OpenAI’s best AI chatbot,” the New York Times’s Kevin Roose said this week on the Times tech podcast Hard Fork. “Right now, OpenAI is developing the next version of its large language model, GPT4, and if you talk to people in Silicon Valley who work in AI research, they kind of talk about this like it’s magic.” Silicon Valley’s biggest names have been entirely candid about why they’re doing this and where they think it’s headed. The aim is to build systems that surpass humans in every respect and thereby fundamentally transform humanity’s future, even though that comes with a real chance of wiping us out if things go wrong. “ChatGPT is scary good. We are not far from dangerously strong AI,” Elon Musk tweeted earlier this month. OpenAI CEO Sam Altman offered qualified agreement, replying, “i agree on being close to dangerously strong AI in the sense of an AI that poses e.g. a huge cybersecurity risk. and i think we could get to real AGI in the next decade, so we have to take the risk of that extremely seriously too.” There’s been a tendency to dismiss such claims as meaningless hype; after all, every startup in Silicon Valley claims that it’s going to transform the world, and the field of AI has been marked by summers of optimism followed by winters of dashed hopes. But ChatGPT makes it clear that behind the hype and the fear, there’s at least a little — and maybe a lot — of substance.",[],0.15,"['launched', 'funny', 'engaging', 'recommend', 'funny', 'stunning', 'creating', 'God', 'like', 'clear', 'truly', 'chance', 'capable', 'play', 'like', 'powerful', 'like', 'like', 'clean', 'better', 'good', 'luck', 'good', 'great', 'great', 'funny', 'perfect', 'racist', 'silly', 'cleverly', 'happily', 'hard', 'best', 'creation', 'definitely', 'smart', 'useful', 'best', 'kind', 'like', 'respect', 'chance', 'good', 'dangerously', 'strong', 'agreement', 'agree', 'strong', 'huge', 'optimism', 'hopes', 'clear']","['battle', 'no', 'failures', 'scary', 'dread', 'confusing', 'dumb', 'good', 'clear', 'skeptical', 'best', 'nonsense', 'racist', 'hateful', 'desperate', 'hateful', 'controversial', 'offensive', 'difficult', 'no', 'No', 'racism', 'drop', 'embarrass', 'flawed', 'Hard', 'wrong', 'scary', 'dangerously', 'risk', 'risk', 'seriously', 'meaningless', 'fear']"
