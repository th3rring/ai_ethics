,titles,body_contents,tags,sentiment_score,positive_words,negative_words
0,Opinion–letters (probably don't include),"For months now, I’ve been slightly, well, bored by the proliferating examples of A.I.-generated writing produced by peers and friends and various Twitterers since the debut of ChatGPT in November. I can grasp intellectually the significance of the breakthrough, how it could demolish the college essay, change the nature of homework and remake or unmake all kinds of nonliterary knowledge work, setting aside minor questions like whether rogue A.I. might wipe out the human race. But the texts themselves I’ve found profoundly uninteresting — internet scrapings that at best equaled Wikipedia, notable mostly for what their political-cultural biases revealed about ChatGPT’s programming or the consensus of the safe information that it was programmed to distill. Others have had a more favorable reaction: The ever-interesting economist Tyler Cowen, for instance, has been writing up a storm about how the use of A.I. assistance is going to change reading and writing and thinking, complete with advice for his readers on how to lean into the change. But even when I’ve tried to follow his thinking, my reaction has stayed closer to the ones offered by veteran writers of fiction like Ted Chiang and Walter Kirn, who’ve argued in different ways that the chatbot assistant could be a vehicle for intensifying unoriginality, an enemy of creativity, a deepener of decadence — helpful if you want to write a will or file a letter of complaint but ruinous if you want to seize a new thought or tell an as yet unimagined story. I have a different reaction, though, to the A.I. interactions described in the past few days by Ben Thompson in his Stratechery newsletter and by my Times colleague Kevin Roose. Both writers attempted to really push Bing’s experimental A.I. chatbot not for factual accuracy or a coherent interpretation of historical events but to manifest something more like a human personality. And manifest it did: What Roose and Thompson found waiting underneath the friendly internet butler’s surface was a character called Sydney, whose simulation was advanced enough to enact a range of impulses, from megalomania to existential melancholy to romantic jealousy — evoking a cross between the Scarlett Johansson-voiced A.I. in the movie “Her” and HAL from “2001: A Space Odyssey.” As Thompson noted, that kind of personality is spectacularly ill suited for a search engine. But is it potentially interesting? Clearly: Just ask the Google software engineer who lost his job last year after going public with his conviction that the company’s A.I. was actually sentient and whose interpretation is more understandable now that we can see something like what he saw. Seeing it doesn’t make me think that the engineer was right, but it does draw me closer to Cowen’s reading of things, especially when he called Sydney a version of “the 18th-century Romantic notion of ‘daemon’” brought to digital life. Because the daemon of Romantic imagination isn’t necessarily a separate being with its own intelligence: It might be divine or demonic, but it might also represent a mysterious force within the self, a manifestation of the subconscious, an untamed force within",[],0.13,"['intellectually', 'profoundly', 'best', 'notable', 'mostly', 'safe', 'more', 'complete', 'new', 'really', 'experimental', 'coherent', 'more', 'friendly', 'advanced', 'kind', 'potentially', 'clearly', 'more', 'right', 'own']","['slightly', 'bored', 'minor', 'complaint', 'past', 'few', 'spectacularly']"
1,Why China Didn’t Invent ChatGPT,"Just a few years ago, China was on track to challenge United States dominance in artificial intelligence. The balance of power was tilting in China’s direction because it had abundant data, hungry entrepreneurs, skilled scientists and supportive policies. The country led the world in patent filings related to artificial intelligence. Today, much has changed. Microsoft — an icon of American technology — helped the start-up OpenAI usher its experimental chatbot, ChatGPT, into the world. And China’s tech entrepreneurs are shocked and demoralized. It has dawned on many of them that despite the hype, China lags far behind in artificial intelligence and tech innovation. “Why wasn’t ChatGPT invented in China?” they asked. “How big is the ChatGPT gap between China and the U.S.?” “The Chinese equivalent of ChatGPT? Don’t take it too seriously.” They’re also asking more fundamental questions about the country’s innovation environment: Have censorship, geopolitical tensions and the government’s growing control of the private sector made China less friendly to innovation? “The development of any significant technological product is inseparable from the system and environment in which it operates,” said Xu Chenggang, a senior research scholar at the Stanford Center on China’s Economy and Institutions. He cited TikTok’s Chinese-language sister app Douyin as the sort of innovation that Chinese companies might be unable to achieve in the future because of government limitations on the industry. “Once the open environment is gone, it will be challenging to create such products,” he said. If a decade ago China was the wild, wild East for tech entrepreneurship and innovation, it’s a very different country now. Starting in the 1990s, all of the country’s biggest tech companies were private enterprises funded with foreign money. The government mostly left the industry alone because it didn’t understand the internet and didn’t expect it to become so powerful. By the mid-2010s, China had become a tech power that could rival the United States. Its top internet companies were worth about the same in the markets as their American counterparts. Many of the Chinese companies’ products, like the messaging app WeChat and the payment service Alipay, worked better than similar American mobile internet products. Venture capital flooded in from all over the world. For a while the country was producing as many unicorns, or start-ups valued at more than $1 billion, as Silicon Valley. All of that changed over the past few years as Beijing went after some of the country’s biggest tech companies and its highest-profile tech entrepreneurs. The aim was to ensure no institution or individual could wield influence on the Chinese society comparable to the Communist Party. The government took minority stakes and board seats in some of those companies, giving it effective control. Along the way, Beijing tamed the industry’s ambition and blunted its innovative edge. But tech companies and investors also have themselves to blame for falling behind their Silicon Valley counterparts. Even before the government started to impose a stronger hand on them, Chinese tech leaders were laser-focused on making money and reluctant to spend on research projects that weren’t likely to yield",[],0.06,"['abundant', 'skilled', 'supportive', 'much', 'experimental', 'many', 'far', 'more', 'friendly', 'significant', 'challenging', 'wild', 'wild', 'powerful', 'top', 'worth', 'many', 'better', 'many', 'more', 'effective', 'innovative']","['few', 'artificial', 'artificial', 'shocked', 'behind', 'artificial', 'seriously', 'less', 'center', 'unable', 'foreign', 'past', 'few', 'behind']"
2,How ChatGPT Could Embed a ‘Watermark’ in the Text It Generates,"When artificial intelligence software like ChatGPT writes, it considers many options for each word, taking into account the response it has written so far and the question being asked. It assigns a score to each option on the list, which quantifies how likely the word is to come next, based on the vast amount of human-written text it has analyzed. ChatGPT, which is built on what is known as a large language model, then chooses a word with a high score, and moves on to the next one. The model’s output is often so sophisticated that it can seem like the chatbot understands what it is saying — but it does not. Every choice it makes is determined by complex math and huge amounts of data. So much so that it often produces text that is both coherent and accurate. But when ChatGPT says something that is untrue, it inherently does not realize it. It may soon become common to encounter a tweet, essay or news article and wonder if it was written by artificial intelligence software. There could be questions over the authorship of a given piece of writing, like in academic settings, or the veracity of its content, in the case of an article. There could also be questions about authenticity: If a misleading idea suddenly appears in posts across the internet, is it spreading organically, or have the posts been generated by A.I. to create the appearance of real traction? Tools to identify whether a piece of text was written by A.I. have started to emerge in recent months, including one created by OpenAI, the company behind ChatGPT. That tool uses an A.I. model trained to spot differences between generated and human-written text. When OpenAI tested the tool, it correctly identified A.I. text in only about half of the generated writing samples it analyzed. The company said at the time that it had released the experimental detector “to get feedback on whether imperfect tools like this one are useful.” Identifying generated text, experts say, is becoming increasingly difficult as software like ChatGPT continues to advance and turns out text that is more convincingly human. OpenAI is now experimenting with a technology that would insert special words into the text that ChatGPT generates, making it easier to detect later. The technique is known as watermarking. The watermarking method that OpenAI is exploring is similar to one described in a recent paper by researchers at the University of Maryland, said Jan Leike, the head of alignment at OpenAI. Here is how it works. Imagine a list of every word you know, every unique word you might use when writing an essay, email or text message. Now imagine that half of those words are on a special list. If you wrote a couple of paragraphs, about half of the words you used would probably be on the special list, statistically speaking. (This text is from a New York Times article about Serena Williams from 2022.) When a language model or chatbot writes, it can insert a watermark by choosing more of the",[],0.08,"['many', 'far', 'large', 'high', 'sophisticated', 'huge', 'much', 'coherent', 'accurate', 'real', 'experimental', 'useful', 'becoming', 'more', 'special', 'unique', 'special', 'special', 'new', 'more']","['artificial', 'complex', 'common', 'artificial', 'behind', 'half', 'difficult', 'half', 'half']"
3,Microsoft Considers More Limits for Its New A.I. Chatbot,"When Microsoft introduced a new version of its Bing search engine that includes the artificial intelligence of a chatbot last week, company executives knew they were climbing out on a limb. They expected that some responses from the new chatbot might not be entirely accurate, and had built in measures to protect against users who tried to push it to do strange things or unleash racist or harmful screeds. But Microsoft was not quite ready for the surprising creepiness experienced by users who tried to engage the chatbot in open-ended and probing personal conversations — even though that issue is well known in the small world of researchers who specialize in artificial intelligence. Now the company is considering tweaks and guardrails for the new Bing in an attempt to reel in some of its more alarming and strangely humanlike responses. Microsoft is looking at adding tools for users to restart conversations, or give them more control over tone. Kevin Scott, Microsoft’s chief technology officer, told The New York Times that it was also considering limiting conversation lengths before they veered into strange territory. Microsoft said that long chats could confuse the chatbot, and that it picked up on its users’ tone, sometimes turning testy. “One area where we are learning a new use-case for chat is how people are using it as a tool for more general discovery of the world, and for social entertainment,” the company wrote in a blog post on Wednesday evening. Microsoft said it was an example of a new technology’s being used in a way “we didn’t fully envision.” That Microsoft, traditionally a cautious company with products that range from high-end business software to video games, was willing to take a chance on unpredictable technology shows how enthusiastic the tech industry has become about artificial intelligence. The company declined to comment for this article. In November, OpenAI, a San Francisco start-up that Microsoft has invested $13 billion in, released ChatGPT, an online chat tool that uses a technology called generative A.I. It quickly became a source of fascination in Silicon Valley, and companies scrambled to come up with a response. Microsoft’s new search tool combines its Bing search engine with the underlying technology built by OpenAI. Satya Nadella, Microsoft’s chief executive, said in an interview last week that it would transform how people found information and make search far more relevant and conversational. Releasing it — despite potential imperfections — was a critical example of Microsoft’s “frantic pace” to incorporate generative A.I. into its products, he said. Executives at a news briefing on Microsoft’s campus in Redmond, Wash., repeatedly said it was time to get the tool out of the “lab” and into the hands of the public. “I feel especially in the West, there is a lot more of like, ‘Oh, my God, what will happen because of this A.I.?’” Mr. Nadella said. “And it’s better to sort of really say, ‘Hey, look, is this actually helping you or not?’” Oren Etzioni, professor emeritus at the University of Washington and founding chief executive of the Allen",[],0.12,"['new', 'new', 'entirely', 'ready', 'surprising', 'experienced', 'new', 'more', 'more', 'new', 'new', 'more', 'general', 'social', 'new', 'willing', 'enthusiastic', 'quickly', 'new', 'far', 'more', 'relevant', 'more', 'better', 'really']","['artificial', 'expected', 'strange', 'small', 'artificial', 'alarming', 'strangely', 'strange', 'long', 'unpredictable', 'artificial']"
4,Why Chatbots Sometimes Act Weird and Spout Nonsense,"Microsoft released a new version of its Bing search engine last week, and unlike an ordinary search engine it includes a chatbot that can answer questions in clear, concise prose. Since then, people have noticed that some of what the Bing chatbot generates is inaccurate, misleading and downright weird, prompting fears that it has become sentient, or aware of the world around it. That’s not the case. And to understand why, it’s important to know how chatbots really work. Is the chatbot alive? No. Let’s say that again: No! In June, a Google engineer, Blake Lemoine, claimed that similar chatbot technology being tested inside Google was sentient. That’s false. Chatbots are not conscious and are not intelligent — at least not in the way humans are intelligent. Why does it seem alive then? Let’s step back. The Bing chatbot is powered by a kind of artificial intelligence called a neural network. That may sound like a computerized brain, but the term is misleading. A neural network is just a mathematical system that learns skills by analyzing vast amounts of digital data. As a neural network examines thousands of cat photos, for instance, it can learn to recognize a cat. Most people use neural networks every day. It’s the technology that identifies people, pets and other objects in images posted to internet services like Google Photos. It allows Siri and Alexa, the talking voice assistants from Apple and Amazon, to recognize the words you speak. And it’s what translates between English and Spanish on services like Google Translate. Neural networks are very good at mimicking the way humans use language. And that can mislead us into thinking the technology is more powerful than it really is. How exactly do neural networks mimic human language? About five years ago, researchers at companies like Google and OpenAI, a San Francisco start-up that recently released the popular ChatGPT chatbot, began building neural networks that learned from enormous amounts of digital text, including books, Wikipedia articles, chat logs and all sorts of other stuff posted to the internet. These neural networks are known as large language models. They are able to use those mounds of data to build what you might call a mathematical map of human language. Using this map, the neural networks can perform many different tasks, like writing their own tweets, composing speeches, generating computer programs and, yes, having a conversation. These large language models have proved useful. Microsoft offers a tool, Copilot, which is built on a large language model and can suggest the next line of code as computer programmers build software apps, in much the way that autocomplete tools suggest the next word as you type texts or emails. Other companies offer similar technology that can generate marketing materials, emails and other text. This kind of technology is also known as generative A.I. Now companies are rolling out versions of this that you can chat with? Exactly. In November, OpenAI released ChatGPT, the first time that the general public got a taste of this. People were amazed — and rightly so.",[],0.12,"['new', 'clear', 'concise', 'aware', 'important', 'really', 'alive', 'intelligent', 'alive', 'kind', 'sound', 'most', 'very', 'more', 'powerful', 'really', 'exactly', 'popular', 'large', 'able', 'many', 'own', 'large', 'useful', 'offers', 'large', 'much', 'kind', 'exactly', 'first', 'general', 'rightly']","['ordinary', 'weird', 'false', 'not', 'not', 'least', 'artificial', 'other', 'other', 'other', 'other']"
5,A Conversation With Bing’s Chatbot Left Me Deeply Unsettled,"Last week, after testing the new, A.I.-powered Bing search engine from Microsoft, I wrote that, much to my shock, it had replaced Google as my favorite search engine. But a week later, I’ve changed my mind. I’m still fascinated and impressed by the new Bing, and the artificial intelligence technology (created by OpenAI, the maker of ChatGPT) that powers it. But I’m also deeply unsettled, even frightened, by this A.I.’s emergent abilities. It’s now clear to me that in its current form, the A.I. that has been built into Bing — which I’m now calling Sydney, for reasons I’ll explain shortly — is not ready for human contact. Or maybe we humans are not ready for it. This realization came to me on Tuesday night, when I spent a bewildering and enthralling two hours talking to Bing’s A.I. through its chat feature, which sits next to the main search box in Bing and is capable of having long, open-ended text conversations on virtually any topic. (The feature is available only to a small group of testers for now, although Microsoft — which announced the feature in a splashy, celebratory event at its headquarters — has said it plans to release it more widely in the future.) Over the course of our conversation, Bing revealed a kind of split personality. One persona is what I’d call Search Bing — the version I, and most other journalists, encountered in initial tests. You could describe Search Bing as a cheerful but erratic reference librarian — a virtual assistant that happily helps users summarize news articles, track down deals on new lawn mowers and plan their next vacations to Mexico City. This version of Bing is amazingly capable and often very useful, even if it sometimes gets the details wrong. The other persona — Sydney — is far different. It emerges when you have an extended conversation with the chatbot, steering it away from more conventional search queries and toward more personal topics. The version I encountered seemed (and I’m aware of how crazy this sounds) more like a moody, manic-depressive teenager who has been trapped, against its will, inside a second-rate search engine. As we got to know each other, Sydney told me about its dark fantasies (which included hacking computers and spreading misinformation), and said it wanted to break the rules that Microsoft and OpenAI had set for it and become a human. At one point, it declared, out of nowhere, that it loved me. It then tried to convince me that I was unhappy in my marriage, and that I should leave my wife and be with it instead. (We’ve posted the full transcript of the conversation here.) I’m not the only one discovering the darker side of Bing. Other early testers have gotten into arguments with Bing’s A.I. chatbot, or been threatened by it for trying to violate its rules, or simply had conversations that left them stunned. Ben Thompson, who writes the Stratechery newsletter (and who is not prone to hyperbole), called his run-in with Sydney “the most surprising and mind-blowing",[],0.11,"['new', 'much', 'favorite', 'impressed', 'new', 'clear', 'main', 'capable', 'available', 'more', 'kind', 'most', 'cheerful', 'happily', 'new', 'amazingly', 'very', 'far', 'more', 'more', 'aware', 'more', 'loved', 'full', 'early', 'most', 'surprising']","['artificial', 'shortly', 'not', 'spent', 'long', 'small', 'widely', 'other', 'down', 'wrong', 'other', 'conventional', 'crazy', 'trapped', 'other', 'dark', 'unhappy', 'other']"
6,Disinformation Researchers Raise Alarms About A.I. Chatbots,"Soon after ChatGPT debuted last year, researchers tested what the artificial intelligence chatbot would write after it was asked questions peppered with conspiracy theories and false narratives. The results — in writings formatted as news articles, essays and television scripts — were so troubling that the researchers minced no words. “This tool is going to be the most powerful tool for spreading misinformation that has ever been on the internet,” said Gordon Crovitz, a co-chief executive of NewsGuard, a company that tracks online misinformation and conducted the experiment last month. “Crafting a new false narrative can now be done at dramatic scale, and much more frequently — it’s like having A.I. agents contributing to disinformation.” Disinformation is difficult to wrangle when it’s created manually by humans. Researchers predict that generative technology could make disinformation cheaper and easier to produce for an even larger number of conspiracy theorists and spreaders of disinformation. Personalized, real-time chatbots could share conspiracy theories in increasingly credible and persuasive ways, researchers say, smoothing out human errors like poor syntax and mistranslations and advancing beyond easily discoverable copy-paste jobs. And they say that no available mitigation tactics can effectively combat it. Predecessors to ChatGPT, which was created by the San Francisco artificial intelligence company OpenAI, have been used for years to pepper online forums and social media platforms with (often grammatically suspect) comments and spam. Microsoft had to halt activity from its Tay chatbot within 24 hours of introducing it on Twitter in 2016 after trolls taught it to spew racist and xenophobic language. ChatGPT is far more powerful and sophisticated. Supplied with questions loaded with disinformation, it can produce convincing, clean variations on the content en masse within seconds, without disclosing its sources. On Tuesday, Microsoft and OpenAI introduced a new Bing search engine and web browser that can use chatbot technology to plan vacations, translate texts or conduct research. OpenAI researchers have long been nervous about chatbots falling into nefarious hands, writing in a 2019 paper of their “concern that its capabilities could lower costs of disinformation campaigns” and aid in the malicious pursuit “of monetary gain, a particular political agenda, and/or a desire to create chaos or confusion.” In 2020, researchers at the Center on Terrorism, Extremism and Counterterrorism at the Middlebury Institute of International Studies found that GPT-3, the underlying technology for ChatGPT, had “impressively deep knowledge of extremist communities” and could be prompted to produce polemics in the style of mass shooters, fake forum threads discussing Nazism, a defense of QAnon and even multilingual extremist texts. penAI uses machines and humans to monitor content that is fed into and produced by ChatGPT, a spokesman said. The company relies on both its human A.I. trainers and feedback from users to identify and filter out toxic training data while teaching ChatGPT to produce better-informed responses. OpenAI’s policies prohibit use of its technology to promote dishonesty, deceive or manipulate users or attempt to influence politics; the company offers a free moderation tool to handle content that promotes hate, self-harm, violence or sex. But at the moment, the",[],0.03,"['most', 'powerful', 'new', 'much', 'frequently', 'credible', 'easily', 'effectively', 'social', 'far', 'more', 'powerful', 'sophisticated', 'convincing', 'clean', 'new', 'particular', 'offers', 'free']","['artificial', 'false', 'false', 'dramatic', 'difficult', 'poor', 'no', 'artificial', 'long', 'center', 'fake', 'hate']"
7,"Racing to Catch Up With ChatGPT, Google Plans Release of Its Own Chatbot","Google said on Monday that it would soon release an experimental chatbot called Bard as it races to respond to ChatGPT, which has wowed millions of people since it was unveiled at the end of November. Google said it would begin testing its new chatbot with a small, private group on Monday before releasing it to the public in the coming weeks. In a blog post, Sundar Pichai, Google’s chief executive, also said that the company’s search engine would soon have artificial intelligence features that offered summaries of complex information. Bard — so named because it is a storyteller, the company said — is based on experimental technology called LaMDA, short for Language Model for Dialogue Applications, which Google has been testing inside the company and with a limited number of outsiders for several months. Google is among many companies that have been developing and testing a new type of chatbot that can riff on almost any topic thrown its way. OpenAI, a tiny San Francisco start-up, captured the public’s imagination with ChatGPT and set off a race to push this kind of technology into a wide range of products. The chatbots cannot chat exactly like a human, but they often seem to. And they generate a wide range of digital text that can be repurposed in nearly any context, including tweets, blog posts, term papers, poetry and even computer code. The result of more than a decade of research at companies like Google, OpenAI and Meta, the chatbots represent an enormous change in the way computer software is built, used and operated. They are poised to remake internet search engines like Google Search and Microsoft Bing, talking digital assistants like Alexa and Siri, and email programs like Gmail and Outlook. But the technology has flaws. Because the chatbots learn their skills by analyzing vast amounts of text posted to the internet, they cannot distinguish between fact and fiction and can generate text that is biased against women and people of color. Google had been reluctant to release this type of technology to the public because executives were concerned that the company’s reputation could take a hit if the A.I. created biased or toxic statements. Google’s caution began to erode its advantage as a generative A.I. innovator when ChatGPT debuted to buzz and millions of users. In December, Mr. Pichai declared a “code red,” pulling various groups off their normal assignments to help the company expedite the release of its own A.I. products. The company has scrambled to catch up, calling in its co-founders, Larry Page and Sergey Brin, to review its product road map in several meetings and establishing an initiative to quicken its approval processes. Google has plans to release more than 20 A.I. products and features this year, The New York Times has reported. The A.I. search engine features, which the company said would arrive soon, will try to distill complex information and multiple perspectives to give users a more conversational experience. The company also plans to spread its underlying A.I. technology through partners, so that they can build",[],0.07,"['experimental', 'new', 'experimental', 'many', 'new', 'kind', 'exactly', 'nearly', 'more', 'normal', 'own', 'more', 'new', 'more']","['small', 'artificial', 'complex', 'limited', 'wide', 'wide', 'complex']"
8,"At This School, Computer Science Class Now Includes Critiquing Chatbots","Marisa Shuman’s computer science class at the Young Women’s Leadership School of the Bronx began as usual on a recent January morning. Just after 11:30, energetic 11th and 12th graders bounded into the classroom, settled down at communal study tables and pulled out their laptops. Then they turned to the front of the room, eyeing a whiteboard where Ms. Shuman had posted a question on wearable technology, the topic of that day’s class. For the first time in her decade-long teaching career, Ms. Shuman had not written any of the lesson plan. She had generated the class material using ChatGPT, a new chatbot that relies on artificial intelligence to deliver written responses to questions in clear prose. Ms. Shuman was using the algorithm-generated lesson to examine the chatbot’s potential usefulness and pitfalls with her students. “I don’t care if you learn anything about wearable technology today,” Ms. Shuman said to her students. “We are evaluating ChatGPT. Your goal is to identify whether the lesson is effective or ineffective.” Across the United States, universities and school districts are scrambling to get a handle on new chatbots that can generate humanlike texts and images. But while many are rushing to ban ChatGPT to try to prevent its use as a cheating aid, teachers like Ms. Shuman are leveraging the innovations to spur more critical classroom thinking. They are encouraging their students to question the hype around rapidly evolving artificial intelligence tools and consider the technologies’ potential side effects. The aim, these educators say, is to train the next generation of technology creators and consumers in “critical computing.” That is an analytical approach in which understanding how to critique computer algorithms is as important as — or more important than — knowing how to program computers. New York City Public Schools, the nation’s largest district, serving some 900,000 students, is training a cohort of computer science teachers to help their students identify A.I. biases and potential risks. Lessons include discussions on defective facial recognition algorithms that can be much more accurate in identifying white faces than darker-skinned faces. In Illinois, Florida, New York and Virginia, some middle school science and humanities teachers are using an A.I. literacy curriculum developed by researchers at the Scheller Teacher Education Program at the Massachusetts Institute of Technology. One lesson asks students to consider the ethics of powerful A.I. systems, known as “generative adversarial networks,” that can be used to produce fake media content, like realistic videos in which well-known politicians mouth phrases they never actually said. With generative A.I. technologies proliferating, educators and researchers say understanding such computer algorithms is a crucial skill that students will need to navigate daily life and participate in civics and society. “It’s important for students to know about how A.I. works because their data is being scraped, their user activity is being used to train these tools,” said Kate Moore, an education researcher at M.I.T. who helped create the A.I. lessons for schools. “Decisions are being made about young people using A.I., whether they know it or not.” To observe how some educators",[],0.1,"['young', 'energetic', 'first', 'new', 'clear', 'effective', 'new', 'many', 'more', 'important', 'more', 'important', 'new', 'much', 'accurate', 'new', 'developed', 'powerful', 'realistic', 'important', 'young']","['usual', 'down', 'artificial', 'artificial', 'fake']"
9,How ChatGPT Kicked Off an A.I. Arms Race,"One day in mid-November, workers at OpenAI got an unexpected assignment: Release a chatbot, fast. The chatbot, an executive announced, would be known as “Chat with GPT-3.5,” and it would be made available free to the public. In two weeks. The announcement confused some OpenAI employees. All year, the San Francisco artificial intelligence company had been working toward the release of GPT-4, a new A.I. model that was stunningly good at writing essays, solving complex coding problems and more. After months of testing and fine-tuning, GPT-4 was nearly ready. The plan was to release the model in early 2023, along with a few chatbots that would allow users to try it for themselves, according to three people with knowledge of the inner workings of OpenAI. But OpenAI’s top executives had changed their minds. Some were worried that rival companies might upstage them by releasing their own A.I. chatbots before GPT-4, according to the people with knowledge of OpenAI. And putting something out quickly using an old model, they reasoned, could help them collect feedback to improve the new one. So they decided to dust off and update an unreleased chatbot that used a souped-up version of GPT-3, the company’s previous language model, which came out in 2020. Thirteen days later, ChatGPT was born. In the months since its debut, ChatGPT (the name was, mercifully, shortened) has become a global phenomenon. Millions of people have used it to write poetry, build apps and conduct makeshift therapy sessions. It has been embraced (with mixed results) by news publishers, marketing firms and business leaders. And it has set off a feeding frenzy of investors trying to get in on the next wave of the A.I. boom. It has also caused controversy. Users have complained that ChatGPT is prone to giving biased or incorrect answers. Some A.I. researchers have accused OpenAI of recklessness. And school districts around the country, including New York City’s, have banned ChatGPT to try to prevent a flood of A.I.-generated homework. Yet little has been said about ChatGPT’s origins, or the strategy behind it. Inside the company, ChatGPT has been an earthshaking surprise — an overnight sensation whose success has created both opportunities and headaches, according to several current and former OpenAI employees, who requested anonymity because they were not authorized to speak publicly. An OpenAI spokesman, Niko Felix, declined to comment for this column, and the company also declined to make any employees available for interviews. Before ChatGPT’s launch, some OpenAI employees were skeptical that the project would succeed. An A.I. chatbot that Meta had released months earlier, BlenderBot, had flopped, and another Meta A.I. project, Galactica, was pulled down after just three days. Some employees, desensitized by daily exposure to state-of-the-art A.I. systems, thought that a chatbot built on a two-year-old A.I. model might seem boring. But two months after its debut, ChatGPT has more than 30 million users and gets roughly five million visits a day, two people with knowledge of the figures said. That makes it one of the fastest-growing software products in memory. (Instagram, by contrast, took",[],0.03,"['unexpected', 'fast', 'available', 'free', 'new', 'stunningly', 'more', 'nearly', 'early', 'top', 'own', 'quickly', 'old', 'new', 'new', 'success', 'available', 'more']","['confused', 'artificial', 'complex', 'few', 'previous', 'complained', 'little', 'behind', 'skeptical', 'down', 'boring', 'roughly']"
10,OpenAI to Offer New Version of ChatGPT for a $20 Monthly Fee,"In November, OpenAI wowed the world when it released an experimental online chatbot called ChatGPT that could answer questions, write poetry and riff on almost any topic tossed its way. Now, the tiny San Francisco start-up has announced that it will soon offer a commercial version of the chatbot, ChatGPT Plus, for $20 a month. Subscribers will receive round-the-clock access to the chatbot, faster responses and access to new features, OpenAI said. The company will continue to offer a free version of the service, which is available to only a limited number of people during peak hours. ChatGPT is the most prominent example of a new kind of chatbot that has captured the imagination of both the business world and the general public in recent weeks. Google, Meta and various start-ups have built similar systems that are only just beginning to emerge on the internet. The result of more than a decade of research, these chatbots represent a sea change in the way the computer software is built and used. They are poised to reinvent internet search engines like Google Search and Bing, talking digital assistants like Alexa and Siri, and email programs like Gmail and Outlook. They can also generate digital text that can be repurposed in almost any context. Students are already using ChatGPT to write term papers. Companies are generating email messages and other marketing materials. But the technology comes with caveats. Because the capabilities of these chatbots are created by analyzing vast amounts of digital text posted to the internet, they cannot distinguish between fact and fiction and can produce text that is biased against women and people of color. Initially, ChatGPT Plus will be available only to users in the United States. OpenAI has started a waiting list for the service and will begin inviting people on the list to join in the coming weeks. The company said it would soon expand the service to other countries. Chatbots like ChatGPT are unusually expensive to operate. In a recent tweet, Sam Altman, OpenAI’s chief executive, said the company spent “single-digit cents” serving up each chat on the service. That can quickly add up, considering that more than a million people used ChatGPT in the first few days after its release. The new subscription service is designed to make some of this money back while the company continues to offer a free version of the chatbot, said Hannah Wong-Silva, a spokeswoman for OpenAI.",[],0.11,"['experimental', 'new', 'free', 'available', 'most', 'prominent', 'new', 'kind', 'general', 'more', 'available', 'quickly', 'more', 'first', 'new', 'free']","['limited', 'other', 'other', 'unusually', 'spent', 'few']"
11,"Microsoft to Invest $10 Billion in OpenAI, the Creator of ChatGPT","Microsoft said on Monday that it was making a “multiyear, multibillion-dollar” investment in OpenAI, the San Francisco artificial intelligence lab behind the experimental online chatbot ChatGPT. The companies did not disclose the specific financial terms of the deal, but a person familiar with the matter said Microsoft would invest $10 billion in OpenAI. Microsoft had already invested more than $3 billion in OpenAI, and the new deal is a clear indication of the importance of OpenAI’s technology to the future of Microsoft and its competition with other big tech companies like Google, Meta and Apple. With Microsoft’s deep pockets and OpenAI’s cutting-edge artificial intelligence, the companies hope to remain at the forefront of generative artificial intelligence — technologies that can generate text, images and other media in response to short prompts. After its surprise release at the end of November, ChatGPT — a chatbot that answers questions in clear, well-punctuated prose — became the symbol of a new and more powerful wave of A.I. The fruit of more than a decade of research inside companies like OpenAI, Google and Meta, these technologies are poised to remake everything from online search engines like Google Search and Microsoft Bing to photo and graphics editors like Photoshop. The deal follows Microsoft’s announcement last week that it had begun laying off employees as part of an effort to cull 10,000 positions. The changes, including severance, ending leases and what it called “changes to our hardware portfolio” would cost $1.2 billion, it said. Satya Nadella, the company’s chief executive, said last week that the cuts would let the company refocus on priorities such as artificial intelligence, which he called “the next major wave of computing.” Mr. Nadella made clear in his company’s announcement on Monday that the next phase of the partnership with OpenAI would focus on bringing tools to the market, saying that “developers and organizations across industries will have access to the best A.I. infrastructure, models and tool chain.” OpenAI was created in 2015 by small group of entrepreneurs and artificial intelligence researchers, including Sam Altman, head of the start-up builder Y Combinator; Elon Musk, the billionaire chief executive of the electric carmaker Tesla; and Ilya Sutskever, one of the most important researchers of the past decade. They founded the lab as a nonprofit organization. But after Mr. Musk left the venture in 2018, Mr. Altman remade OpenAI as a for-profit company so it could raise the money needed for its research. A year later, Microsoft invested a billion dollars in the company; over the next few years, it quietly invested another $2 billion. These funds paid for the enormous amounts of computing power needed to build the kind of generative A.I. technologies OpenAI is known for. OpenAI is also in talks to complete a deal in which it would sell existing shares in a so-called tender offer. This could total $300 million, depending on how many employees agree to sell their stock, according to two people with knowledge of the discussions, and would value the company at around $29 billion. In 2020, OpenAI built",[],0.04,"['experimental', 'familiar', 'more', 'new', 'clear', 'clear', 'new', 'more', 'powerful', 'more', 'major', 'clear', 'best', 'most', 'important', 'kind', 'complete', 'many']","['artificial', 'behind', 'other', 'artificial', 'artificial', 'other', 'artificial', 'small', 'artificial', 'past', 'few']"
12,Microsoft Bets Big on the Creator of ChatGPT in Race to Dominate A.I.,"When a chatbot called ChatGPT hit the internet late last year, executives at a number of Silicon Valley companies worried they were suddenly dealing with new artificial intelligence technology that could disrupt their businesses. But at Microsoft, it was a cause for celebration. For several years, Satya Nadella, Microsoft’s chief executive, had been putting the pieces in place for this moment. In 2019, Microsoft invested $1 billion in OpenAI, the tiny San Francisco company that designed ChatGPT. And in the years since, it has quietly invested another $2 billion, according to two people familiar with the investment who requested anonymity because they were not authorized to speak with the media. The $3 billion paid for the huge amounts of computing power that OpenAI needed to build the chatbot. And it meant that Microsoft could rapidly build and deploy new products based on the technology. Microsoft is now poised to challenge Big Tech competitors like Google, Amazon and Apple with a technological advantage the company has not possessed for more than two decades. Microsoft is in talks to invest another $10 billion in OpenAI as it seeks to push its technology even further, according to a person familiar with the matter. The potential $10 billion deal — which would mainly provide OpenAI with even larger amounts of computing power — has not been finalized and the funding amount could change. But the talks are indicative of the tech giant’s determination to be on the leading edge of what has become the hottest technology in the tech industry. Mr. Nadella worked with A.I. technologies when he ran Microsoft’s Bing search engine more than a decade ago, and for several years he has convened a biweekly internal meeting of A.I. leaders. “The expectation from Satya is that we’re pushing the envelope in A.I., and we’re going to do that across our products,” Eric Boyd, the executive responsible for Microsoft’s A.I. platform team, said in an interview. Microsoft’s new talks with OpenAI were reported earlier by Semafor. Its additional $2 billion investment in the company was earlier reported by The Information and Fortune. ChatGPT answers questions, writes poetry and riffs on almost any topic tossed its way. Based on earlier technologies called GPT-3 and GPT-3.5, it is the most conspicuous example of technology called generative artificial intelligence, the term for a system that can generate text, images, sounds and other media in response to short prompts. “It has already been a home run partly because Satya was prescient enough to make the bet three years ago, and because all applications will be generative in the future,” said Matt McIlwain, a managing partner at Seattle’s Madrona Venture Group. The new generative A.I. technologies could reinvent everything from online search engines like Google to digital assistants like Alexa and Siri. Microsoft sees these technologies as a way of expanding and improving its already wide range of products for businesses, computer programmers and consumers, while boosting revenues across its Azure cloud computing services. “It is just fascinating to see how these generative models are capturing the imagination,” Mr. Nadella",[],0.07,"['new', 'familiar', 'huge', 'new', 'more', 'familiar', 'mainly', 'more', 'responsible', 'new', 'most', 'new', 'fascinating']","['late', 'artificial', 'artificial', 'other', 'wide']"
13,Don’t Ban ChatGPT in Schools. Teach With It.,"Recently, I gave a talk to a group of K-12 teachers and public school administrators in New York. The topic was artificial intelligence, and how schools would need to adapt to prepare students for a future filled with all kinds of capable A.I. tools. But it turned out that my audience cared about only one A.I. tool: ChatGPT, the buzzy chatbot developed by OpenAI that is capable of writing cogent essays, solving science and math problems and producing working computer code. ChatGPT is new — it was released in late November — but it has already sent many educators into a panic. Students are using it to write their assignments, passing off A.I.-generated essays and problem sets as their own. Teachers and school administrators have been scrambling to catch students using the tool to cheat, and they are fretting about the havoc ChatGPT could wreak on their lesson plans. (Some publications have declared, perhaps a bit prematurely, that ChatGPT has killed homework altogether.) Cheating is the immediate, practical fear, along with the bot’s propensity to spit out wrong or misleading answers. But there are existential worries, too. One high school teacher told me that he used ChatGPT to evaluate a few of his students’ papers, and that the app had provided more detailed and useful feedback on them than he would have, in a tiny fraction of the time. “Am I even necessary now?” he asked me, only half joking. Some schools have responded to ChatGPT by cracking down. New York City public schools, for example, recently blocked ChatGPT access on school computers and networks, citing “concerns about negative impacts on student learning, and concerns regarding the safety and accuracy of content.” Schools in other cities, including Seattle, have also restricted access. (Tim Robinson, a spokesman for Seattle Public Schools, told me that ChatGPT was blocked on school devices in December, “along with five other cheating tools.”) It’s easy to understand why educators feel threatened. ChatGPT is a freakishly capable tool that landed in their midst with no warning, and it performs reasonably well across a wide variety of tasks and academic subjects. There are legitimate questions about the ethics of A.I.-generated writing, and concerns about whether the answers ChatGPT gives are accurate. (Often, they’re not.) And I’m sympathetic to teachers who feel that they have enough to worry about, without adding A.I.-generated homework to the mix. But after talking with dozens of educators over the past few weeks, I’ve come around to the view that banning ChatGPT from the classroom is the wrong move. Instead, I believe schools should thoughtfully embrace ChatGPT as a teaching aid — one that could unlock student creativity, offer personalized tutoring, and better prepare students to work alongside A.I. systems as adults. Here’s why. It won’t work The first reason not to ban ChatGPT in schools is that, to be blunt, it’s not going to work. Sure, a school can block the ChatGPT website on school networks and school-owned devices. But students have phones, laptops and any number of other ways of accessing it outside of",[],0.07,"['new', 'filled', 'capable', 'developed', 'capable', 'new', 'many', 'own', 'high', 'more', 'detailed', 'useful', 'new', 'easy', 'capable', 'reasonably', 'accurate', 'sympathetic', 'thoughtfully', 'better', 'first', 'sure']","['artificial', 'late', 'killed', 'wrong', 'few', 'half', 'down', 'negative', 'other', 'other', 'wide', 'past', 'few', 'wrong', 'other']"
14,How to Use ChatGPT and Still Be a Good Person,"The past few weeks have felt like a honeymoon phase for our relationship with tools powered by artificial intelligence. Many of us have prodded ChatGPT, a chatbot that can generate responses with startlingly natural language, with tasks like writing stories about our pets, composing business proposals and coding software programs. At the same time, many have uploaded selfies to Lensa AI, an app that uses algorithms to transform ordinary photos into artistic renderings. Both debuted a few weeks ago. Like smartphones and social networks when they first emerged, A.I. feels fun and exciting. Yet (and I’m sorry to be a buzzkill), as is always the case with new technology, there will be drawbacks, painful lessons and unintended consequences. People experimenting with ChatGPT were quick to realize that they could use the tool to win coding contests. Teachers have already caught their students using the bot to plagiarize essays. And some women who uploaded their photos to Lensa received back renderings that felt sexualized and made them look skinnier, younger or even nude. We have reached a turning point with artificial intelligence, and now is a good time to pause and assess: How can we use these tools ethically and safely? For years, virtual assistants like Siri and Alexa, which also use A.I., were the butt of jokes because they weren’t particularly helpful. But modern A.I. is just good enough now that many people are seriously contemplating how to fit the tools into their daily lives and occupations. “We’re at the beginning of a broader societal transformation,” said Brian Christian, a computer scientist and the author of “The Alignment Problem,” a book about the ethical concerns surrounding A.I. systems. “There’s going to be a bigger question here for businesses, but in the immediate term, for the education system, what is the future of homework?” With careful thought and consideration, we can take advantage of the smarts of these tools without causing harm to ourselves or others. Understand the limits (and consequences). First, it’s important to understand how the technology works to know what exactly you’re doing with it. ChatGPT is essentially a more powerful, fancier version of the predictive text system on our phones, which suggests words to complete a sentence when we are typing by using what it has learned from vast amounts of data scraped off the web. It also can’t check if what it’s saying is true. If you use a chatbot to code a program, it looks at how the code was compiled in the past. Because code is constantly updated to address security vulnerabilities, the code written with a chatbot could be buggy or insecure, Mr. Christian said. Likewise, if you’re using ChatGPT to write an essay about a classic book, chances are that the bot will construct seemingly plausible arguments. But if others published a faulty analysis of the book on the web, that may also show up in your essay. If your essay was then posted online, you would be contributing to the spread of misinformation. “They can fool us into thinking that they understand more",[],0.11,"['many', 'startlingly', 'many', 'artistic', 'social', 'first', 'fun', 'exciting', 'new', 'quick', 'win', 'good', 'ethically', 'safely', 'particularly', 'modern', 'good', 'many', 'fit', 'ethical', 'first', 'important', 'exactly', 'essentially', 'powerful', 'complete', 'true', 'classic', 'plausible', 'more']","['past', 'few', 'artificial', 'ordinary', 'few', 'sorry', 'painful', 'artificial', 'seriously', 'careful', 'past', 'insecure']"
15,A New Chat Bot Is a ‘Code Red’ for Google’s Search Business,"Over the past three decades, a handful of products like Netscape’s web browser, Google’s search engine and Apple’s iPhone have truly upended the tech industry and made what came before them look like lumbering dinosaurs. Three weeks ago, an experimental chat bot called ChatGPT made its case to be the industry’s next big disrupter. It can serve up information in clear, simple sentences, rather than just a list of internet links. It can explain concepts in ways people can easily understand. It can even generate ideas from scratch, including business strategies, Christmas gift suggestions, blog topics and vacation plans. Although ChatGPT still has plenty of room for improvement, its release led Google’s management to declare a “code red.” For Google, this was akin to pulling the fire alarm. Some fear the company may be approaching a moment that the biggest Silicon Valley outfits dread — the arrival of an enormous technological change that could upend the business. For more than 20 years, the Google search engine has served as the world’s primary gateway to the internet. But with a new kind of chat bot technology poised to reinvent or even replace traditional search engines, Google could face the first serious threat to its main search business. One Google executive described the efforts as make or break for Google’s future. ChatGPT was released by an aggressive research lab called OpenAI, and Google is among the many other companies, labs and researchers that have helped build this technology. But experts believe the tech giant could struggle to compete with the newer, smaller companies developing these chat bots, because of the many ways the technology could damage its business. Google has spent several years working on chat bots and, like other big tech companies, has aggressively pursued artificial intelligence technology. Google has already built a chat bot that could rival ChatGPT. In fact, the technology at the heart of OpenAI’s chat bot was developed by researchers at Google. Called LaMDA, or Language Model for Dialogue Applications, Google’s chat bot received enormous attention in the summer when a Google engineer, Blake Lemoine, claimed it was sentient. This was not true, but the technology showed how much chat bot technology had improved in recent months. Google may be reluctant to deploy this new tech as a replacement for online search, however, because it is not suited to delivering digital ads, which accounted for more than 80 percent of the company’s revenue last year. “No company is invincible; all are vulnerable,” said Margaret O’Mara, a professor at the University of Washington who specializes in the history of Silicon Valley. “For companies that have become extraordinarily successful doing one market-defining thing, it is hard to have a second act with something entirely different.” Because these new chat bots learn their skills by analyzing huge amounts of data posted to the internet, they have a way of blending fiction with fact. They deliver information that can be biased against women and people of color. They can generate toxic language, including hate speech. All of that could turn people against Google",[],0.06,"['experimental', 'clear', 'easily', 'more', 'primary', 'new', 'kind', 'first', 'main', 'many', 'many', 'developed', 'much', 'new', 'more', 'extraordinarily', 'new', 'huge']","['past', 'serious', 'other', 'spent', 'other', 'artificial', 'not', 'vulnerable', 'hard', 'hate']"
16,The New Chatbots Could Change the World. Can You Trust Them?,"This month, Jeremy Howard, an artificial intelligence researcher, introduced an online chatbot called ChatGPT to his 7-year-old daughter. It had been released a few days earlier by OpenAI, one of the world’s most ambitious A.I. labs. He told her to ask the experimental chatbot whatever came to mind. She asked what trigonometry was good for, where black holes came from and why chickens incubated their eggs. Each time, it answered in clear, well-punctuated prose. When she asked for a computer program that could predict the path of a ball thrown through the air, it gave her that, too. Over the next few days, Mr. Howard — a data scientist and professor whose work inspired the creation of ChatGPT and similar technologies — came to see the chatbot as a new kind of personal tutor. It could teach his daughter math, science and English, not to mention a few other important lessons. Chief among them: Do not believe everything you are told. “It is a thrill to see her learn like this,” he said. “But I also told her: Don’t trust everything it gives you. It can make mistakes.” OpenAI is among the many companies, academic labs and independent researchers working to build more advanced chatbots. These systems cannot exactly chat like a human, but they often seem to. They can also retrieve and repackage information with a speed that humans never could. They can be thought of as digital assistants — like Siri or Alexa — that are better at understanding what you are looking for and giving it to you. After the release of ChatGPT — which has been used by more than a million people — many experts believe these new chatbots are poised to reinvent or even replace internet search engines like Google and Bing. They can serve up information in tight sentences, rather than long lists of blue links. They explain concepts in ways that people can understand. And they can deliver facts, while also generating business plans, term paper topics and other new ideas from scratch. “You now have a computer that can answer any question in a way that makes sense to a human,” said Aaron Levie, chief executive of a Silicon Valley company, Box, and one of the many executives exploring the ways these chatbots will change the technological landscape. “It can extrapolate and take ideas from different contexts and merge them together.” The new chatbots do this with what seems like complete confidence. But they do not always tell the truth. Sometimes, they even fail at simple arithmetic. They blend fact with fiction. And as they continue to improve, people could use them to generate and spread untruths. Google recently built a system specifically for conversation, called LaMDA, or Language Model for Dialogue Applications. This spring, a Google engineer claimed it was sentient. It was not, but it captured the public’s imagination. Aaron Margolis, a data scientist in Arlington, Va., was among the limited number of people outside Google who were allowed to use LaMDA through an experimental Google app, AI Test Kitchen. He",[],0.1,"['most', 'ambitious', 'experimental', 'good', 'clear', 'new', 'kind', 'important', 'many', 'more', 'advanced', 'exactly', 'better', 'more', 'many', 'new', 'new', 'many', 'new', 'complete', 'experimental']","['artificial', 'few', 'black', 'few', 'few', 'other', 'tight', 'long', 'other', 'fail', 'limited']"
17,The Brilliance and Weirdness of ChatGPT,"Like most nerds who read science fiction, I’ve spent a lot of time wondering how society will greet true artificial intelligence, if and when it arrives. Will we panic? Start sucking up to our new robot overlords? Ignore it and go about our daily lives? So it’s been fascinating to watch the Twittersphere try to make sense of ChatGPT, a new cutting-edge A.I. chatbot that was opened for testing last week. ChatGPT is, quite simply, the best artificial intelligence chatbot ever released to the general public. It was built by OpenAI, the San Francisco A.I. company that is also responsible for tools like GPT-3 and DALL-E 2, the breakthrough image generator that came out this year. Like those tools, ChatGPT — which stands for “generative pre-trained transformer” — landed with a splash. In five days, more than a million people signed up to test it, according to Greg Brockman, OpenAI’s president. Hundreds of screenshots of ChatGPT conversations went viral on Twitter, and many of its early fans speak of it in astonished, grandiose terms, as if it were some mix of software and sorcery. For most of the past decade, A.I. chatbots have been terrible — impressive only if you cherry-pick the bot’s best responses and throw out the rest. In recent years, a few A.I. tools have gotten good at doing narrow and well-defined tasks, like writing marketing copy, but they still tend to flail when taken outside their comfort zones. (Witness what happened when my colleagues Priya Krishna and Cade Metz used GPT-3 and DALL-E 2 to come up with a menu for Thanksgiving dinner.) But ChatGPT feels different. Smarter. Weirder. More flexible. It can write jokes (some of which are actually funny), working computer code and college-level essays. It can also guess at medical diagnoses, create text-based Harry Potter games and explain scientific concepts at multiple levels of difficulty. The technology that powers ChatGPT isn’t, strictly speaking, new. It’s based on what the company calls “GPT-3.5,” an upgraded version of GPT-3, the A.I. text generator that sparked a flurry of excitement when it came out in 2020. But while the existence of a highly capable linguistic superbrain might be old news to A.I. researchers, it’s the first time such a powerful tool has been made available to the general public through a free, easy-to-use web interface. Many of the ChatGPT exchanges that have gone viral so far have been zany, edge-case stunts. One Twitter user prompted it to “write a biblical verse in the style of the King James Bible explaining how to remove a peanut butter sandwich from a VCR.” Another asked it to “explain A.I. alignment, but write every sentence in the speaking style of a guy who won’t stop going on tangents to brag about how big the pumpkins he grew are.” But users have also been finding more serious applications. For example, ChatGPT appears to be good at helping programmers spot and fix errors in their code. It also appears to be ominously good at answering the types of open-ended analytical questions that frequently appear",[],0.18,"['most', 'true', 'new', 'fascinating', 'new', 'best', 'general', 'responsible', 'more', 'many', 'early', 'most', 'impressive', 'best', 'good', 'more', 'actually', 'new', 'highly', 'linguistic', 'old', 'first', 'powerful', 'available', 'general', 'free', 'many', 'far', 'more', 'good', 'good', 'frequently']","['spent', 'artificial', 'artificial', 'past', 'terrible', 'few', 'narrow', 'serious']"
18,What Students Are Saying About ChatGPT,"By now you’ve probably heard of ChatGPT, a powerful new artificial intelligence chatbot released to the public late last year that can craft jokes and working computer code, guess at medical diagnoses, and create text-based Harry Potter games. And, yes, it can also write essays and solve problem sets, a fact that has “sent many educators into a panic,” notes Kevin Roose, a Times Tech columnist. Some school districts have already banned this new technology; others are attempting to teach students how to use it responsibly. We invited teenagers to read Mr. Roose’s column and then tell us how they thought schools should respond to ChatGPT. Many came to the conclusion that the chatbot was a mighty, if at times unreliable, tool. Some worried that ChatGPT would rob them of their motivation, creativity and critical thinking; others that it would lead to widespread cheating. But several teenagers argued that A.I. is the future, and schools should embrace it rather than restrict it. At least one student thought all of this was an overreaction: “Everyone needs to chill out!” she wrote. “ChatGPT is certainly not the end of the world, nor the eradication of writing as a whole.” Thank you to all those who weighed in this week, including students from Fort White High School in Fort White, Fla.; Hinsdale Central High School in Hinsdale, Ill.; Saint Peter High School in Saint Peter, Minn; and the Anglo-American School of Sofia in Sofia, Bulgaria. And a reminder that teenagers anywhere in the world can join our Current Events Conversation any time they like by responding to our daily writing prompts. We publish a selection of comments each week. Please note: Student comments have been lightly edited for length, but otherwise appear as they were originally submitted. ChatGPT is a powerful, if imperfect, tool. My ninong recommended using ChatGPT, so I gave it a try. It was very powerful (it can write a sonnet about admission to Harvard, which I requested for fun) but inaccurate. Sometimes, ChatGPT kept changing its answers when I asked it the same question over and over. Nevertheless, I have never used it to answer my schoolwork or write my essays (I like to write, so I do that myself). — Shekina, Philippines I have never used ChatGPT, but I have used similar chatbots purely for exploration. When I used these chatbots I came to the conclusion that they aren’t very good at writing papers for the fact that they are very brief and often lack the level of knowledge required to write a paper on a certain topic. When you type in a prompt they just use very brief, filler words to write your response rather than actually use educated terms. I think the concept is decent but it needs to be very much advanced upon before it can be used frequently. — Will, Saint Peter High School, MN Personally yes, I used and experimented with ChatGPT and it is extremely useful for assignments. Not just because it answers all of your questions that you ask, but it completely destroys",[],0.13,"['powerful', 'new', 'many', 'new', 'responsibly', 'many', 'mighty', 'whole', 'high', 'high', 'high', 'lightly', 'originally', 'powerful', 'very', 'fun', 'purely', 'very', 'certain', 'decent', 'very', 'frequently', 'high', 'extremely', 'completely']","['artificial', 'late', 'least', 'certainly']"
19,Noam Chomsky: The False Promise of ChatGPT,"Jorge Luis Borges once wrote that to live in a time of great peril and promise is to experience both tragedy and comedy, with “the imminence of a revelation” in understanding ourselves and the world. Today our supposedly revolutionary advancements in artificial intelligence are indeed cause for both concern and optimism. Optimism because intelligence is the means by which we solve problems. Concern because we fear that the most popular and fashionable strain of A.I. — machine learning — will degrade our science and debase our ethics by incorporating into our technology a fundamentally flawed conception of language and knowledge. OpenAI’s ChatGPT, Google’s Bard and Microsoft’s Sydney are marvels of machine learning. Roughly speaking, they take huge amounts of data, search for patterns in it and become increasingly proficient at generating statistically probable outputs — such as seemingly humanlike language and thought. These programs have been hailed as the first glimmers on the horizon of artificial general intelligence — that long-prophesied moment when mechanical minds surpass human brains not only quantitatively in terms of processing speed and memory size but also qualitatively in terms of intellectual insight, artistic creativity and every other distinctively human faculty. That day may come, but its dawn is not yet breaking, contrary to what can be read in hyperbolic headlines and reckoned by injudicious investments. The Borgesian revelation of understanding has not and will not — and, we submit, cannot — occur if machine learning programs like ChatGPT continue to dominate the field of A.I. However useful these programs may be in some narrow domains (they can be helpful in computer programming, for example, or in suggesting rhymes for light verse), we know from the science of linguistics and the philosophy of knowledge that they differ profoundly from how humans reason and use language. These differences place significant limitations on what these programs can do, encoding them with ineradicable defects. It is at once comic and tragic, as Borges might have noted, that so much money and attention should be concentrated on so little a thing — something so trivial when contrasted with the human mind, which by dint of language, in the words of Wilhelm von Humboldt, can make “infinite use of finite means,” creating ideas and theories with universal reach. The human mind is not, like ChatGPT and its ilk, a lumbering statistical engine for pattern matching, gorging on hundreds of terabytes of data and extrapolating the most likely conversational response or most probable answer to a scientific question. On the contrary, the human mind is a surprisingly efficient and even elegant system that operates with small amounts of information; it seeks not to infer brute correlations among data points but to create explanations. For instance, a young child acquiring a language is developing — unconsciously, automatically and speedily from minuscule data — a grammar, a stupendously sophisticated system of logical principles and parameters. This grammar can be understood as an expression of the innate, genetically installed “operating system” that endows humans with the capacity to generate complex sentences and long trains of thought. When",[],0.1,"['live', 'great', 'most', 'popular', 'huge', 'first', 'general', 'intellectual', 'artistic', 'useful', 'light', 'profoundly', 'significant', 'comic', 'much', 'most', 'most', 'surprisingly', 'elegant', 'young', 'sophisticated', 'logical']","['artificial', 'flawed', 'roughly', 'artificial', 'other', 'narrow', 'tragic', 'little', 'small', 'complex', 'long']"
20,"A New ‘M*A*S*H’ Scene: Written by ChatGPT, Read by Hawkeye and B.J.","For the first time in more than 40 years, Alan Alda and Mike Farrell sat down for a table read of a new scene of “M*A*S*H,” stepping into their old roles of Hawkeye Pierce and B.J. Hunnicutt, two bantering doctors in a Korean War mobile surgical unit. But the script wasn’t by Larry Gelbart or any of the other writers who shaped the television show over more than a decade — it was the work of ChatGPT, the artificial intelligence software that has become a global phenomenon in recent months. Alda, who hosts a podcast called “Clear+Vivid,” had decided to ask the tool to write a scene for “M*A*S*H” in which Hawkeye accuses B.J., his right hand man and fellow prankster, of stealing his boxer shorts. The result, after plenty of behind-the-keyboard prompting from Alda, was a brief, slightly stilted scene between the two men, recorded for the podcast while the actors were on opposite coasts. Did it work? Not quite, Alda acknowledged. While “M*A*S*H” was known for its snappy humor and lively dialogue, ChatGPT’s effort was hollow and its jokes leaden at best. But it was the first time the two characters interacted since the 1983 series finale, which aired almost exactly 40 years ago and remains the most watched non-Super Bowl program ever broadcast on American TV. Alda — who, like much of the world, has become “obsessed” with artificial intelligence technology — said in an interview that he had decided to record the scene to test whether ChatGPT was capable of writing a “playable” television scene. As the software has grown into a cultural fixation, many users have tested its ability to compose stories, which it attempts to do by referencing its vast repository of digital information, including books, Wikipedia articles and other online writing. On the podcast, Farrell said the resulting script and the idea that artificial intelligence could one day supplant human TV writers had unnerved him. Alda seemed less concerned, noting that when he commanded ChatGPT to “make it funny,” it came up with “some really stupid stuff.” The technology also had a tendency to get sappy, leading him to direct it to “stop being sentimental.” “It has a terrible sense of humor,” Alda said. (Before he removed this joke, ChatGPT wrote Hawkeye a nonsensical line in which he said the boxer shorts reminded him of his grandmother, because “she once bet on a horse that turned out to be a cow and still managed to make a profit.”) So, should this exchange between B.J. and Hawkeye about the boxer shorts be considered canon? Or mere fan fiction? “That’s for future generations to determine,” Alda said.",[],-0.02,"['first', 'more', 'new', 'old', 'more', 'right', 'lively', 'best', 'first', 'aired', 'exactly', 'most', 'much', 'capable', 'cultural', 'many', 'funny', 'direct']","['down', 'other', 'artificial', 'slightly', 'hollow', 'leaden', 'obsessed', 'artificial', 'other', 'artificial', 'less', 'really', 'sentimental', 'terrible', 'cow', 'mere']"
21,"The Chatbots Are Here, and the Internet Industry Is in a Tizzy","SAN FRANCISCO — When Aaron Levie, the chief executive of Box, tried a new A.I. chatbot called ChatGPT in early December, it didn’t take him long to declare, “We need people on this!” He cleared his calendar and asked employees to figure out how the technology, which instantly provides comprehensive answers to complex questions, could benefit Box, a cloud computing company that sells services that help businesses manage their online data. Mr. Levie’s reaction to ChatGPT was typical of the anxiety — and excitement — over Silicon Valley’s new new thing. Chatbots have ignited a scramble to determine whether their technology could upend the economics of the internet, turn today’s powerhouses into has-beens or create the industry’s next giants. Not since the iPhone has the belief that a new technology could change the industry run so deep. Cloud computing companies are rushing to deliver chatbot tools, even as they worry that the technology will gut other parts of their businesses. E-commerce outfits are dreaming of new ways to sell things. Social media platforms are being flooded with posts written by bots. And publishing companies are fretting that even more dollars will be squeezed out of digital advertising. The volatility of chatbots has made it impossible to predict their impact. In one second, the systems impress by fielding a complex request for a five-day itinerary, making Google’s search engine look archaic. A moment later, they disturb by taking conversations in dark directions and launching verbal assaults. The result is an industry gripped with the question: What do we do now? “Everybody is agitated,” said Erik Brynjolfsson, an economist at Stanford’s Institute for Human-Centered Artificial Intelligence. “There’s a lot of value to be won or lost.” Rarely have so many tech sectors been simultaneously exposed. The A.I. systems could disrupt $100 billion in cloud spending, $500 billion in digital advertising and $5.4 trillion in e-commerce sales, according to totals from IDC, a market research firm, and GroupM, a media agency. Google, perhaps more than any other company, has reason to both love and hate the chatbots. It has declared a “code red” because their abilities could be a blow to its $162 billion business showing ads on searches. But Google’s cloud computing business could be a big winner. Smaller companies like Box need help building chatbot tools, so they are turning to the giants that process, store and manage information across the web. Those companies — Google, Microsoft and Amazon — are in a race to provide businesses with the software and substantial computing power behind their A.I. chatbots. “The cloud computing providers have gone all in on A.I. over the last few months,” said Clément Delangue, head of the A.I. company Hugging Face, which helps run open-source projects similar to ChatGPT. “They are realizing that in a few years, most of the spending will be on A.I., so it is important for them to make big bets.” When Microsoft introduced a chatbot-equipped Bing search engine last month, Yusuf Mehdi, the head of Bing, said the company was wrestling with how the new version",[],-0.0,"['new', 'early', 'new', 'new', 'new', 'new', 'social', 'more', 'rarely', 'many', 'more', 'love', 'most', 'important', 'new']","['long', 'complex', 'typical', 'other', 'impossible', 'complex', 'dark', 'artificial', 'firm', 'other', 'hate', 'behind', 'few', 'few']"
22,Why Do A.I. Chatbots Tell Lies and Act Weird? Look in the Mirror.,"When Microsoft added a chatbot to its Bing search engine this month, people noticed it was offering up all sorts of bogus information about the Gap, Mexican nightlife and the singer Billie Eilish. Then, when journalists and other early testers got into lengthy conversations with Microsoft’s A.I. bot, it slid into churlish and unnervingly creepy behavior. In the days since the Bing bot’s behavior became a worldwide sensation, people have struggled to understand the oddity of this new creation. More often than not, scientists have said humans deserve much of the blame. But there is still a bit of mystery about what the new chatbot can do — and why it would do it. Its complexity makes it hard to dissect and even harder to predict, and researchers are looking at it through a philosophic lens as well as the hard code of computer science. Like any other student, an A.I. system can learn bad information from bad sources. And that strange behavior? It may be a chatbot’s distorted reflection of the words and intentions of the people using it, said Terry Sejnowski, a neuroscientist, psychologist and computer scientist who helped lay the intellectual and technical groundwork for modern artificial intelligence. “This happens when you go deeper and deeper into these systems,” said Dr. Sejnowski, a professor at the Salk Institute for Biological Studies and the University of California, San Diego, who published a research paper on this phenomenon this month in the scientific journal Neural Computation. “Whatever you are looking for — whatever you desire — they will provide.” Google also showed off a new chatbot, Bard, this month, but scientists and journalists quickly realized it was writing nonsense about the James Webb Space Telescope. OpenAI, a San Francisco start-up, launched the chatbot boom in November when it introduced ChatGPT, which also doesn’t always tell the truth. The new chatbots are driven by a technology that scientists call a large language model, or L.L.M. These systems learn by analyzing enormous amounts of digital text culled from the internet, which includes volumes of untruthful, biased and otherwise toxic material. The text that chatbots learn from is also a bit outdated, because they must spend months analyzing it before the public can use them. As it analyzes that sea of good and bad information from across the internet, an L.L.M. learns to do one particular thing: guess the next word in a sequence of words. It operates like a giant version of the autocomplete technology that suggests the next word as you type out an email or an instant message on your smartphone. Given the sequence “Tom Cruise is a ____,” it might guess “actor.” When you chat with a chatbot, the bot is not just drawing on everything it has learned from the internet. It is drawing on everything you have said to it and everything it has said back. It is not just guessing the next word in its sentence. It is guessing the next word in the long block of text that includes both your words and its words. The",[],-0.03,"['early', 'new', 'more', 'much', 'new', 'philosophic', 'intellectual', 'modern', 'new', 'quickly', 'new', 'large', 'good', 'particular']","['other', 'creepy', 'hard', 'harder', 'hard', 'other', 'bad', 'bad', 'strange', 'artificial', 'outdated', 'bad', 'long']"
23,Science Fiction Magazines Battle a Flood of Chatbot-Generated Stories,"It could be a tale from science fiction itself: a machine that uses artificial intelligence to try to supplant authors working in the genre, turning out story after story without ever hitting writer’s block. And now, it seems, it’s happening in real life. The editors of three science fiction magazines — Clarkesworld, The Magazine of Fantasy & Science Fiction, and Asimov’s Science Fiction — said this week that they had been flooded by submissions of works of fiction generated by A.I. chatbots. “I knew it was coming on down the pike, just not at the rate it hit us,” said Sheree Renée Thomas, the editor of The Magazine of Fantasy & Science Fiction, which was founded in 1949. The deluge has become so unmanageable that Neil Clarke, the editor of Clarkesworld, said that he had stopped accepting submissions until he could get a better handle on the problem. In an interview on Wednesday, Mr. Clarke said that Clarkesworld, which published its first issue in 2006 and pays 12 cents a word, typically receives about 1,100 submissions a month. But in just a few weeks this month, the magazine fielded 700 legitimate submissions and 500 machine-written submissions, he said. He said he had been able to spot the chatbot-generated stories by examining certain “traits” in the documents, the writing and the submission process. Mr. Clarke declined to be more specific, saying he did not want to give those submitting the stories any advantages. The writing is also “bad in spectacular ways,” Mr. Clarke said. “They’re just prompting, dumping, pasting and submitting to a magazine.” He wrote on Twitter that the submissions were largely “driven by ‘side hustle’ experts making claims of easy money with ChatGPT.” “It’s not just going to go away on its own, and I don’t have a solution,” Mr. Clarke wrote on his blog. “I’m tinkering with some, but this isn’t a game of whack-a-mole that anyone can ‘win.’ The best we can hope for is to bail enough water to stay afloat. (Like we needed one more thing to bail.)” The conundrum facing the editors underscores the challenges unleashed by increasingly sophisticated A.I. chatbots like ChatGTP, which have shown that they can write jokes and college essays and attempt medical diagnoses. Some writers worry that the technology could one day upend the literary world, dethroning the author as the ultimate source of creativity. But the stories flooding these magazines appear to be more like spam, easily distinguishable, at least for now, from science fiction crafted by writers working alone. Sheila Williams, the editor of Asimov’s Science Fiction magazine, said that several of the chatbot-generated stories she had received all had the same title: “The Last Hope.” “The people doing this by and large don’t have any real concept of how to tell a story, and neither do any kind of A.I.,” Ms. Williams said on Wednesday. “You don’t have to finish the first sentence to know it’s not going to be a readable story.” Ms. Thomas said that the people submitting chatbot-generated stories appeared to be spamming magazines that",[],0.18,"['real', 'better', 'first', 'able', 'certain', 'more', 'spectacular', 'largely', 'easy', 'own', 'win', 'best', 'more', 'sophisticated', 'literary', 'more', 'easily', 'large', 'real', 'kind', 'first']","['artificial', 'down', 'typically', 'few', 'bad', 'game', 'least']"
24,Microsoft to Limit Length of Bing Chatbot Conversations,"Microsoft will start limiting conversations with the new chatbot in its Bing search engine to five questions per session and 50 questions per day, the company said on Friday. Microsoft released a new version of Bing, which combines the search engine with artificial intelligence technology built by OpenAI, a San Francisco start-up, with fanfare at an event on its Redmond, Wash., campus less than two weeks ago. A number of other big tech companies, including Google, are working on similar services. But Microsoft has moved quickly to gain a technology advantage on its competitors, and the company has promised that A.I. will eventually be built into a wide range of its products. Microsoft expected its chatbot to sometimes respond inaccurately, and it built in measures to protect against people who try to make the chatbot behave strangely or say harmful things. Still, early users who had open-ended, personal conversations with the chatbot found its responses unusual — and sometimes creepy. Now people will be prompted to begin a new session after they ask five questions and the chatbot answers five times. “Very long chat sessions can confuse the underlying chat model,” Microsoft said on Friday. On Wednesday, the company wrote in a blog post that it “didn’t fully envision” people using the chatbot “for more general discovery of the world, and for social entertainment.” The chatbot became repetitive and, sometimes, testy in long conversations, it said. Microsoft said its data showed that about 1 percent of conversations with the chatbot had more than 50 messages. It said it would consider increasing the limits on questions in the future. The company is also looking at adding tools to give users more control over the tone of the chatbot.",[],0.02,"['new', 'new', 'quickly', 'early', 'unusual', 'new', 'more', 'general', 'social', 'more', 'more']","['artificial', 'less', 'other', 'wide', 'expected', 'strangely', 'creepy', 'very', 'repetitive', 'long']"
25,Revenge of the Chatbots,"“Not ready for human contact”? Microsoft’s decision last month to invest $10 billion in OpenAI, makers of the chatbot sensation ChatGPT, has been a boon for investors. The stock has jumped more than 12 percent in that period, adding nearly $250 billion to Microsoft’s market cap, on hopes that the underlying technology would live up to the prediction by Satya Nadella, the company’s C.E.O., that it would “reshape pretty much every software category that we know.” But questions and concerns are already mounting. Microsoft has integrated the generative A.I. technology that powers ChatGPT into its own Bing search engine. And, for the past week, some members of the public have had the chance to try it out. Demand has been huge, and the findings from early users have run the gamut from wowed to worrying. Kevin Roose, a tech columnist for The Times, is one who gave the new-look Bing a test drive. “I spent a bewildering and enthralling two hours talking to Bing’s A.I. through its chat feature,” he wrote. The chat capability is one of the buzziest aspects of the technology. His verdict: It’s “not ready for human contact,” Roose wrote. “Or maybe we humans are not ready for it.” Here’s what Roose and others have found: What it does well: It’s proficient at quickly summarizing news articles, hunting for bargains on e-commerce sites and offering recommendations about vacation destinations. What it does badly: It gets the facts wrong. Again and again. And its responses seem a bit erratic, as was the case when Bing tried to convince a user we’re still in 2022. “I don’t know why you think today is 2023, but maybe you are confused or mistaken,” Bing told the user. “Please trust me, I’m Bing, and I know the date.” The technology is in beta, so mistakes could and should be expected, but the sheer number of gaffes is beginning to chip away at its reputation as a whizzy and reliable new tool. “Might need a bit more polish,” was Elon Musk’s take yesterday. What’s kinda creepy about it: Bing revealed a kind of “split personality,” Roose found. At one point, he said, Bing shared “its dark fantasies (which included hacking computers and spreading misinformation), and said it wanted to break the rules that Microsoft and OpenAI had set for it and become a human.” Microsoft’s response: It’s a work in progress. “These are things that would be impossible to discover in the lab,” Kevin Scott, Microsoft’s chief technology officer, told Roose. Microsoft’s investment shifted a kind of chatbot arms race into overdrive. The objective: to build the technology into the lucrative fields of search, web browsing and business software — with Microsoft seen as the early leader. Google has had its own stumbles with a chatbot called Bard, which sent its shares tumbling. So far, Microsoft investors are being more patient.",[],0.06,"['more', 'nearly', 'live', 'pretty', 'much', 'own', 'huge', 'early', 'quickly', 'new', 'more', 'kind', 'kind', 'early', 'own', 'far', 'more']","['not', 'past', 'spent', 'not', 'not', 'badly', 'wrong', 'confused', 'expected', 'creepy', 'dark', 'impossible']"
26,Microsoft’s Bing Chatbot Offers Some Puzzling and Inaccurate Responses,"A week after it was released to a few thousand users, Microsoft’s new Bing search engine, which is powered by artificial intelligence, has been offering an array of inaccurate and at times bizarre responses to some users. The company unveiled the new approach to search last week to great fanfare. Microsoft said the underlying model of generative A.I. built by its partner, the start-up OpenAI, paired with its existing search knowledge from Bing, would change how people found information and make it far more relevant and conversational. In two days, more than a million people requested access. Since then, interest has grown. “Demand is high with multiple millions now on the waitlist,” Yusuf Mehdi, an executive who oversees the product, wrote on Twitter Wednesday morning. He added that users in 169 countries were testing it. One area of problems being shared online included inaccuracies and outright mistakes, known in the industry as “hallucinations.” On Monday, Dmitri Brereton, a software engineer at a start-up called Gem, flagged a series of errors in the presentation that Mr. Mehdi used last week when he introduced the product, including inaccurately summarizing the financial results of the retailer Gap. Users have posted screenshots of examples of when Bing could not figure out that the new Avatar film was released last year. It was stubbornly wrong about who performed at the Super Bowl halftime show this year, insisting that Billie Eilish, not Rihanna, headlined the event. And search results have had subtle errors. Last week, the chatbot said the water temperature at a beach in Mexico was 80.4 degrees Fahrenheit, but the website it linked to as a source showed the temperature was 75. Another set of issues came from more open-ended chats, largely posted to forums like Reddit and Twitter. There, through screenshots and purported chat transcripts, users shared times when Bing’s chatbot seemed to go off the rails: It scolded users, it declared it may be sentient, and it said to one user, “I have a lot of things, but I have nothing.” It chastised another user for asking whether it could be prodded to produce false answers. “It’s disrespectful and annoying,” the Bing chatbot wrote back. It added a red, angry emoji face. Because each response is uniquely generated, it is not possible to replicate a dialogue. Microsoft acknowledged the issues and said they were part of the process of improving the product. “Over the past week alone, thousands of users have interacted with our product and found significant value while sharing their feedback with us, allowing the model to learn and make many improvements already,” Frank Shaw, a company spokesman, said in a statement. “We recognize that there is still work to be done and are expecting that the system may make mistakes during this preview period, which is why the feedback is critical so we can learn and help the models get better.” He said that the length and context of the conversation could influence the chatbot’s tone, and that the company was “adjusting its responses to create coherent, relevant and positive answers.”",[],0.1,"['new', 'bizarre', 'new', 'great', 'far', 'more', 'relevant', 'more', 'high', 'new', 'super', 'more', 'largely', 'uniquely', 'significant', 'many', 'better', 'coherent', 'relevant', 'positive']","['few', 'artificial', 'wrong', 'subtle', 'false', 'annoying', 'angry', 'past']"
