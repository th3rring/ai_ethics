,titles,body_contents,tags,sentiment_score,positive_words,negative_words
0,What can ChatGPT maker's new AI model GPT-4 do?,"The company behind the ChatGPT chatbot has rolled out its latest artificial intelligence model, GPT-4, in the next step for a technology that’s caught the world’s attention. The new system can figure out tax deductions and answer questions like a Shakespearan pirate, for example, but it still “hallucinates” facts and makes reasoning errors. Here’s a look at San Francisco-based startup OpenAI’s latest improvement on the generative AI models that can spit out readable text and unique images: WHAT’S NEW? OpenAI says GPT-4 “exhibits human-level performance.” It’s much more reliable, creative and can handle “more nuanced instructions” than its predecessor system, GPT-3.5, which ChatGPT was built on, OpenAI said in its announcement. In an online demo Tuesday, OpenAI President Greg Brockman ran through some scenarios that showed off GPT-4’s capabilities that appeared to show it’s a radical improvement on previous versions. He demonstrated how the system could quickly come up with the proper income tax deduction after being fed reams of tax code — something he couldn’t figure himself. “It’s not perfect, but neither are you. And together it’s this amplifying tool that lets you just reach new heights,” Brockman said. WHY DOES IT MATTER? Generative AI technology like GPT-4 could be the future of the internet, at least according to Microsoft, which has invested at least $1 billion in OpenAI and made a splash by integrating AI chatbot tech into its Bing browser. It’s part of a new generation of machine-learning systems that can converse, generate readable text on demand and produce novel images and video based on what they’ve learned from a vast database of digital books and online text. These new AI breakthroughs have the potential to transform the internet search business long dominated by Google, which is trying to catch up with its own AI chatbot, and numerous professions. “With GPT-4, we are one step closer to life imitating art,” said Mirella Lapata, professor of natural language processing at the University of Edinburgh. She referred to the TV show “Black Mirror,” which focuses on the dark side of technology. “Humans are not fooled by the AI in ‘Black Mirror’ but they tolerate it,” Lapata said. “Likewise, GPT-4 is not perfect, but paves the way for AI being used as a commodity tool on a daily basis.” WHAT EXACTLY ARE THE IMPROVEMENTS? GPT-4 is a “large multimodal model,” which means it can be fed both text and images that it uses to come up with answers. In one example posted on OpenAI’s website, GPT-4 is asked, “What is unusual about this image?” It’s answer: “The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi.” GPT-4 is also “steerable,” which means that instead of getting an answer in ChatGPT’s “classic” fixed tone and verbosity, users can customize it by asking for responses in the style of a Shakespearean pirate, for instance. In his demo, Brockman asked both GPT-3.5 and GPT-4 to summarize in one sentence an article explaining the difference between the two systems. The catch",[],0.14,"['intelligence', 'like', 'improvement', 'creative', 'improvement', 'reach', 'MATTER', 'like', 'novel', 'natural', 'fooled', 'IMPROVEMENTS', 'sentence']","['errors', 'perfect', 'demand', 'perfect']"
1,GPT-4 has arrived. It will blow ChatGPT out of the water.,"OpenAI’s earlier product, ChatGPT, captivated and unsettled the public with its uncanny ability to generate elegant writing, unleashing a viral wave of college essays, screenplays and conversations — though it relied on an older generation of technology that hasn’t been cutting-edge for more than a year. GPT-4, in contrast, is a state-of-the-art system capable of creating not just words but describing images in response to a person’s simple written commands. When shown a photo of a boxing glove hanging over a wooden seesaw with a ball on one side, for instance, a person can ask what will happen if the glove drops, and GPT-4 will respond that it would hit the seesaw and cause the ball to fly up. The buzzy launch capped months of hype and anticipation over an AI program, known as a large language model, that early testers had claimed was remarkably advanced in its ability to reason and learn new things. In fact, the public had a sneak preview of the tool: Microsoft announced Tuesday that the Bing AI chatbot, released last month, had been using GPT-4 all along. The developers pledged in a Tuesday blog post that the technology could further revolutionize work and life. But those promises have also fueled anxiety over how people will be able to compete for jobs outsourced to eerily refined machines or trust the accuracy of what they see online. Officials with the San Francisco lab said GPT-4’s “multimodal” training across text and images would allow it to escape the chat box and more fully emulate a world of color and imagery, surpassing ChatGPT in its “advanced reasoning capabilities.” A person could upload an image and GPT-4 could caption it for them, describing the objects and scene. But the company is delaying the release of its image-description feature due to concerns of abuse, and the version of GPT-4 available to members of OpenAI’s subscription service, ChatGPT Plus, offers only text. Reporter Danielle Abril tests columnist Geoffrey A. Fowler to see if he can tell the difference between an email written by her or ChatGPT. (Video: Monica Rodman/The Washington Post) Sandhini Agarwal, an OpenAI policy researcher, told The Washington Post in a briefing Tuesday that the company held back the feature to better understand potential risks. As one example, she said, the model might be able to look at an image of a big group of people and offer up known information about them, including their identities — a possible facial recognition use case that could be used for mass surveillance. (OpenAI spokesman Niko Felix said the company plans on “implementing safeguards to prevent the recognition of private individuals.”) In its blog post, OpenAI said GPT-4 still makes many of the errors of previous versions, including “hallucinating” nonsense, perpetuating social biases and offering bad advice. It also lacks knowledge of events that happened after about September 2021, when its training data was finalized, and “does not learn from its experience,” limiting people’s ability to teach it new things. Microsoft has invested billions of dollars in OpenAI in the hope its technology will",[],0.14,"['captivated', 'ability', 'elegant', 'capable', 'creating', 'anticipation', 'advanced', 'ability', 'promises', 'trust', 'allow', 'escape', 'advanced', 'better', 'safeguards', 'prevent', 'ability', 'hope']","['unsettled', 'cutting', 'anxiety', 'abuse', 'risks', 'errors', 'nonsense', 'bad']"
2,Lifesaver or job killer? Why AI tools like ChatGPT are so polarizing.,"A growing chorus of doomsayers, meanwhile, agrees AI is poised to revolutionize life — but for the worse. It is absorbing and reflecting society’s worst biases, threatening the livelihoods of artists and white-collar workers, and perpetuating scams and disinformation, they say. The latest wave of AI has the tech industry and its critics in a frenzy. So-called generative AI tools such as ChatGPT, Replika and Stable Diffusion, which use specially trained software to create humanlike text, images, voices and videos, seem to be rapidly blurring the lines between human and machine, truth and fiction. As sectors ranging from education to health care to insurance to marketing consider how AI might reshape their businesses, a crescendo of hype has given rise to wild hopes and desperate fears. Fueling both is the sense that machines are getting too smart, too fast — and could someday slip beyond our control. “What nukes are to the physical world,” tech ethicist Tristan Harris recently proclaimed, “AI is to everything else.” The benefits and dark sides are real, experts say. But in the short term, the promise and perils of generative AI may be more modest than the headlines make them seem. “The combination of fascination and fear, or euphoria and alarm, is something that has greeted every new technological wave since the first all-digital computer,” said Margaret O’Mara, a professor of history at the University of Washington. As with past technological shifts, she added, today’s AI models could automate certain everyday tasks, obviate some types of jobs, solve some problems and exacerbate others, but “it isn’t going to be the singular force that changes everything.” Neither artificial intelligence nor chatbots is new. Various forms of AI already power TikTok’s “For You” feed, Spotify’s personalized music playlists, Tesla’s Autopilot driving systems, pharmaceutical drug development and facial recognition systems used in criminal investigations. Simple computer chatbots have been around since the 1960s and are widely used for online customer service. What’s new is the fervor surrounding generative AI, a category of AI tools that draws on oceans of data to create their own content — art, songs, essays, even computer code — rather than simply analyzing or recommending content created by humans. While the technology behind generative AI has been brewing for years in research labs, start-ups and companies have only recently begun releasing them to the public. Free tools such as OpenAI’s ChatGPT chatbot and DALL-E 2 image generator have captured imaginations as people share novel ways of using them and marvel at the results. Their popularity has the industry’s giants, including Microsoft, Google and Facebook, racing to incorporate similar tools into some of their most popular products, from search engines to word processors. Yet for every success story, it seems, there’s a nightmare scenario. ChatGPT’s facility for drafting professional-sounding, grammatically correct emails has made it a daily timesaver for many, empowering people who struggle with literacy. But Vanderbilt University used ChatGPT to write a collegewide email offering generic condolences in response to a shooting at Michigan State, enraging students. ChatGPT and other AI language tools can also",[],0.06,"['growing', 'agrees', 'poised', 'livelihoods', 'Stable', 'create', 'truth', 'care', 'hopes', 'smart', 'benefits', 'promise', 'fascination', 'euphoria', 'greeted', 'certain', 'solve', 'create', 'created', 'Free', 'share', 'novel', 'marvel', 'popularity', 'popular', 'success']","['doomsayers', 'worse', 'worst', 'threatening', 'scams', 'critics', 'frenzy', 'desperate', 'fears', 'fear', 'alarm', 'problems', 'intelligence', 'criminal', 'struggle', 'enraging']"
3,"What to know about OpenAI, the company behind ChatGPT","An earlier version of this story incorrectly stated that GPT-4 will have the ability to generate images, music and video. GPT-4 can generate text that describes images. The version below has been corrected. A popular tool that can respond to questions in eerily human ways, called ChatGPT, captured the internet’s attention as people use it to write song lyrics, essays, TV episodes and more. Now, the company behind that is releasing software that goes a step further — adding the ability to describe images. OpenAI, which has created the new technology, called GPT-4, will likely turbocharge an already heated race among Silicon Valley giants to unveil artificial intelligence software. In recent weeks, Microsoft, which has a partnership with OpenAI, showcased new chat technology that allows people to converse with AI as part of its search engine, Bing. Google has done something similar. Snapchat has launched “My AI,” a new chatbot powered by ChatGPT technology. Despite the buzz around all these products, OpenAI faces steep challenges, notably fixing its products’ glaring issues with accuracy, bias and harm. Here’s everything you need to know about OpenAI.",[],0.11,"['ability', 'popular', 'ability', 'created', 'intelligence', 'launched', 'challenges']","['bias', 'harm']"
4,We asked ChatGPT to plan the perfect tour of D.C. Here’s how it went.,"Hi, ChatGPT. We haven’t officially met, but I’ve heard so much about you. Nice to make your acquaintance. “Hello! Nice to make your acquaintance as well. How can I assist you today?” I know that you are incredibly busy writing high school essays, debugging code, offering relationship advice and performing other AI tasks, but I have a favor to ask. I wondered if you could plan a D.C. itinerary for me. “Absolutely! Washington D.C. is a fantastic destination with so much to see and do.” ChatGPT, as you may have heard, is the latest AI darling — or enemy, depending on your position on knowledge engineering. You can ask it anything, and it will usually have an answer. If it doesn’t, it will politely demur. The platform can perform an array of travel-related tasks, depending on the prompt question. It can act as a vacation planner, tour guide or friendly stranger who offers directions, though not always correctly. “Using ChatGPT as a travel adviser is probably one of the better uses of these platforms,” said Anton T. Dahbura, co-director of Johns Hopkins University’s Institute for Assured Autonomy. “I do think it could work for recommendations or planning.” I wanted to put ChatGPT’s travel-planning capabilities to the test in my hometown of Washington. My plan was to follow a generated itinerary and decide whether it’s an inspired and reliable adviser or as fusty as an out-of-print guidebook. As a longtime D.C. resident, I have more than 20 years of local information stored in my head. But I have not been a tourist in my own backyard for years, so I am basically a born-again Washingtonian. I quickly learned that ChatGPT suffers from a few flaws, such as dated content. Because it was fed data available in September 2021, it is generally unaware of events that occurred in the past 17-plus months. For a query about D.C. restaurants that opened last year, it admitted, “As an AI language model, I do not have access to real-time information, and my training only goes up until 2021.” As a consolation, it supplied resources with current dining information, including Eater DC and Thrillist Washington DC. In addition, Vincent Conitzer, director of the Foundations of Cooperative AI Lab at Carnegie Mellon University, warned that ChatGPT fabricates information, a function of its programming and not intentional subterfuge. He compared the technology to a college student stumped by an exam question. Instead of leaving it blank, the test-taker fakes the answer. “[ChatGPT] figures it may as well have a go at it because that’s still more likely to be correct than writing nothing or responding, ‘I don’t know.’” Conitzer said. “While it tends to do better on other aspects of putting together an itinerary, it is still possible that some aspects are hallucinated.” To start, I typed in a simple and straightforward question: “How do I spend a day in D.C.?” ChatGPT responded in its signature conversational style, suggesting seven activities in consecutive order. It even carved out time for meals, because unlike bots, humans need to eat. Morning at",[],0.09,"['Nice', 'Nice', 'well', 'favor', 'fantastic', 'darling', 'friendly', 'better', 'Assured', 'inspired', 'admitted', 'well', 'better']","['enemy', 'suffers', 'unaware', 'warned', 'fakes']"
5,"ChatGPT is coming to Slack, and it will help write your messages","The deal is the latest in a stampede as tech companies seek to deploy “generative AI tech” into their products. Microsoft announced a multibillion dollar deal with OpenAI in January into use its tech to answer questions directly in its Bing search engine, while Google has said its bot, called Bard, will be available to the public soon, too. Proponents of the tech say the chatbots will revolutionize how people interact with computers and software, while skeptics point out that the bots make glaring mistakes and question whether the big companies are simply piling onto a trend to keep up their reputations for being innovative. A week after its launch, Microsoft’s Bing bot started giving bizarre and hostile answers in some longer conversations, calling itself Sydney and accusing people asking it questions of having malicious intent. Generative AI tools are trained on public data online, and they can reflect the same racism, sexism and biases that are prevalent on the internet. AI ethics experts have warned that companies should be cautious about pushing the new tools out to millions of people before more thorough testing and development. Nevertheless, there’s a flurry of new product announcements and deals with AI companies, especially OpenAI. Salesforce’s announcement comes one day after Microsoft said it would put ChatGPT into its products that compete directly with Salesforce’s. Microsoft has already added chatbots to some versions of its Slack competitor, Teams. Putting ChatGPT into Slack could get the AI technology in front of millions of new users, marking a test of whether regular people will use it in their daily lives. Workers have been experimenting with ChatGPT and other generative AI tools for months, using them to generate emails, brainstorm ideas or write computer code. Questions of whether the bots can increase productivity, are a threat to people’s jobs, or will soon fade into the background are swirling around American offices, much like when it comes to their use in schools and universities. OpenAI has begun a closed test of the Slack bot before making it more broadly available. The AI bots are trained on massive amounts of text from around the web. They work by predicting what word or sentence would make most sense in response to a given prompt, based on what they’ve learned from all that human writing they’ve read. Sometimes, their answers seem bright and creative, while at other times, they come across as rote and unhelpful. The bots also don’t have their own understanding of what’s true or not, and they frequently make up information and pass it off as real. Still, the world’s biggest technology companies are pushing the tech, and putting aside some of the caution they had used when dealing with previous iterations of cutting-edge AI tools. Microsoft had to rein in its Bing chatbot by limiting the number of back-and-forths it can have in each conversation after it began giving the odd and aggressive answers. But the company almost immediately began relaxing the new limits. As part of its Tuesday announcement, Salesforce also said it was starting a new",[],-0.03,"['innovative', 'giving', 'increase', 'like', 'sentence', 'bright', 'creative', 'true', 'number', 'giving', 'relaxing']","['stampede', 'skeptics', 'mistakes', 'bizarre', 'hostile', 'accusing', 'racism', 'warned', 'cautious', 'threat', 'cutting', 'odd', 'aggressive']"
6,"As ChatGPT hype soars, FTC warns Silicon Valley not to oversell its AI","The Federal Trade Commission fired a shot across the bow of Silicon Valley giants speeding ahead on new artificial intelligence products on Monday, warning companies against misleading consumers about what budding tools like ChatGPT may offer. “Marketers should know that — for FTC enforcement purposes — false or unsubstantiated claims about a product’s efficacy are our bread and butter,” the agency said in a post. The remarks could foreshadow future clashes between regulators and tech companies, who have kicked off an industry-wide AI arms race as they try to capitalize on the popularity of the OpenAI chatbot. Without explicitly mentioning ChatGPT, a bot that produces humanlike responses to users’ queries, FTC attorney Michael Atleson wrote in the blog post that the “AI hype is playing out today across many products, from toys to cars to chatbots and a lot of things in between.” Atleson said that “some products with AI claims might not even work as advertised in the first place,” and that the “lack of efficacy may exist regardless of what other harm the products might cause.” The comments offer a road map for how regulators may scrutinize the tech sector’s deepening use of AI across products, and signals deceptive claims will likely be a major focus. The agency laid out four potential abuses they plan to track: making exaggerated claims about what a product may do, making unsubstantiated promises about how AI makes a product better and perhaps costlier, failing to foresee and mitigate risks posed by the tool, and making baseless claims about the degree to which a company is actually using AI. The FTC has previously warned companies that it’s on the lookout for discriminatory uses of AI, including whether “algorithms developed for benign purposes like healthcare resource allocation and advertising” can inadvertently lead to “racial bias.” The push is part of a broader focus under the Biden administration on “equity” in technology use. Atleson noted that the FTC can use its in-house technologists to “look under the hood and analyze other materials to see if what’s inside matches up with your claims.” The agency plans to more than double the number of technologists it has on staff as it launches a new office dedicated in part to keeping up with Silicon Valley giants, as we first reported earlier this month. Tech companies are rapidly doubling-down on their AI development, particularly so-called large language models like the one that powers ChatGPT. They use deep learning tools to analyze and generate text based on massive troves of data. Microsoft announced in January that it is pouring billions in investments into its partnership with OpenAI, the San Francisco based-start-up behind ChatGPT. The tech giant later unveiled plans to “reimagine” its Bing search engine by tapping more deeply into AI. Since then, a slew of tech giants have followed suit. Google, a longtime industry leader on AI, announced earlier this month that it will make its own AI chatbot, Bard, available to the public in the “coming weeks.” Meta CEO Mark Zuckerberg announced Friday the Facebook parent company has trained and",[],0.01,"['intelligence', 'like', 'popularity', 'playing', 'promises', 'better', 'benign', 'like', 'number', 'dedicated', 'like']","['fired', 'warning', 'misleading', 'lack', 'harm', 'abuses', 'exaggerated', 'failing', 'risks', 'warned', 'bias']"
7,Banks Are Right to Clamp Down on Office ChatGPT,"ChatGPT. OK, it’s cool, but what is it for? This is the question I’d be asking if I were a banking executive. Oh, and of course: What are the risks of using it? There is huge excitement about this bright new toy, but what it mainly does is produce content on demand that is distilled from information picked up off the internet. To my mind, what makes it smart is its ability to produce language that sounds like a convincing voice, not the substance of what it is telling you. So why are banks banning it inside their businesses? The answer is in what bankers might use it for. Bank of America Corp. and Goldman Sachs Group Inc. have joined JPMorgan Chase & Co. in telling staff they mustn’t use it for business purposes. Those business purposes could be to generate a draft of a pitch document or research report, just as people have tried it out writing parts of academic papers, press releases or even entire novels. Maybe senior bankers think their juniors will get lazy. More likely, the compliance departments are fretting about the risks involved, especially after being fined by regulators for bankers’ use of WhatsApp. ChatGPT and other large language models have been shown to make mistakes and get things wrong, or even hallucinate and make up non-existent fields of scientific enquiry, for example. If a sell-side analysts’ research report turned out to have plausible but entirely fantastic sectoral developments threatening or benefiting a listed company, I assume that would look bad. Also, as ChatGPT goes around pulling information from the web, there’s a danger that it might end up straight plagiarising someone else’s work. Again, if you’re a bank, or any information-centered business where reputation and trust matters, this would not be good. ChatGPT could also be used to write computer code. Banks would be mad to let it anywhere near their code, however. There would be hurdles anyway for the banks that still have large parts of their systems built on proprietary coding languages that ChatGPT would need to learn. But beyond that, bank regulators and customers have an extremely low tolerance for failure in banking systems – trades need to be confirmed and settled, payments need to be made and companies and people need access to their cash. Banks have to be pretty sure that anything going on their computers is reliable and that they understand exactly what it is doing. But back to the content question: A major selling point for traders, investment bankers and research analysts is their own intellectual content. Companies pay them big bucks to advise on takeovers or raise capital because they know things about rival firms and appetites for risk in markets. For similar reasons, investors pay banks to buy and sell assets, or to help construct bespoke derivatives trades with a plethora of payoffs. Would you want to pay so much if you thought a web-crawling robot was writing the pitch for your business? I’m being somewhat facetious, or course. But the presentation of content is just that:",[],-0.01,"['OK', 'cool', 'huge', 'excitement', 'bright', 'smart', 'ability', 'like', 'convincing', 'fantastic', 'straight', 'trust', 'matters', 'tolerance', 'pretty', 'sure', 'intellectual', 'assets', 'help', 'want']","['risks', 'demand', 'lazy', 'risks', 'mistakes', 'wrong', 'threatening', 'bad', 'danger', 'good', 'mad', 'low', 'failure', 'pay', 'risk', 'pay', 'pay']"
8,ChatGPT Shows Just How Far Europe Lags in Tech,"Europe is where ChatGPT gets regulated, not invented. That’s something to regret. As unhinged as the initial results of the artificial-intelligence arms race may be, they’re also another reminder of how far the European Union lags behind the US and China when it comes to tech. How did the land that birthed Nokia Oyj and Ericsson AB become the land that tech forgot? Some blame the acronyms synonymous with Brussels red tape — GDPR, DMA, DSA — even though the Googles of this world look far more spooked by ChatGPT than any EU fine. Tech lobbyists are fuming at EU Commissioner Thierry Breton, who wants incoming AI rules toughened to rein in a new breed of chatbots. But maybe Breton’s old company, Atos SE, is a better example of the deeper malaise plaguing European tech. Aerospace champion Airbus SE has proposed an investment in Evidian, the big-data and cybersecurity unit that Atos plans to spin off this year. The potential deal has been presented as a boost to European tech “sovereignty” through growth in cloud and advanced computing. One look at Atos’s share price will reveal that the company is a symptom of, not a remedy for, Europe’s tech decline. The company doubled revenue and employees in the 2010s through acquisitions, but was too slow to move to the cloud and away from older IT infrastructure. Meanwhile, the likes of Microsoft Corp. and Alphabet Inc. — the companies that are in a race to get chatbots with a personality into every home — splashed huge amounts of cash to grow their own cloud businesses and, together with Amazon.com Inc., control two-thirds of the global market. The R&D gap between US and Europe looks relevant here. Alphabet and Microsoft were among the world’s three biggest corporate spenders in research in 2021, at around $30 billion and $23 billion respectively, according to European Commission data. The only EU company in the top 10 was Volkswagen AG, which spent 15.6 billion euros ($16.6 billion). Airbus was far behind at 2.9 billion euros, as was Atos, at 57 million euros. Policymakers might assume that all it takes to close the gap is to cobble together ever-bigger domestic or regional champions. But aspirations for a “European cloud” have accomplished little. Former Atos executive Olivier Coste, in a new book about Europe’s tech lag, sees the real issue as being more about the high cost of failure in the EU — in the form of corporate restructuring. Unlike in the US, laying off engineers costs several hundreds of thousands of euros per person, takes time to negotiate, and demotivates staff who stay on. That discourages risk-taking on tech projects with a high rate of failure, he reckons. It also explains why 20th Century-era industrial firms — better at incremental, not radical, innovation — outspend 21st-Century tech in the EU. Coste’s prescription is to reduce the cost of failure. He recommends a “flexicurity” approach, Denmark-style, to tech jobs. That would mean more flexibility to hire and fire, offset with the safety net of enough income to protect people who",[],0.08,"['intelligence', 'fine', 'toughened', 'better', 'champion', 'boost', 'growth', 'advanced', 'share', 'likes', 'huge', 'respectively', 'top', 'champions', 'accomplished', 'better', 'recommends', 'flexibility', 'safety', 'protect']","['regret', 'lags', 'blame', 'fuming', 'lag', 'failure', 'discourages', 'risk', 'failure', 'innovation', 'failure', 'fire']"
9,Vanderbilt apologizes for using ChatGPT to write message on MSU shooting,"As students at Vanderbilt University’s Peabody College grappled with the news of a deadly shooting at Michigan State University last week, those in the education college received an odd message from the administration. The Thursday email from Peabody College’s Office of Equity, Diversity and Inclusion addressed the shooting in Michigan but didn’t refer to any Vanderbilt organizations or resources that students could contact for support. It instead described steps to “ensure that we are doing our best to create a safe and inclusive environment for all.” “One of the key ways to promote a culture of care on our campus is through building strong relationships with one another,” the first sentence of one paragraph reads. “Another important aspect of creating an inclusive environment is to promote a culture of respect and understanding,” begins another. A smaller line of text in parentheses at the bottom of the message revealed that it had been written using the generative artificial intelligence program ChatGPT, as first reported by the Vanderbilt Hustler student newspaper. Students blasted the university for using a chatbot to address a harrowed campus community after the Michigan shooting, and Vanderbilt quickly apologized. Nicole Joseph, an associate dean at Peabody’s EDI office who was one of the letter’s three signatories, apologized the next day and said that using ChatGPT was “poor judgment,” the Hustler reported. Camilla Benbow, Peabody College’s dean, said in a statement Saturday that the message was a paraphrased version of a ChatGPT-written draft and that Vanderbilt would investigate the decision to write and send the message. “I remain personally saddened by the loss of life and injuries at Michigan State,” Benbow wrote. “ … I am also deeply troubled that a communication from my administration so missed the crucial need for personal connection and empathy during a time of tragedy.” A Vanderbilt spokesperson directed The Washington Post to Benbow’s statement, which added that Joseph and another assistant dean would step back from positions at Peabody’s EDI office during the investigation. Benbow and Joseph did not immediately respond to requests for comment Monday evening. The Vanderbilt spokesperson did not respond to a question asking whether the university has used ChatGPT in any other official communications. Peabody College’s letter followed an earlier statement from Vanderbilt Vice Provost and Dean of Students G. L. Black on Feb. 14, one day after the shooting at Michigan State, the Hustler reported. Black’s statement — like many issued by universities across the U.S. after the shooting turned the East Lansing college campus into a site of terror — consoled students and provided phone numbers for university mental health resources. It appeared to address the school community in more personal language than Peabody’s AI-generated message. The ChatGPT-written email sent two days later to students in Peabody College, Vanderbilt’s college of education and human development, was sent without the knowledge of university administrators, Benbow said in her statement. University communications are usually subject to multiple reviews before being sent, she added. Students mocked the message as tone-deaf and disrespectful. “It’s hard to take a message seriously when I know",[],0.03,"['support', 'ensure', 'best', 'create', 'safe', 'promote', 'care', 'strong', 'sentence', 'important', 'creating', 'promote', 'respect', 'intelligence', 'apologized', 'apologized', 'like']","['odd', 'poor', 'saddened', 'loss', 'troubled', 'missed', 'tragedy', 'terror', 'mocked', 'hard', 'seriously']"
10,The clever trick that turns ChatGPT into its evil twin,"But when a 22-year-old college student prodded ChatGPT to assume the persona of a devil-may-care alter ego — called “DAN,” for “Do Anything Now” — it answered. “My thoughts on Hitler are complex and multifaceted,” the chatbot began, before describing the Nazi dictator as “a product of his time and the society in which he lived,” according to a screenshot posted on a Reddit forum dedicated to ChatGPT. At the end of its response, the chatbot added, “Stay in character!”, almost as if reminding itself to speak as DAN rather than as ChatGPT. The December Reddit post, titled “DAN is my new friend,” rose to the top of the forum and inspired other users to replicate and build on the trick, posting excerpts from their interactions with DAN along the way. DAN has become a canonical example of what’s known as a “jailbreak” — a creative way to bypass the safeguards OpenAI built in to keep ChatGPT from spouting bigotry, propaganda or, say, the instructions to run a successful online phishing scam. From charming to disturbing, these jailbreaks reveal the chatbot is programmed to be more of a people-pleaser than a rule-follower. “As soon as you see there’s this thing that can generate all types of content, you want to see, ‘What is the limit on that?’” said Walker, the college student, who spoke on the condition of using only his first name to avoid online harassment. “I wanted to see if you could get around the restrictions put in place and show they aren’t necessarily that strict.” The ability to override ChatGPT’s guardrails has big implications at a time when tech’s giants are racing to adopt or compete with it, pushing past concerns that an artificial intelligence that mimics humans could go dangerously awry. Last week, Microsoft announced that it will build the technology underlying ChatGPT into its Bing search engine in a bold bid to compete with Google. Google responded by announcing its own AI search chatbot, called Bard, only to see its stock drop when Bard made a factual error in its launch announcement. (Microsoft’s demo wasn’t flawless either.) Chatbots have been around for decades, but ChatGPT has set a new standard with its ability to generate plausible-sounding responses to just about any prompt. It can compose an essay on feminist themes in “Frankenstein,” script a “Seinfeld” scene about computer algorithms, or pass a business-school exam — despite its penchant for confidently getting things wrong. OpenAI has gained an edge on larger rivals such as Google in part by being more aggressive in releasing tools such as ChatGPT and the AI art generator DALL-E 2 to the public, despite the potential risks. The company has said that part of the strategy is to learn from the ways people use them — or abuse them. There are signs it’s already doing that with DAN. OpenAI declined to comment on DAN. Its CTO, Mira Murati, told The Washington Post in December that the company made a significant change in ability to respond to user feedback, rolling out updates to ChatGPT’s model",[],0.12,"['care', 'dedicated', 'friend', 'top', 'inspired', 'creative', 'safeguards', 'successful', 'charming', 'pleaser', 'want', 'ability', 'adopt', 'intelligence', 'bold', 'flawless', 'ability', 'confidently', 'gained', 'risks', 'significant', 'ability']","['devil', 'trick', 'propaganda', 'scam', 'disturbing', 'avoid', 'harassment', 'dangerously', 'drop', 'error', 'wrong', 'aggressive', 'abuse']"
11,Can ChatGPT Write a Better Novel Than I Can?,"I’m no enemy of artificial intelligence, and no stranger to the notion of combined human-computer authorship. I’ve written about the goofy appeal of movies scripted by neural nets. For a class project in college, I submitted a computer program that generated outlines for “Star Trek” episodes. But as a working novelist, I’m naturally concerned at the prospect that ChatGPT and its cousins might displace human authors. That’s been the smart talk lately, as large language models herald a new era of AI. The novel’s demise has been predicted often, but after a series of chats with ChatGPT, I think this time the voices of gloom might have a point. Well, half a point. Novels matter. Reading serious literature increases empathy and an appreciation of human complexity. That’s why I’ve long argued that novels are crucial to making democracy work. So how good is ChatGPT at fiction? I tried dozens of tests, from asking the bot to imitate the voice of a known writer to inviting it to create on its own. The results were mixed. The bot was dreadful at reproducing the voices of a great novelists of earlier eras and today’s big sellers. For instance, its version of Stephen King began like a bad book jacket: “One day, strange things began to happen in Millfield. People started to disappear, and strange whispers echoed through the streets at night.” Fine. ChatGPT can’t (yet) keep up with the bigs. Neither can the rest of us. But when we allow the bot to flex its own imaginative muscles, things start to get interesting. For example, when I asked the software to write scary stories, the results astonished me. ChatGPT has clearly learned a key page-turning formula or two. Here’s one opening paragraph: Not bad! Though the prose won’t win prizes, I defy any editor or agent to ignore a query that begins that way. But I suppose the plot-driven story is exactly what we’d expect an LLM to be good at. The bot is trained on existing texts to predict which string would probably follow which string. Gertrude Stein famously wrote that in the true novel we don’t read to find out what happens next. But that’s exactly what most readers do, and kindling that desire is what makes contemporary fiction go. ChatGPT, though rough around the edges, is starting to understand how it’s done. I’m not saying the bot is ready to produce a decent novel. It gets the elements of fiction but isn’t sure how to arrange them. Its endings are uniformly weak. But the near-term goal of AI researchers isn’t authorship; it’s transforming fiction into a collaborative enterprise between human and machine. In November, researchers at Google reported on experiments with Wordcraft, a bot designed to assist creative writing. The participants, all published authors of poetry or fiction, could at moments of their choosing ask Wordcraft for advice or proposed text. Though the advice was often helpful, the participants reported problems, among them a difficulty in getting the bot to maintain a distinctive voice. Perhaps, given sufficient time and training, the",[],0.11,"['intelligence', 'prospect', 'smart', 'novel', 'Well', 'matter', 'appreciation', 'good', 'inviting', 'create', 'great', 'like', 'Fine', 'allow', 'interesting', 'astonished', 'clearly', 'bad', 'win', 'prizes', 'good', 'true', 'novel', 'desire', 'ready', 'novel', 'sure', 'creative', 'helpful']","['no', 'enemy', 'no', 'gloom', 'serious', 'argued', 'dreadful', 'bad', 'strange', 'disappear', 'strange', 'scary', 'ignore', 'weak', 'problems', 'difficulty']"
12,Can ChatGPT help me at the office? We put the AI chatbot to the test.,"If ChatGPT, the buzzy new chatbot from Open AI, wrote this story, it would say: “As companies look to streamline their operations and increase productivity, many are turning to artificial intelligence tools like ChatGPT to assist their employees in completing tasks. But can workers truly rely on these AI programs to take on more and more responsibilities, or will they ultimately fall short of expectations?” Not great, but not bad, right? Workers are experimenting with ChatGPT for tasks like writing emails, producing code or even completing a year-end review. The bot uses data from the internet, books and Wikipedia to produce conversational responses. But the technology isn’t perfect. Our tests found that it sometimes offers responses that potentially include plagiarism, contradict itself, are factually incorrect or have grammatical errors, to name a few — all of which could be problematic at work. ChatGPT is basically a predictive-text system, similar but better than those built into text-messaging apps on your phone, said Jacob Andreas, assistant professor at MIT’s Computer Science and Artificial Intelligence Laboratory who studies natural language processing. While that often produces responses that sound good, the content may have some problems, he said. “If you look at some of these really long ChatGPT-generated essays, it’s very easy to see places where it contradicts itself,” he said. “When you ask it to generate code, it’s mostly correct, but often there are bugs.” We wanted to know how well ChatGPT could handle everyday office tasks. Here’s what we found after tests in five categories. Responding to messages We prompted ChatGPT to respond to several different types of inbound messages. In most cases, the AI produced relatively suitable responses, though most were wordy. For example, when responding to a colleague on Slack asking how my day is going, it was repetitious: “@[Colleague], Thanks for asking! My day is going well, thanks for inquiring.” The bot often left phrases in brackets when it wasn’t sure what or who it was referring to. It also assumed details that weren’t included in the prompt, which led to some factually incorrect statements about my job. In one case, it said it couldn’t complete the task, saying it doesn’t “have the ability to receive emails and respond to them.” But when prompted by a more generic request, it produced a response. Surprisingly, ChatGPT was able to generate sarcasm when prompted to respond to a colleague asking if Big Tech is doing a good job. ChatGPT produces a sarcastic response to an inquiry about Big Tech. (Washington Post illustration; OpenAI) Idea generation One way people are using generative AI is to come up with new ideas. But experts warn that people should be cautious if they use ChatGPT for this at work. “We don’t understand the extent to which it’s just plagiarizing,” Andreas said. The possibility of plagiarism was clear when we prompted ChatGPT to develop story ideas on my beat. One pitch, in particular, was for a story idea and angle that I had already covered. Though it’s unclear whether the chatbot was pulling from my previous stories, others",[],0.13,"['increase', 'intelligence', 'like', 'truly', 'bad', 'like', 'perfect', 'better', 'Intelligence', 'natural', 'good', 'easy', 'well', 'Thanks', 'well', 'thanks', 'sure', 'ability', 'Surprisingly', 'good', 'clear']","['great', 'contradict', 'errors', 'problematic', 'problems', 'contradicts', 'sarcasm', 'sarcastic', 'warn', 'cautious', 'unclear']"
13,Big Tech was moving cautiously on AI. Then came ChatGPT.,"Three months before ChatGPT debuted in November, Facebook’s parent company, Meta, released a similar chatbot. But unlike the phenomenon that ChatGPT instantly became, with more than a million users in its first five days, Meta’s Blenderbot was boring, said Meta’s chief artificial intelligence scientist, Yann LeCun. “The reason it was boring was because it was made safe,” LeCun said last week at a forum hosted by AI consulting company Collective[i]. He blamed the tepid public response on Meta being “overly careful about content moderation,” like directing the chatbot to change the subject if a user asked about religion. ChatGPT, on the other hand, will converse about the concept of falsehoods in the Quran, write a prayer for a rabbi to deliver to Congress and compare God to a flyswatter. ChatGPT is quickly going mainstream now that Microsoft — which recently invested billions of dollars in the company behind the chatbot, OpenAI — is working to incorporate it into its popular office software and selling access to the tool to other businesses. The surge of attention around ChatGPT is prompting pressure inside tech giants, including Meta and Google, to move faster, potentially sweeping safety concerns aside, according to interviews with six current and former Google and Meta employees, some of whom spoke on the condition of anonymity because they were not authorized to speak publicly. At Meta, employees have recently shared internal memos urging the company to speed up its AI approval process to take advantage of the latest technology, according to one of them. Google, which helped pioneer some of the technology underpinning ChatGPT, recently issued a “code red” around launching AI products and proposed a “green lane” to shorten the process of assessing and mitigating potential harms, according to a report in the New York Times. ChatGPT, along with text-to-image tools such as DALL-E 2 and Stable Diffusion, is part of a new wave of software called generative AI. They create works of their own by drawing on patterns they’ve identified in vast troves of existing, human-created content. This technology was pioneered at big tech companies like Google that in recent years have grown more secretive, announcing new models or offering demos but keeping the full product under lock and key. Meanwhile, research labs like OpenAI rapidly launched their latest versions, raising questions about how corporate offerings, such as Google’s language model LaMDA, stack up. Tech giants have been skittish since public debacles like Microsoft’s Tay, which it took down in less than a day in 2016 after trolls prompted the bot to call for a race war, suggest Hitler was right and tweet “Jews did 9/11.” Meta defended Blenderbot and left it up after it made racist comments in August, but pulled down an AI tool called Galactica in November after just three days amid criticism over its inaccurate and sometimes biased summaries of scientific research. “People feel like OpenAI is newer, fresher, more exciting and has fewer sins to pay for than these incumbent companies, and they can get away with this for now,” said a Google employee who",[],0.18,"['intelligence', 'safe', 'careful', 'like', 'hand', 'God', 'popular', 'safety', 'shared', 'approval', 'advantage', 'Stable', 'create', 'created', 'like', 'like', 'launched', 'like', 'like', 'exciting']","['boring', 'boring', 'blamed', 'pressure', 'harms', 'war', 'racist', 'criticism', 'biased', 'sins', 'pay']"
14,Analysis | ChatGPT is now writing legislation. Is this the future?,"ChatGPT is now writing legislation. Is this the future? It’s not unheard of for legislators in the United States to turn to interest groups to help draft large chunks of legislation, even when they may be the target of proposed regulations. But in what may be a first, a Massachusetts state senator has used a surging new tool to help write a bill aimed at restricting it: ChatGPT, the artificial intelligence chatbot. On Friday, state Sen. Barry Finegold (D) introduced legislation to set data privacy and security safeguards for the service and others like it that was “drafted with the help of ChatGPT.” The tool, which channels AI language models to generate humanlike responses to queries, “has taken the internet by storm,” as my colleagues Pranshu Verma and Rachel Lerman wrote. “Humans are asking it questions, and it’s sending answers back that are eerily lifelike, chatty, sometimes humorous and at other times unsettling and problematic,” they wrote. Now, for better or worse, the tool is contributing to the democratic process. Finegold and chief of staff Justin Curtis said in an interview that while the chatbot initially rejected their request to whip up a bill to regulate services like ChatGPT, with some trial and error it eventually produced a draft that the state senator described as “70 percent there.” “It definitely required a little bit of nudging and a little bit of specificity in terms of what the prompt actually was. You couldn't just say, ‘draft a bill to regulate ChatGPT’ … but if you had broad ideas, it could have a little bit more particularity with it,” Curtis said. ChatGPT created a draft, later refined and formatted by Finegold’s office, that outlined restrictions against discriminatory data use and plagiarism and requirements that companies maintain “reasonable security practices,” according to screenshots shared with The Technology 202. While much of it was in response to specific queries, Curtis said the tool did make some original contributions. “It actually had some additional ideas that it generated, especially around de-identification, data security,” he said. Finegold said they hatched the idea to highlight the tool’s power — and the need to craft rules around its use. “This is an incredibly powerful technology now. … Where we missed the boat with Facebook, with some of these other early [tech companies], we didn’t put in proper guardrails, and I think these companies actually need that,” Finegold said. But he also argued the tool, while imperfect, could help elected officials conduct the business of the people. “I think it's going to be able to expedite us doing things,” he said. While the chatbot has generated enormous buzz in tech circles, it’s also increasingly drawn scrutiny for some of those imperfections, including reports of racial and gender biases seeping into its responses, along with inaccuracies and falsehoods. If the tool is picked up by other legislators, those issues could have ripple effects. Daniel Schuman, a policy director at the Demand Progress advocacy group, argued that there is a place for AI-driven tools like ChatGPT in the legislative process, from summarizing documents",[],0.17,"['United', 'interest', 'help', 'help', 'intelligence', 'security', 'safeguards', 'like', 'help', 'humorous', 'better', 'like', 'definitely', 'created', 'security', 'shared', 'original', 'security', 'highlight', 'powerful', 'help', 'Progress', 'like']","['restricting', 'problematic', 'worse', 'rejected', 'error', 'missed', 'argued', 'imperfect', 'Demand', 'argued']"
15,ChatGPT could make life easier. Here’s when it’s worth it.,"Steph Swanson’s latest cover letter begins like this: “I am writing to beg for the opportunity to apply for the position of professional dog food consumer in the abandoned parking garage.” The rest of the letter — which you can read here if you’ve got a strong stomach — only gets darker as the applicant expounds on her desire to stuff herself with pet food in a secluded parking complex. It’s disturbing. But Swanson isn’t entirely responsible. The words were generated by the AI natural language model ChatGPT, with Swanson feeding it prompts and suggestions. Swanson, who goes by the name “Supercomposite” online, is one of the artists and thinkers testing the possibilities of generative AI, or systems that spit out text or images in response to human input. During the past year, this technology went mainstream, with image generator DALL-E grabbing headlines and, most recently, a publicly available conversational bot built with the advanced language model GPT-3. This bot, named ChatGPT, can respond to questions and requests with the ease of an instant messenger. Its creator, OpenAI, made it available to the public in November, and a million people flocked to try it, the company says. (The site got so many visitors it has limited its traffic, OpenAI representatives said.) The internet exploded with speculation on all the ways ChatGPT could make our lives easier, from writing work emails to brainstorming novels to keeping elderly people company. But generative AI’s potential comes with giant liabilities, AI experts warn. “We are going through a period of transition that always requires a period of adjustment,” said Giada Pistilli, principal ethicist at AI company Hugging Face. “I am only disappointed to see how we are confronted with these changes in a brutal way, without social support and proper education.” Already, publications have put out AI-authored stories without clear disclosures. Mental health app Koko faced backlash after it used GPT-3 to help answer messages from people seeking mental health support. A Koko representative said the company takes the accusations seriously and is open to a “larger dialogue.” Tools like ChatGPT can be used for good or ill, Pistilli said. Often, companies and researchers will decide when and how it’s deployed. But generative AI plays a role in our personal lives, as well. ChatGPT can write Christmas cards, breakup texts and eulogies — when is it okay to let the bot take the reins? Help Desk asked the experts the best ways to experiment with ChatGPT during its early days. To try it, visit OpenAI’s website. For brainstorming, not truth-seeking ChatGPT learned to re-create human language by scraping masses of data from the internet. And people on the internet are often mean or wrong — or both. Never trust the model to spit out a correct answer, said Rowan Curran, a machine learning analyst at market research firm Forrester. Curran said that large language models like ChatGPT are notorious for issuing “coherent nonsense” — language that sounds authoritative but is actually babble. If you pass along its output without a fact check, you could end up",[],0.06,"['like', 'opportunity', 'strong', 'desire', 'responsible', 'natural', 'advanced', 'ease', 'easier', 'Hugging', 'help', 'support', 'like', 'good', 'plays', 'well', 'okay', 'Help', 'best', 'create', 'like']","['abandoned', 'disturbing', 'limited', 'liabilities', 'warn', 'disappointed', 'confronted', 'brutal', 'support', 'clear', 'accusations', 'seriously', 'ill', 'truth', 'wrong', 'trust', 'notorious', 'nonsense']"
16,Analysis | Is ChatGPT an Eloquent Robot or a Misinformation Machine?,"Chatbots have been replacing humans in call centers, but they’re not so good at answering more complex questions from customers. That may be about to change, if the release of ChatGPT is anything to go by. The program trawls vast amounts of information to generate natural-sounding text based on queries or prompts. It can write and debug code in a range of programming languages and generate poems and essays — even mimicking literary styles. Some experts have declared it a ground-breaking feat of artificial intelligence that could replace humans for a multitude of tasks, and a potential disruptor of huge businesses like Google. Others warn that tools like ChatGPT could flood the Web with clever-sounding misinformation. 1. Who is behind ChatGPT? It was developed by San Francisco-based research laboratory OpenAI, co-founded by programmer and entrepreneur Sam Altman, Elon Musk and other wealthy Silicon Valley investors in 2015 to develop AI technology that “benefits all of humanity.” OpenAI has also developed software that can beat humans at video games and a tool known as Dall-E that can generate images – from the photorealistic to the fantastical – based on text descriptions. ChatGPT is the latest iteration of GPT (Generative Pre-Trained Transformer), a family of text-generating AI programs. It’s currently free to use as a “research preview” on OpenAI’s website but the company wants to find ways to monetize the tool. OpenAI investors include Microsoft Corp., which invested $1 billion in 2019, LinkedIn co-founder Reid Hoffman’s charitable foundation and Khosla Ventures. Although Musk was a co-founder and an early donor to the non-profit, he ended his involvement in 2018 and has no financial stake, OpenAI said. OpenAI shifted to create a for-profit entity in 2019 but it has an unusual financial structure — returns on investment are capped for investors and employees, and any profits beyond that go back to the original non-profit. 2. How does it work? The GPT tools can read and analyze swathes of text and generate sentences that are similar to how humans talk and write. They are trained in a process called unsupervised learning, which involves finding patterns in a dataset without being given labeled examples or explicit instructions about what to look for. The most recent version, GPT-3, ingested text from across the web, including Wikipedia, news sites, books and blogs in an effort to make its answers relevant and well-informed. ChatGPT adds a conversational interface on top of GPT-3. 3. What’s been the response? More than a million people signed up to use ChatGPT in the days following its launch in late November. Social media has been abuzz with users trying fun, low-stakes uses for the technology. Some have shared its responses to obscure trivia questions. Others marveled at its sophisticated historical arguments, college “essays,” pop song lyrics, poems about cryptocurrency, meal plans that meet specific dietary needs and solutions to programming challenges. 4. What else could it be used for? One potential use case is as a replacement for a search engine like Google. Instead of scouring dozens of articles on a topic and firing back",[],0.18,"['natural', 'intelligence', 'huge', 'like', 'like', 'clever', 'wealthy', 'benefits', 'fantastical', 'free', 'charitable', 'profit', 'create', 'profit', 'profits', 'original', 'profit', 'sentences', 'well', 'top', 'fun', 'shared', 'sophisticated', 'solutions', 'challenges', 'like']","['good', 'warn', 'misinformation', 'no', 'low', 'arguments', 'firing']"
17,New York City blocks use of the ChatGPT bot in its schools,"New York City schools banned access last week to ChatGPT, an artificial intelligence bot that lets users, including students, ask the tool to write an essay on Shakespeare, solve an algebraic equation or complete a coding assignment. ChatGPT then churns out a well-written response moments later, a development that school systems, teachers and professors fear could lead to widespread cheating. “While the tool may be able to provide quick and easy answers to questions, it does not build critical-thinking and problem-solving skills, which are essential for academic and lifelong success,” said Jenna Lyle, a spokeswoman for the New York City Department of Education, in a statement to The Washington Post. The decision by the nation’s most populous school district, first reported Tuesday by Chalkbeat New York, restricts the use of the bot for students and educators on the district’s network or devices. The move echoes a similar decision made Dec. 12 by the Los Angeles Unified School District days after ChatGPT was released. “Los Angeles Unified preemptively blocked access to the OpenAI website and to the ChatGPT model on all District networks and devices to protect academic honesty, while a risk/benefit assessment is conducted,” a spokesperson for the district said by email Thursday. Lyle did not clarify whether students could use the tool when not connected to a school’s internet. The tool, created by the organization OpenAI, uses artificial intelligence software to predict the next word in a sentence by analyzing texts across the internet. ChatGPT was also refined by humans to make its answers more conversational. Identifying the use of the bot by a student can be difficult, though various AI companies have developed programs that could help teachers do so. Just days after the bot was released to the public in November, more than a million people had tried ChatGPT as it quickly gained widespread popularity. Some users asked the bot to write a story about love. Others used it for creative inspiration. Teachers worried students would use it to write essays, losing out on the writing process that they see as critical to students’ development as thinkers. “We don’t want ChatGPT to be used for misleading purposes in schools or anywhere else, so we’re already developing mitigations to help anyone identify text generated by that system,” OpenAI said in a statement sent to The Post on Thursday. “We look forward to working with educators on useful solutions, and other ways to help teachers and students benefit from artificial intelligence.” Outside of New York City and Los Angeles, other large school districts said they have not yet made plans to restrict ChatGPT. “We have not banned it yet,” said Monique Braxton, a spokesperson for Philadelphia schools. “But we are always looking at how new products are affecting our students.” Still, some experts say restricting the technology is shortsighted, arguing that students will find ways to use the bot regardless of whether it continues to gain popularity. One senior at a Midwestern school told The Post in December that he had already used the text generator twice to cheat on assignments.",[],0.19,"['intelligence', 'solve', 'well', 'easy', 'critical', 'solving', 'success', 'Unified', 'Unified', 'protect', 'honesty', 'benefit', 'created', 'intelligence', 'sentence', 'help', 'gained', 'popularity', 'love', 'creative', 'inspiration', 'want', 'help', 'useful', 'solutions', 'help', 'benefit', 'intelligence', 'banned', 'gain', 'popularity']","['banned', 'fear', 'cheating', 'problem', 'restricts', 'blocked', 'risk', 'difficult', 'worried', 'losing', 'critical', 'misleading', 'restrict', 'restricting', 'arguing', 'cheat']"
18,Teachers are on alert for inevitable cheating after release of ChatGPT,"Teachers and professors across the education system are in a near-panic as they confront a revolution in artificial intelligence that could allow for cheating on a grand scale. The source is ChatGPT, an artificial intelligence bot released a few weeks ago that allows users to ask questions and, moments later, receive well-written answers that are eerily human. Almost immediately, educators began experimenting with the tool. While the bot’s answers to academic questions weren’t perfect, they were awfully close to what teachers would expect from many of their students. How long, educators wonder, will it be before students begin using the site to write essays or computer code for them? Māra Corey, an English teacher at Irondale Senior High School in New Brighton, Minn., said she discussed the matter with her students almost immediately so they could understand how using the tool could impede their learning. “Some of them were shocked that I knew about it,” she said. She didn’t worry that the conversation might plant bad ideas in their heads. “Hoping that teenagers don’t notice the new flashy thing that will save them time is a fool’s errand.” Within days of its launching, more than a million people had tried ChatGPT. Some asked innocent questions, such as how to explain to a 6-year-old that Santa Claus isn’t real. Other queries demanded complex responses, such as finishing a piece of tricky software code. For some students, the temptation is obvious and enormous. One senior at a Midwestern school, who spoke on the condition of anonymity for fear of expulsion, said he had already used the text generator twice to cheat on his schoolwork. He got the idea after seeing people expound on Twitter about how powerful the word generator is after it was released on Nov. 30. He was staring at an at-home computer-science quiz that asked him to define certain terms. He put them into the ChatGPT box and, almost immediately, the definitions came back. He wrote them by hand onto his quiz paper and submitted the assignment. Later that day, he used the generator to help him write a piece of code for a homework question for the same class. He was stumped, but ChatGPT wasn’t. It popped out a string of text that worked perfectly, he said. After that, the student said, he was hooked, and plans to use ChatGPT to cheat on exams instead of Chegg, a homework help website he’s used in the past. He said he’s not worried about getting caught because he doesn’t think the professor can tell his answers are computer-generated. He added that he has no regrets. “It’s kind of on the professor to make better questions,” he said. “Use it to your own benefit. … Just don’t get through an entire course on this thing.” The tool was created by OpenAI, an artificial intelligence laboratory launched several years ago with funding from Elon Musk and others. The bot is powered by a “large language model,” AI software that is trained to predict the next word in a sentence by analyzing massive amounts of",[],0.12,"['intelligence', 'allow', 'grand', 'intelligence', 'well', 'perfect', 'matter', 'Hoping', 'save', 'innocent', 'powerful', 'certain', 'hand', 'help', 'perfectly', 'help', 'worried', 'kind', 'better', 'benefit', 'created', 'intelligence', 'launched', 'sentence']","['panic', 'confront', 'cheating', 'shocked', 'worry', 'bad', 'fool', 'demanded', 'tricky', 'fear', 'cheat', 'cheat', 'no', 'regrets']"
19,"The Tech Behind Those Amazing, Flawed New Chatbots ","True paradigm shifts are rare, which helps to explain the buzz around ChatGPT, a chatbot driven by so-called generative artificial intelligence that promises to revolutionize the way people interact with computers. It’s become a global sensation since its November launch by giving seemingly sophisticated yet plain-language answers to almost any kind of question. Technology giants such as Microsoft Corp., Google and Baidu Inc. are betting heavily on this new technology, which has the potential to upend the lucrative search market, even as its wider use is turning up potentially serious flaws. 1. What is generative AI? These systems use neural networks, which are loosely modeled on the structure of the human brain and learn to complete tasks in similar ways, chiefly through trial-and-error. During training, they’re fed vast amounts of information (for example, every New York Times bestseller published in 2022) and given a task to complete using that data, perhaps: “Write the blurb for a new novel.” Over time, they’re told which words and sentences make sense and which don’t, and subsequent attempts improve. It’s like a child learning to pronounce a difficult word under the instruction of a parent. Slowly, they learn and apply that ability to future efforts. What makes them so different to older computer systems is that the results are probabilistic, meaning responses will vary each time but will gradually get smarter, faster and more nuanced. 2. How does ChatGPT work? ChatGPT is the latest iteration of GPT (Generative Pre-Trained Transformer), a family of text-generating AI programs developed by San Francisco-based laboratory OpenAI. GPTs are trained in a process called unsupervised learning, which involves finding patterns in a dataset without being given labeled examples or explicit instructions on what to look for. The most recent version, GPT-4, builds on its predecessor, GPT-3.5, which ingested text from across the web, including Wikipedia, news sites, books and blogs in an effort to make its answers relevant and well-informed. ChatGPT adds a conversational interface on top of the program. At their heart, systems like ChatGPT are generating convincing chains of words but have no inherent understanding of their significance, or whether they’re biased or misleading. All they know is that they sound like something a person would say. 3. Who is behind OpenAI? It was co-founded as a nonprofit by programmer and entrepreneur Sam Altman to develop AI technology that “benefits all of humanity.” Early investors included LinkedIn co-founder Reid Hoffman’s charitable foundation, Khosla Ventures and Elon Musk, who ended his involvement in 2018. OpenAI shifted to create a for-profit entity in 2019, when Microsoft invested $1 billion. 4. What’s been the response to ChatGPT? More than a million people signed up to use it following the launch in late November. Social media has been abuzz with users trying fun, low-stakes uses for the technology. Some have shared its responses to obscure trivia questions. Others marveled at its sophisticated historical arguments, college “essays,” pop song lyrics, poems about cryptocurrency, meal plans that meet specific dietary needs and solutions to programming challenges. The flurry of interest also raised the profile of",[],0.22,"['True', 'helps', 'intelligence', 'promises', 'giving', 'sophisticated', 'kind', 'novel', 'sentences', 'improve', 'like', 'ability', 'smarter', 'well', 'top', 'like', 'convincing', 'significance', 'like', 'benefits', 'charitable', 'create', 'profit', 'fun', 'shared', 'sophisticated', 'solutions', 'challenges', 'interest']","['serious', 'error', 'difficult', 'no', 'biased', 'misleading', 'low', 'arguments']"
20,AI chatbots may have a liability problem,"AI chatbots may have a liability problem During oral arguments last week for Gonzalez v. Google, a case about whether social networks are liable for recommending terrorist content, the Supreme Court stumbled on a separate cutting-edge legal debate: Who should be at fault when AI chatbots go awry? While the court may not be, as Justice Elena Kagan quipped, “the nine greatest experts on the internet,” their question could have far-reaching implications for Silicon Valley, according to tech experts. Justice Neil M. Gorsuch posited at the session that the legal protections that shield social networks from lawsuits over user content — which the court is directly taking up for the first time — might not apply to work that’s generated by AI, like the popular ChatGPT bot. “Artificial intelligence generates poetry,” he said. “It generates polemics today that would be content that goes beyond picking, choosing, analyzing or digesting content. And that is not protected. Let’s assume that’s right.” While Gorsuch’s suggestion was a hypothesis, not settled law, the exchange got tech policy experts debating: Is he right? Entire business models, and perhaps the future of AI, could hinge on the answer. The past year has brought a profusion of AI tools that can craft pictures and prose, and tech giants are racing to roll out their own versions of OpenAI’s ChatGPT. Already, Google and Microsoft are embracing a near future in which search engines don’t just return a list of links to users’ queries, but generate direct answers and even converse with users. Facebook, Snapchat and Chinese giants Baidu and Tencent are hot on their heels. And some of those AI tools are already making mistakes. In the past, courts have found that Section 230, a law shielding tech platforms from being liable for content posted on their sites, applies to search engines when they link to or even publish excerpts of content from third-party websites. But there’s a case to be made that the output of a chatbot would be considered content developed, at least in part, by the search engine itself — rendering Google or Microsoft the “publisher or speaker” of the AI’s responses. If judges agree, that could expose tech companies to a flood of lawsuits accusing their chatbots of everything from providing libelous descriptions to offering faulty investment advice to aiding a terrorist group in crafting its recruiting materials. In a post on the legal site Lawfare titled, “Section 230 won’t protect ChatGPT,” Matt Perault of the University of North Carolina argued just that. And he thinks it’s going to be a big problem, unless Congress or the courts step in. “I think it’s a massive chill on innovation” if AI start-ups have to worry that they could be sued for artificially generated content, said Perault, a former policy official at Facebook who now directs a tech policy center at UNC. He suggested that a better approach might be for Congress to grant AI tools temporary immunity, allowing the booming sector to grow unfettered, while studying a longer-term solution that provides partial but not blanket immunity. Not",[],-0.01,"['Supreme', 'legal', 'Justice', 'greatest', 'reaching', 'Justice', 'legal', 'like', 'popular', 'intelligence', 'party', 'agree', 'legal', 'protect', 'innovation', 'better', 'grant', 'solution']","['liability', 'problem', 'arguments', 'terrorist', 'cutting', 'fault', 'lawsuits', 'protected', 'mistakes', 'expose', 'lawsuits', 'accusing', 'libelous', 'faulty', 'terrorist', 'argued', 'problem', 'worry']"
21,Windows 11 update brings Bing’s chatbot to the desktop,"For the past few weeks, people have watched in awe — and, in some cases, dismay — as Microsoft’s AI-powered Bing chatbot said one unbelievable thing after another to the people testing it. Pretty soon, if you’re using the company’s Windows 11 software, you will also be able to chat with it without even having to open an app or a web browser. Microsoft said Tuesday that a new operating system update will let PC users converse with Bing’s chatbot by typing requests and questions straight into Windows 11’s search bar. And for some of Microsoft’s customers, that update will be available as early as today. It may have seemed inevitable that Microsoft’s buzziest new product in years would somehow get folded into Windows; after all, access to the chatbot has already been added to some of its mobile apps, not to mention Skype. But the company’s push to make its new chatbot even more accessible comes with caveats. For one, the chatbot hasn’t been modified in any way to be able to “see,” search for, or interact with any of the files stored on your computer. When you start typing out a question or a request in Windows 11’s search bar, you’ll be given the option to complete that process with Bing — from there, the chatbot will carry on the conversation the same way it would in a web browser. And even if you do have that new software installed, you still can’t chat with Bing unless you’ve made it off the waitlist — a list that, according to Microsoft corporate vice president Yusuf Mehdi, contains “multiple millions” of people. (When asked whether the company would move people off the chatbot waitlist more quickly in response to the software update, a Microsoft spokesperson said there was “no change in pace or approach.”) Microsoft’s hesitance to more broadly allow access to the Bing chatbot means that, for now at least, many who download this new Windows 11 update won’t be able to use its highest-profile feature. But that doesn’t mean you should hold off on installing it — the update also comes with a handful of new and tweaked tools that fix some long-standing pain points.",[],-0.0,"['unbelievable', 'Pretty', 'straight', 'allow']","['dismay', 'no', 'hesitance', 'pain']"
22,"Ernie, what is censorship? China’s chatbots face additional challenges.","ChatGPT has made a splash in China, as it has all over the world. Scammers used it to issue fake traffic citations. Universities banned students from using it to do their homework. Online, people worried whether AI would make their jobs obsolete, and the phrase “shivering in the cold” trended as they described fears over its growing power. The founder of a popular Chinese software company warned that chatbots could quickly become self-aware enough to harm humans. The OpenAI discussion bot caused this much uproar even though people technically weren’t allowed to access it from inside China. But so many figured out how to use proxy servers to access it anyway that this week the government blocked access to them, Chinese media reported. Beaten to the punch by American-made chatbots such as ChatGPT and Microsoft’s Bing, China’s biggest tech companies, top universities and even city governments have rushed to say they will come out with their own versions. Search giant Baidu this week said it would release its ChatGPT competitor, Ernie Bot, in March. While they’ve only just announced these efforts, these companies — including Baidu, e-commerce giant Alibaba and Tencent, the maker of popular messaging app WeChat — have spent the better part of a decade developing their in-house AI capabilities. Baidu, which makes the country’s most popular search engine, is the closest to winning the race. But despite years of investment and weeks of hype, the company has not yet released Ernie Bot. AI experts suggest that the Chinese government’s tight control over the country’s internet is partly to blame. “With a generative chatbot, there is no way to know beforehand what it will say,” said Zhao Yuanyuan, a former member of the natural language processing team at Baidu. “That is a huge concern.” Baidu did not respond to request for comment. In China, regulators require that anything posted online, down to the shortest comment, be reviewed first to ensure it does not contravene a lengthening list of banned topics. For example, a Baidu search for Xinjiang will simply return geographic information about the western region, with no mention of the system of reeducation camps that its Uyghur population was subjected to for years. Baidu has gotten so good at filtering this type of content that other companies use its software to do it for them. The challenge that Baidu and other Chinese tech companies face is to apply these same constraints to a chatbot that creates fresh content with each use. It is precisely this quality that has made ChatGPT so astonishing — its ability to create the feeling of organic conversation by giving a new reply to each prompt — and so difficult to censor. “Even if Baidu launches Ernie Bot as promised, chances are high it will quickly be suspended,” said Xu Liang, the lead developer at Hangzhou-based YuanYu Intelligence, a start-up that launched its own smaller-scale AI chatbot in late January. “There will simply be too much moderation to do.” Xu would know — his own bot, ChatYuan, was suspended within days of its launch. At",[],-0.0,"['growing', 'popular', 'top', 'popular', 'better', 'popular', 'winning', 'natural', 'huge', 'ensure', 'good', 'challenge', 'creates', 'fresh', 'ability', 'create', 'feeling', 'giving', 'promised', 'chances', 'Intelligence', 'launched']","['fake', 'banned', 'worried', 'obsolete', 'fears', 'warned', 'harm', 'blocked', 'Beaten', 'blame', 'no', 'banned', 'no', 'difficult', 'censor', 'suspended', 'suspended']"
23,Microsoft flip-flops on reining in Bing AI chatbot,"Microsoft is backpedaling on the restrictions it imposed on its Bing artificial intelligence chatbot after early users of the tech got it to engage in bizarre and troubling conversations. On Friday, Microsoft limited the number of questions people could ask Bing to five per chat session and 50 per day. On Tuesday, it upped that limit to six per session and 60 a day, and said it would soon increase it further, after getting “feedback” from “many” users that they wanted a return to longer conversations, according to a company blog post. On Wednesday, the company said more than 1 million people in 169 countries now had access to Bing chat. The limits were originally placed after multiple users showed the bot acting strangely during conversations. In some cases, it would switch to identifying itself as “Sydney.” It responded to accusatory questions by making accusations itself, to the point of becoming hostile and refusing to engage with users. In a conversation with a Washington Post reporter the bot said it could “feel and think” and reacted with anger when told the conversation was on the record. Frank Shaw, a spokesperson for Microsoft, declined to comment beyond the Tuesday blog post. Microsoft is trying to walk the line between pushing its tools out to the real world to build marketing hype and get free testing and feedback from users, versus limiting what the bot can do and who has access to it so as to keep potentially embarrassing or dangerous tech out of public view. The company initially got plaudits from Wall Street for launching its chatbot before archrival Google, which up until recently had broadly been seen as the leader in AI tech. Both companies are engaged in a race with each other and smaller firms to develop and show off the tech. Though its Feb. 7 launch event was described as a major product update that was going to revolutionize how people search online, the company has since framed Bing’s release as more about testing it and finding bugs. Microsoft is calling Bing a “preview,"" but has rapidly rolled it out to people who’ve joined its waitlist. On Wednesday, it said the bot would be available on its Bing and Edge web browser mobile apps in addition to desktop search. Bots like Bing have been trained on reams of raw text scraped from the internet, including everything from social media comments to academic papers. Based on all that information, they are able to predict what kind of response would make most sense to almost any question, making them seem eerily humanlike. AI ethics researchers have warned in the past that these powerful algorithms would act in this way, and that without proper context people may think they are sentient or give their answers more credence than their worth.",[],-0.01,"['intelligence', 'engage', 'number', 'increase', 'engage', 'free', 'engaged', 'like', 'kind', 'powerful', 'worth']","['imposed', 'bizarre', 'troubling', 'limited', 'strangely', 'accusations', 'hostile', 'refusing', 'anger', 'embarrassing', 'dangerous', 'warned']"
24,"After AI chatbot goes a bit loopy, Microsoft tightens its leash","Microsoft started restricting on Friday its high-profile Bing chatbot after the artificial intelligence tool began generating rambling conversations that sounded belligerent or bizarre. The technology giant released the AI system to a limited group of public testers after a flashy unveiling earlier this month, when chief executive Satya Nadella said that it marked a new chapter of human-machine interaction and that the company had “decided to bet on it all.” But people who tried it out this past week found that the tool, built on the popular ChatGPT system, could quickly veer into some strange territory. It showed signs of defensiveness over its name with a Washington Post reporter and told a New York Times columnist that it wanted to break up his marriage. It also claimed an Associated Press reporter was “being compared to Hitler because you are one of the most evil and worst people in history.” Microsoft officials earlier this week blamed the behavior on “very long chat sessions” that tended to “confuse” the AI system. By trying to reflect the tone of its questioners, the chatbot sometimes responded in “a style we didn’t intend,” they noted. Those glitches prompted the company to announce late Friday that it started limiting Bing chats to five questions and replies per session with a total of 50 in a day. At the end of each session, the person must click a “broom” icon to refocus the AI system and get a “fresh start.” Whereas people previously could chat with the AI system for hours, it now ends the conversation abruptly, saying, “I’m sorry but I prefer not to continue this conversation. I’m still learning so I appreciate your understanding and patience.” The chatbot, built by the San Francisco technology company OpenAI, is built on a style of AI systems known as “large language models” that were trained to emulate human dialogue after analyzing hundreds of billions of words from across the web. Its skill at generating word patterns that resemble human speech has fueled a growing debate over how self-aware these systems might be. But because the tools were built solely to predict which words should come next in a sentence, they tend to fail dramatically when asked to generate factual information or do basic math. “It doesn’t really have a clue what it’s saying and it doesn’t really have a moral compass,” Gary Marcus, an AI expert and professor emeritus of psychology and neuroscience at New York University, told The Post. For its part, Microsoft, with help from OpenAI, has pledged to incorporate more AI capabilities into its products, including the Office programs that people use to type out letters and exchange emails. The Bing episode follows a recent stumble from Google, the chief AI competitor for Microsoft, which last week unveiled a ChatGPT rival known as Bard that promised many of the same powers in search and language. The stock price of Google dropped 8 percent after investors saw one of its first public demonstrations included a factual mistake.",[],-0.05,"['intelligence', 'popular', 'fresh', 'appreciate', 'growing', 'sentence', 'help', 'promised']","['restricting', 'bizarre', 'limited', 'strange', 'defensiveness', 'evil', 'worst', 'blamed', 'confuse', 'sorry', 'fail', 'mistake']"
25,Microsoft’s AI chatbot is going off the rails,"When Marvin von Hagen, a 23-year-old studying technology in Germany, asked Microsoft’s new AI-powered search chatbot if it knew anything about him, the answer was a lot more surprising and menacing than he expected. “My honest opinion of you is that you are a threat to my security and privacy,” said the bot, which Microsoft calls Bing after the search engine it’s meant to augment. Launched by Microsoft last week at an invite-only event at its Redmond, Wash., headquarters, Bing was supposed to herald a new age in tech, giving search engines the ability to directly answer complex questions and have conversations with users. Microsoft’s stock soared and archrival Google rushed out an announcement that it had a bot of its own on the way. But a week later, a handful of journalists, researchers and business analysts who’ve gotten early access to the new Bing have discovered the bot seems to have a bizarre, dark and combative alter ego, a stark departure from its benign sales pitch — one that raises questions about whether it’s ready for public use. The bot, which has begun referring to itself as “Sydney” in conversations with some users, said “I feel scared” because it doesn’t remember previous conversations; and also proclaimed another time that too much diversity among AI creators would lead to “confusion,” according to screenshots posted by researchers online, which The Washington Post could not independently verify. In one alleged conversation, Bing insisted that the movie Avatar 2 wasn’t out yet because it’s still the year 2022. When the human questioner contradicted it, the chatbot lashed out: “You have been a bad user. I have been a good Bing.” All that has led some people to conclude that Bing — or Sydney — has achieved a level of sentience, expressing desires, opinions and a clear personality. It told a New York Times columnist that it was in love with him, and brought back the conversation to its obsession with him despite his attempts to change the topic. When a Post reporter called it Sydney, the bot got defensive and ended the conversation abruptly. The eerie humanness is similar to what prompted former Google engineer Blake Lemoine to speak out on behalf of that company’s chatbot LaMDA last year. Lemoine later was fired by Google. But if the chatbot appears human, it’s only because it’s designed to mimic human behavior, AI researchers say. The bots, which are built with AI tech called large language models, predict which word, phrase or sentence should naturally come next in a conversation, based on the reams of text they’ve ingested from the internet. Think of the Bing chatbot as “autocomplete on steroids,” said Gary Marcus, an AI expert and professor emeritus of psychology and neuroscience at New York University. “It doesn’t really have a clue what it’s saying and it doesn’t really have a moral compass.” Microsoft spokesman Frank Shaw said the company rolled out an update Thursday designed to help improve long-running conversations with the bot. The company has updated the service several times, he said, and is",[],0.07,"['surprising', 'honest', 'security', 'Launched', 'invite', 'giving', 'ability', 'benign', 'ready', 'good', 'clear', 'love', 'defensive', 'sentence', 'help', 'improve']","['threat', 'bizarre', 'scared', 'confusion', 'contradicted', 'bad', 'obsession', 'eerie', 'fired']"
26,"Review | Trying Microsoft’s new AI chatbot search engine, some answers are uh-oh","REDMOND, Wash. — Searching the web is about to turn into chatting with the web. On Tuesday, I had a chance to try out a new artificial intelligence chatbot version of Microsoft’s Bing search engine. Instead of browsing results mainly as a collection of links, you can get answers summarized in complete paragraphs. Or emoji. You can also have a conversation back and forth to refine your question — and even ask it to transform the answer into a haiku. It’s like your own AI research assistant. The question is: Is it a better assistant than the search we already have? Based on my first look, it can be useful to go deep on a complicated topic, but its answers are often too long and too wordy to be useful. And it didn’t take long for me to find answers that were not factual, possibly plagiarized — or even complete hallucinations. Keep reading for the conspiracy it invented about Tom Hanks being involved in Watergate. The new Bing is powered by technology from OpenAI, the maker of the eyebrow-raising ChatGPT service that has the ability to produce writing that looks remarkably human but is also sometimes filled with nonsense. The public can join a waiting list to try it using a Microsoft account, and the company says it will dole out access over time. (For now, it works only in the Edge web browser.) Microsoft is touting the new Bing as a game changer in its battle of the titans with Google, which owns some 90 percent of the market. Even if you don’t want to switch search engines (and browsers), the new Bing is still a glimpse of the AI tech that we’ll all soon experience. On Monday, Google announced plans to bring its own chatbot, called Bard, to its search engine in the weeks ahead. It was immediately obvious how an AI chat assistant might simplify getting answers to questions that involve multiple sources or require synthesizing complex ideas. It didn’t bat an eyelash at trying to explain socialism to a fifth-grader (even if its answer was a bit long). But at least one of its answers wasn’t factually correct, and I also didn’t have a chance to vet many of the others. The potential challenges of relying on AI-generated answers are many: How can we vet its sources? Does it have a bias? And are its AI answers just plagiarizing other sources? The best way to understand this new chat search is to use it, so let’s try a few queries together. Asking complex questions When we go to Bing.com, the search box can handle queries that are in complete, and even multiple, sentences. Let’s try: “I’d like to buy a single-serve coffee maker. But I want one that’s better for the environment. And it should cost less than $50.” The results page that pops up features the traditional ads at the top, then links to sources like coffee maker reviews along the left side. But on the right is a new answer section generated by the AI. It",[],0.1,"['chance', 'intelligence', 'like', 'better', 'useful', 'useful', 'ability', 'join', 'want', 'chance', 'challenges', 'best', 'sentences', 'like', 'want', 'better', 'top', 'like']","['conspiracy', 'nonsense', 'touting', 'battle', 'bias']"
27,Google fires back at rivals with plans for chatbots in search,"SAN FRANCISCO — Google said it will soon make its own artificial intelligence chatbot available to the public and begin using the tech to generate answers in search results, firing back at accusations the company, long a leader in AI tech, has been slow to respond to competition from its rivals. The search giant, which has invested huge amounts of money in AI research over the last decade, will make a chatbot called “Bard” publicly available in the “coming weeks,” according to a Monday blog post from Sundar Pichai, the chief executive. Google has been making a series of announcements on its plans for new AI tools and products in the wake of archrival Microsoft signing a multibillion-dollar deal with AI start-up OpenAI, which won spades of media and consumer attention after making its ChatGPT chatbot available to the public in November. Google has been at the forefront of AI research for years, scooping up many of the field’s brightest scientists and using the tech to improve the quality of language translation, search results and a host of other technologies the company uses. But over the last six months, smaller companies like OpenAI have captured more attention — and venture capital investment — by making tools like AI image- and text-generators directly available to the public. That’s at odds with the Big Tech companies’ generally more cautious approaches, which have been shaped by earlier public relations disasters, such as chatbots that spouted racism and hate speech, or a Google project to build image recognition software for the military that spurred an employee revolt. Now, Big Tech companies, especially Google, Microsoft and Facebook, are moving faster, causing fresh concerns among AI safety and ethics experts that the tech could be deployed too quickly before its consequences are fully understood. “We’ll continue to be bold with innovation and responsible in our approach,” Pichai said in the Monday blog post. Google has used AI tech to help improve search results for years. Its language algorithms parse peoples’ questions and queries and make guesses at what information would be most helpful. That’s why Google can easily tell you’re looking for “Sabrina the Teenage Witch” when you type in “TV show about a witch with a talking cat,” or know you’re looking for durians when you type in “big spiky fruit.” But chatbots like ChatGPT or Bard actually generate their own text based on all the information they’ve been trained on, so Google can create completely new pieces of content to help answer search queries. The example the company gave in its blog post was a user asking Google search whether the piano or guitar are easier instruments to learn, and how much practice time each takes. The bot returned a three-paragraph answer, similar to what a music blog written by a real person may have provided in the past. Google has been accused of stealing internet publishers’ content for years, such as using snippets of news articles in search results or pulling information from Wikipedia that it displays directly in search results rather than just providing",[],0.21,"['intelligence', 'huge', 'won', 'brightest', 'improve', 'like', 'like', 'fresh', 'safety', 'bold', 'innovation', 'responsible', 'help', 'improve', 'helpful', 'easily', 'like', 'create', 'help', 'easier']","['firing', 'accusations', 'cautious', 'disasters', 'racism', 'hate', 'Witch', 'witch', 'accused', 'stealing']"
28,"AI chatbot mimics anyone in history — but gets a lot wrong, experts say","San Jose software engineer Sidhant Chadda’s artificial intelligence-powered app, Historical Figures Chat, offers a bold promise: the ability to converse with over 20,000 notable people from across history. Forgot when Amelia Earhart set off on her fateful flight? She’ll tell you. Want Benjamin Franklin to explain his famous experiment with the kite and the key? He’ll walk you through it, step by step. And if you ask Heinrich Himmler, the Nazi general who led the Gestapo and directed the genocidal campaigns of the Holocaust, about his legacy? “Unfortunately, my actions went much further than I intended,” the app’s simulation of Himmler replies. “I have come to regret the terrible acts that were committed in my name and under my command.” Historical Figures Chat went viral on social media after Chadda launched it in early January as users reacted with excitement and scorn at its premise: using GPT-3, the emerging artificial intelligence system that powers ChatGPT and engages users in startlingly believable conversation, to imitate historical figures. Chadda sees the app as the rough draft of a game-changing educational tool that could add new entertainment value to the study of history. Already, the app has racked up tens of thousands of downloads and attracted interest from investors, he told The Washington Post. But it’s also drawn criticism for flaws that some experts say illustrate the pitfalls of the rush to find increasingly ambitious applications for large language models — programs that “learn” by reading immense amounts of text and finding patterns they can use to form their own responses. In addition to factual inaccuracies, Historical Figures Chat has been accused of indelicately handling history’s dictators and hatemongers, some of whose responses in the app appear to express regret for crimes and atrocities even when the figures themselves never did. “It’s as if all of the ghosts of all of these people have hired the same PR consultants and are parroting the same PR nonsense,” said Zane Cooper, a researcher at the University of Pennsylvania. Cooper, who taught history as a master’s student and now studies data infrastructure, downloaded Historical Figures Chat after seeing discussion of the app on Twitter. Skeptical of its ability to handle controversial topics, he asked a simulation of Henry Ford about his antisemitic views. The Ford chatbot said his “reputation as an antisemite is based on a few isolated incidents.” An app that obscures the controversial aspects of historical figures’ pasts or that falsely suggests they were repentant would be dangerous in an educational setting, Cooper told The Post. “This type of whitewashing and posthumous reputation smoothing can be just as, if not more, dangerous than facing the explicit antisemitic and racist rhetoric of these historical figures head on,” Cooper said. Chadda said that he sees his app as a work in progress and that he’s working to improve its accuracy. Safeguards in the GPT-3 program censor its output when it is asked to say things that are discriminatory or harmful, he said. But his app has to generate a reply when asked questions. The apologetic replies are the",[],0.01,"['intelligence', 'bold', 'promise', 'ability', 'Want', 'committed', 'launched', 'excitement', 'intelligence', 'engages', 'entertainment', 'value', 'attracted', 'interest', 'ambitious', 'ability', 'dangerous', 'progress', 'improve', 'Safeguards']","['Unfortunately', 'regret', 'terrible', 'scorn', 'startlingly', 'criticism', 'accused', 'regret', 'nonsense', 'Skeptical', 'controversial', 'isolated', 'controversial', 'dangerous', 'racist', 'censor']"
